[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the Geospatial Community",
    "section": "",
    "text": "Our mission is to build a supportive and inclusive community where students, researchers, and professionals can grow their geospatial skills together. üå±"
  },
  {
    "objectID": "index.html#events-calendar",
    "href": "index.html#events-calendar",
    "title": "Welcome to the Geospatial Community",
    "section": "Events Calendar",
    "text": "Events Calendar"
  },
  {
    "objectID": "index.html#latest-workshop",
    "href": "index.html#latest-workshop",
    "title": "Geospatial Community",
    "section": "Latest Workshop",
    "text": "Latest Workshop\n\n\n\n\n  \n\n\n\n\nSatellite Image Analysis Pipeline in R\n\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2023\n\n\nDr Jacinta Holloway-Brown\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#all-posts",
    "href": "index.html#all-posts",
    "title": "Welcome to the Geospatial Community",
    "section": "All Posts",
    "text": "All Posts\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n\n\n\nDec 23, 2023\n\n\nDecember Newsletter | End of Year re-cap\n\n\n\n\n\n\n\nOct 1, 2023\n\n\nOctober Newsletter | Drop-in help virtual help session | Workshop: Large raster manipulation\n\n\n\n\n\n\n\nSep 30, 2023\n\n\nCreating inset maps using ‚Äòtmap‚Äô\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nSeptember Newsletter | Spatial Autocorrelation | Drop-in help session | Upcoming training\n\n\n\n\n\n\n\nAug 1, 2023\n\n\nAugust Newsletter | Upcoming workshop: Spatial High Performance Computing | In-person catch-up | Geospatial conferences\n\n\n\n\n\n\n\nJul 4, 2023\n\n\nJuly Newsletter | Learn Google Earth Engine this month\n\n\n\n\n\n\n\nJun 3, 2023\n\n\nExtracting spatial data from web map servers in R\n\n\n\n\n\n\n\nJun 1, 2023\n\n\nJune Newsletter | Drop-in sessions | More Python this month\n\n\n\n\n\n\n\nMay 16, 2023\n\n\nNewsletter | Python + Folium | Help Sessions are Back!\n\n\n\n\n\n\n\nApr 12, 2023\n\n\nNewsletter | April Workshop Rasters and Random Forests\n\n\n\n\n\n\n\nMar 23, 2023\n\n\nR Spatial Basics\n\n\n\n\n\n\n\nMar 22, 2023\n\n\nCommunity groups for Continuing Professional Development\n\n\n\n\n\n\n\nMar 10, 2023\n\n\nNewsletter | R spatial basics | How to measure area | Survey results\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nCalculating areas with vector and raster data in R\n\n\n\n\n\n\n\nJan 22, 2023\n\n\nNewsletter | Happy New Year! | February meet-up | R Training\n\n\n\n\n\n\n\nNov 3, 2022\n\n\nBuild a blog with Quarto, Git, and RStudio\n\n\n\n\n\n\n\nSep 9, 2022\n\n\nNewsletter | Climate data blog post + problem solving session + conference in Fiji + more\n\n\n\n\n\n\n\nSep 1, 2022\n\n\nAnalysing Climate data with R\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nNewsletter | Analysing climate data with R + Interactive Maps + AEO Forum + more.\n\n\n\n\n\n\n\nJul 20, 2022\n\n\nInteractive mapping with RShiny\n\n\n\n\n\n\n\nJul 14, 2022\n\n\nNewsletter | Mapping interactively + R for ecologists + Data wrangling + more.\n\n\n\n\n\n\n\nJul 7, 2022\n\n\nProblem Solving Session II\n\n\n\n\n\n\n\nMay 26, 2022\n\n\nProblem Solving Session I\n\n\n\n\n\n\n\nApr 25, 2022\n\n\nCloud computing with Open Data Cube and Python\n\n\n\n\n\n\n\nMar 31, 2022\n\n\nIntroduction to tidy spatial networks\n\n\n\n\n\n\n\nFeb 24, 2022\n\n\nBasic Raster Analysis with R\n\n\n\n\n\n\n\nNov 23, 2021\n\n\nHow to contribute a post (using blogdown on our old site)\n\n\n\n\n\n\n\nOct 28, 2021\n\n\nCreating a geospatial blog with blogdown (on our old site)\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html",
    "href": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html",
    "title": "Creating a geospatial blog with blogdown (on our old site)",
    "section": "",
    "text": "In this post, we will go through the process of creating a geospatial blog, specifically this blog.\nFirst, we will run through how to create a site and host it through github and netlify. Then we will show you the options for publishing both a raster and vector data."
  },
  {
    "objectID": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#good-resources",
    "href": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#good-resources",
    "title": "Creating a geospatial blog with blogdown (on our old site)",
    "section": "Good resources",
    "text": "Good resources\n\nhttps://www.apreshill.com/blog/2020-12-new-year-new-blogdown/\nhttps://www.youtube.com/watch?v=x-Ch0-w1UhY\nhttps://solomonkurz.netlify.app/post/2021-05-03-blogdown-updates-prompted-a-website-overhaul-these-are-my-notes/\nhttps://bookdown.org/yihui/blogdown/installation.html"
  },
  {
    "objectID": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#prerequisites",
    "href": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#prerequisites",
    "title": "Creating a geospatial blog with blogdown (on our old site)",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nFairly recent version of R studio (RStudio IDE version, v1.4.1106 +)\nGithub account\nGIT locally on computer. (Happy git with R https://happygitwithr.com/)\n\ngitforwindows.org\nDownload GNU\nDefault on all settings ÔÇß Make sure to select Git from the command line and also from 3rd party software\n\nSign up for Netlify using Github account"
  },
  {
    "objectID": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#create-a-new-github-repository",
    "href": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#create-a-new-github-repository",
    "title": "Creating a geospatial blog with blogdown (on our old site)",
    "section": "1. Create a new github repository",
    "text": "1. Create a new github repository\nInitialise with readme, but don‚Äôt add the .gitignore file. Then copy link https link."
  },
  {
    "objectID": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#create-a-new-project-in-r-studio",
    "href": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#create-a-new-project-in-r-studio",
    "title": "Creating a geospatial blog with blogdown (on our old site)",
    "section": "2. Create a new project in R studio",
    "text": "2. Create a new project in R studio\nIn R studio, go to File &gt; new project &gt; Version control &gt; git, and Paste the URL from before. Save the project somewhere sensible.\nNow install blogdown with Install.packages(‚Äúblogdown‚Äù), and load with library(blogdown).\n\ninstall.packages(\"blogdown\")\nlibrary(blogdown)\n\nNow to create a new site, just add\n\nnew_site()\n\nThis will give the default theme, but there are a lot of different themes to choose from!\nhttps://themes.gohugo.io/\nIts important to find one that you like, but also that is up to date and works. For this blog, we ended up going with https://themes.gohugo.io/themes/anatole/ over some other options which were buggy, probably due to being out of date.\nSo to build the site with your theme of choice, run\n\nnew_site(theme = \"lxndrblz/anatole\")\n\nAdding theme= ‚Äúgighubusername/themerepo‚Äù of the theme you choose.\nWhen prompted, select y to let blogdown start a server. This will let you preview the site in the viewer. To view in a browser, click the show in new window (next to the broom) to launch it locally.\nGenerally, you can serve the site, and stop serving the sites using\n\nblogdown::serve_site() #to serve the site locally\nblogdown::stop_server() #to stop serving the site"
  },
  {
    "objectID": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#write-a-post",
    "href": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#write-a-post",
    "title": "Creating a geospatial blog with blogdown (on our old site)",
    "section": "3. Write a post",
    "text": "3. Write a post\nHopefully the local site is working. We can now add a new blog post using either\n\nblogdown::new_post() \n\nOR, a better method is to navigate through addins dropdown (under help, right of git icon), click new_post. This brings up a dialog to fill out.\nSelect file type, markdown for simple text, or .Rmd or .Rmarkdown for embedding code.\nNow we can add code chunks! The easiest way to do this is to click the green +c just above the editor.\nAs an example\n\nlibrary(ggplot2)\nggplot(Orange, aes(x = age, \n                   y = circumference, \n                   color = Tree)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\nIf its not working, run\n\nblogdown::check_site() \n\nand follow the instructions next to the [todo] items."
  },
  {
    "objectID": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#load-to-github",
    "href": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#load-to-github",
    "title": "Creating a geospatial blog with blogdown (on our old site)",
    "section": "4. load to github",
    "text": "4. load to github\nIn the files tab, navigate to the .gitignore file. Add so it contains the following .Rproj.user .Rhistory .RData .Ruserdata .DS_Store Thumbs.db /public/ /resources/\nNow run\n\nblogdown::check_gitignore() \n\n#and \n\nblogdown::check_content()\n\nThen commit the files and push to github.\nDue to the massive number of files associated with the themes, we found it better to do the first commit through the shell\nTools&gt;shell&gt;git add -A\nTo authorise github, we found the best option to be to\nControl Panel &gt; User Account &gt; Credential Manager &gt; Windows Credential &gt; Generic Credential\nThen remove git credential\nThen, when you push the repo it‚Äôll ask you for credential through the browser."
  },
  {
    "objectID": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#publish-site",
    "href": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#publish-site",
    "title": "Creating a geospatial blog with blogdown (on our old site)",
    "section": "5. Publish site!",
    "text": "5. Publish site!\nLog into netlify (using github account). Then click new site from git, continuous deployment: Github. you should be able to see the repo from within netlify. Select deploy site.\n\n\n\nDeploys by Netlify\n\n\nIt will give you a temporary URL which is live! Now it will automatically update every time you push changes to github.\nTo change the site name, general &gt; site details &gt; change site name\nNow go back to R studio, and navigate to teh config (yaml or toml) and add in the correct url (probably around line 3)\nRun Blogdown::check_netlify() to find any issues."
  },
  {
    "objectID": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#load-necessary-packages",
    "href": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#load-necessary-packages",
    "title": "Creating a geospatial blog with blogdown (on our old site)",
    "section": "Load necessary packages",
    "text": "Load necessary packages\n\nlibrary(sf)\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\nlibrary(tmap)"
  },
  {
    "objectID": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#get-the-data",
    "href": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#get-the-data",
    "title": "Creating a geospatial blog with blogdown (on our old site)",
    "section": "Get the data",
    "text": "Get the data\nThe process to get the data is stored in a script (scripts/get_osm_data.R), instead of integrating it into this R Markdown file. This allows us to not overload the data provider but always querying the API, every single time the article is rendered! (And we don‚Äôt need to process the data every time either.)\nHere, we only need to read the data from a file that was previously created:\n\ngreen_space &lt;- st_read(\"data/green_spaces.geojson\")\n\nReading layer `green_spaces' from data source \n  `C:\\Git\\geocommunity_blog\\posts\\2021-10-28-creating-a-geospatial-blog-with-blogdown\\data\\green_spaces.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 5861 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 152.6764 ymin: -27.67486 xmax: 153.4664 ymax: -27.00613\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#visualise-on-a-slippy-map",
    "href": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#visualise-on-a-slippy-map",
    "title": "Creating a geospatial blog with blogdown (on our old site)",
    "section": "Visualise on a slippy map",
    "text": "Visualise on a slippy map\nThe tmap package is useful to visualise vector data on a slippy map.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(green_space) +\n  tm_polygons(col = c(\"#43C467\"), alpha = 0.5)\n\n\n\n\n\n\n\nData is copyright OSM contributors but release under an ODBL licence."
  },
  {
    "objectID": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#load-the-packages",
    "href": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#load-the-packages",
    "title": "Creating a geospatial blog with blogdown (on our old site)",
    "section": "Load the packages",
    "text": "Load the packages\n\nlibrary(terra)\n\nterra 1.6.3"
  },
  {
    "objectID": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#import-the-data",
    "href": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#import-the-data",
    "title": "Creating a geospatial blog with blogdown (on our old site)",
    "section": "Import the data",
    "text": "Import the data\nThe data comes from the Bureau of Meteorology website, it is a raster file of average annual rainfall. We‚Äôve put the file into a data directory, inside the blog post‚Äôs directory.\n\nrain &lt;- rast(\"data/rainan.txt\")"
  },
  {
    "objectID": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#inspect",
    "href": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#inspect",
    "title": "Creating a geospatial blog with blogdown (on our old site)",
    "section": "Inspect",
    "text": "Inspect\n\nrain\n\nclass       : SpatRaster \ndimensions  : 691, 886, 1  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : 111.975, 156.275, -44.525, -9.975  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource      : rainan.txt \nname        : rainan \n\n\nOne single band, by default with the WGS 84 CRS.\nThe average rainfall for the whole raster is 451.61 mm."
  },
  {
    "objectID": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#visualise",
    "href": "posts/2021-10-28-creating-a-geospatial-blog-with-blogdown/index.html#visualise",
    "title": "Creating a geospatial blog with blogdown (on our old site)",
    "section": "Visualise",
    "text": "Visualise\nMake sure to add a caption to visualisations, and some alternative text if needed!\n\nplot(rain)\n\n\n\n\nAverage annual rainfall in mm (1980 to 2010)"
  },
  {
    "objectID": "posts/2021-11-23-creating-a-post/index.html",
    "href": "posts/2021-11-23-creating-a-post/index.html",
    "title": "How to contribute a post (using blogdown on our old site)",
    "section": "",
    "text": "We will be covering some basics of multiple programming skills (Git/GitHub, R, html, etc‚Ä¶) with the aim of empowering people to contribute to blogdown websites such as the University of Queensland Geospatial Analysis Community of practice (UQGAC) blog. The idea is to encourage community members to contribute material directly instead of funneling everything through a website administrator."
  },
  {
    "objectID": "posts/2021-11-23-creating-a-post/index.html#geospatial-analysis-community-of-practice---the-university-of-queensland-uqgac",
    "href": "posts/2021-11-23-creating-a-post/index.html#geospatial-analysis-community-of-practice---the-university-of-queensland-uqgac",
    "title": "How to contribute a post (using blogdown on our old site)",
    "section": "Geospatial Analysis Community of Practice - The University of Queensland (UQGAC)",
    "text": "Geospatial Analysis Community of Practice - The University of Queensland (UQGAC)\nUnveil the new website!\nPoint of contact - Mitchel Rudge: mitchel.rudge@uq.edu.au\nSee the About page for more info."
  },
  {
    "objectID": "posts/2021-11-23-creating-a-post/index.html#creating-a-post-on-the-uqgac-website",
    "href": "posts/2021-11-23-creating-a-post/index.html#creating-a-post-on-the-uqgac-website",
    "title": "How to contribute a post (using blogdown on our old site)",
    "section": "Creating a post on the UQGAC website",
    "text": "Creating a post on the UQGAC website\nCatherine Kim, PhD\nPostdoctoral Associate, School of Biological Sciences\nTechnology Trainer, UQ Library\nTwitter: @fishiintheC\nWhat we will cover:\n\nblogdown basics\nGit and GitHub basics\nHow to create a post on the UQGSAC blogdown website\nR Markdown basics\n\nI have pieced this together using many other resources on the above which are mentioned throughout. Thank you to Mitch and St√©phane for their help with this tutorial and workshop!\nWhat you will need:\n\nInstallations - R, RStudio, Git\nA GitHub account (free) with your login and personal access token (PAT) details handy"
  },
  {
    "objectID": "posts/2021-11-23-creating-a-post/index.html#blogdown-basics",
    "href": "posts/2021-11-23-creating-a-post/index.html#blogdown-basics",
    "title": "How to contribute a post (using blogdown on our old site)",
    "section": "blogdown basics",
    "text": "blogdown basics\nblogdown is an R package that allows the creation of websites using R Markdown and Hugo, a static site generator. blogdown websites in R have been all the rage the last few years and you have probably seen many ‚ÄòHugo-Academic theme‚Äô personal websites - all built in R!\n\n\n\nblogdown hex sticker Credit: Creating Website with R Markdown\n\n\n\nThere is a short online book on blogdown written by the developer, Yihui and others.\nA recent article by Allison Hill on starting your own blogdown website from scratch.\nSee Mitch and St√©phane‚Äôs tutorial for UQGSAC on creating a blogdown website.\n\nThis session focuses on how to go about contributing a post to an existing website.\nThe UQGSAC website is built using the anatole theme. There are many themes to choose from and if you know html/CSS you can even build your own theme.\nSo, how do we go about contributing to a blogdown website?\n\n\n\nProgrammer GIF Credit: Capgemini India on GIPHY\n\n\nA good place to start is making sure you have installed the blogdown package:\n\ninstall.packages(\"blogdown\")\n\nArticle about setting global options in blogdown if you need to set the Hugo version in your .Rprofile (blogdown::config_Rprofile()).\nTo allow multiple people to contribute to the same website, the website is hosted on GitHub and Netlify.\nAs the website is already set-up, we will be dealing with Git + GitHub and R + RStudio + R Markdown."
  },
  {
    "objectID": "posts/2021-11-23-creating-a-post/index.html#git---what-is-it",
    "href": "posts/2021-11-23-creating-a-post/index.html#git---what-is-it",
    "title": "How to contribute a post (using blogdown on our old site)",
    "section": "Git - what is it?",
    "text": "Git - what is it?\nA version control software (think track changes) useful for collaborating on and sharing code.\nGit is the software itself that records changes to a set of files locally. There are several hosting platforms that are like online repositories (think Dropbox, Google Drive, etc.) that work with Git: Bitbucket, GitLab, and GitHub to name a few.\nThese platforms not only allow for version control but also to collaborate, organize, and back up projects.\nIn this case, we will be using GitHub to access the website files, make some changes (i.e., add a post), and then incorporate those changes back to the website repository on GitHub which will automatically update the website itself. üôå\nWe will focus on contributing a post to an existing website repository on GitHub, but there are lots of fabulous and free resources online that go more into depth on Git:\n\nIf you need to be convinced to use Git for version control see this article and Happy Git and GitHub for the useR to git started both by Git/R guru Jenny Bryan.\nSee Caitie Kuempel‚Äôs R Ladies Brisbane presentation on getting started with GitHub in RStudio.\n\n\nGit Terminology\nRepository/repo - where a project is stored in GitHub. Think of it like a folder holding all the relevant documents that you can version control, view history, and add collaborators. The repository or repo holds all the relevant files for the website - most of which we will not touch.\nFork - A copy of another user‚Äôs repo on your account. This allows you to freely change a project without affecting the original upstream repo. You can keep your fork synced with changes in the original repo. - this is fetching upstream.\nClone - a copy of a repository that lives on your computer instead of a website server like GitHub. Is still connected to the remote repo online and you can push/pull edits.\nCommit - is one or more changes to a file or set of files that you are asking GitHub to keep track of.\nPush - sending your committed changes to a remote repository on GitHub. Local changes updated on the GitHub website where other people can access.\nPull - incorporating and merging changes. An edit on the remote repository on GitHub can be pulled to a local repository.\nDiff - difference, or changes made that are visible as insertions/deletions for a commit.\nMain/Master - the default branch you are on. Master has recently updated to main, but they are the same thing. You are more likely to come across master on older resources. Jenny Bryan strongly urges you to create a new branch to work off of which requires using command line. For the purpose of contributing to a blogdown website, I will forgo covering this as it is unlikely more than one person will be contributing at the same time.\nOrigin - the remote repo online from which you have cloned your local copy from."
  },
  {
    "objectID": "posts/2021-11-23-creating-a-post/index.html#how-to-create-a-post-in-blogdown",
    "href": "posts/2021-11-23-creating-a-post/index.html#how-to-create-a-post-in-blogdown",
    "title": "How to contribute a post (using blogdown on our old site)",
    "section": "How to create a post in blogdown",
    "text": "How to create a post in blogdown\nStarting with Git and GitHub:\n\n1. Fork the repo\n\nSign in with your GitHub account\nGo to the geocommunity/website repo\nPress the ‚ÄòFork‚Äô button at the top right. \nA forked copy of the repo should now be visible in your GitHub account. YOU/blog_website is the origin for your local copy of the repo in RStudio and geocommunity/blog_website is the upstream repo.\n\n\n\n\nScreenshot of a forked repo on GitHub.\n\n\n\n\n2. Clone the repo in a new RStudio project\n\nYou will need your GitHub credentials handy.\nYou can also set up RStudio so you do not need to input your GitHub credentials every time.\n\nFrom your forked repo, click on the green ‚ÄòCode‚Äô button and copy the link in the pop-up window.\n\n\n\nScreenshot of finding the url to clone.\n\n\nNext, in RStudio, go to File &gt; New Project. In the pop-up window, click the last option ‚ÄòVersion Control‚Äô and then ‚ÄòGit‚Äô. In the following window, paste the url you copied from your forked GitHub repo in the first box which will automatically input the name of the project.\n\n\n\nPop-up windows of cloning a GitHub repo in RStudio\n\n\nConceptually, what we have done is:\n\n\n\nConceptual diagram of forking and cloning in GitHub Credit: Happy Git for the useR\n\n\nNow that we have cloned the repository, let‚Äôs explore the file structure a little in the ‚ÄòFiles‚Äô tab in RStudio. It is NOT a very intuitively set-up even for intermediate users of R. For the purposes of creating a new post to add to the blog, we are mostly concerned with the content/english/ directory that contains the post/ sub-directory.\nThe rest of the files are the ‚Äòbackend‚Äô of the site using html, CSS, js, etc. to build the website. Have a look if you are curious but make changes at the risk of being ‚Äòthat person‚Äô to break the site! But don‚Äôt worry, since we are using Git version control all changes are tracked and reversible.\nNow that we are somewhat familiar with the project structure, let‚Äôs create a new post.\n\n\n3. Create a new post\nIn our new RStudio project housing our forked and cloned GitHub repo of the website:\nUse blogdown::new_post(\"New post name\", ext = \".Rmd\") in the console to create a new post with a .Rmd extension. Alternatively, you can go to the Addins button under the menu and choose ‚ÄòNew Post‚Äô under the BLOGDOWN section and fill in the information in the pop-up window.\n\n\n\nA new blogdown post in RStudio.\n\n\n\nThis will create a new page/post bundle folder or sub-directory within post/ with the date and the name given in new_post() function. e.g., post/2021-11-18-New-post-name.\nAn index.Rmd file has been opened and only contains a YAML header (enclosed by ---). More on that later. Do not change the name of the .Rmd file.\nEach post gets its own bundle which is where your static post-specific files like images or data (.csv files etc.) used in your post should go.\nNote that the ‚ÄúNew post name‚Äù will not only be the incorporated into the sub-directory name, but also the url to the post. Read: choose wisely and concise &gt; long descriptive name.\nThis ‚ÄúNew post name‚Äù will automatically be filled as the ‚ÄòTitle:‚Äô in the .Rmd YAML heading. If you want a longer, descriptive title - change it in the YAML heading.\nIt is recommend you use either blogdown::new_post() or the Addin to create a new post instead of manually creating a new file (File &gt; New File &gt; R Markdown script)\n\nHere, we will stick with the .Rmd extension, but know there are a few file types:\n.md - markdown, cannot run code chunks\n.Rmd - R markdown -&gt; rendered to .html using Pandoc\n.Rmarkdown - also R markdown -&gt; compiled to .markdown documents\nIf you want more of this detailed stuff see: https://bookdown.org/yihui/blogdown/output-format.html.\n\n\n4. Commit the changes i.e., the new post\nLet‚Äôs commit our new post. You can add something like a line of text, or not.\n\nIf you cloned the repo properly there should be a Git tab in the upper right hand window in RStudio where the Environment is. In the Git repo, there should be some files listed (i.e., post/2021-11-23-New-post-name) with different colored boxed under the ‚ÄòStatus‚Äô column - hover with the cursor to see what they mean.\nCheck the ‚ÄòStaged‚Äô box for the files you want to include in this commit.\nClick the Commit button and a window will pop-up. In the bottom section, you will see the changes made to the file as additions (green) and deletions (red) - this is known as the diff in GitHub speak. For a new file, the whole thing will be green because it is all new.\nIn the ‚ÄòCommit message‚Äô box, add a concise but descriptive message of the changes like ‚ÄòAdded a new post bundle.‚Äô Once you are happy with everything (file staged, commit messages, etc.) click the ‚ÄòCommit‚Äô button.\n\n\n\n\nScreenshot of a forked repo on GitHub.\n\n\nSome stuff will happen and as long as you do not see any obvious errors then it has probably all gone well and you‚Äôve made your first commit!\n\n\n\nCelebrate! Credit: http://www.reactiongifs.com/cheering-minions/\n\n\nKnowing when and how often to commit is a bit of an art that comes with experience. In general, you want to commit changes that are related to a single problem and a good commit message. There is also a History button on the top left corner that will list all the commits with messages you have made and you can view the diff by clicking on a commit. All commits have a unique code which you can use to return to a previous commit etc.\nImportant notes:\n\nOnce you have served the site (see Step 6) it will create additional files within your post bundle directory. Be sure to commit all files in the post bundle created when knitting (/index.html, /index_files, etc.) not just the index.Rmd file as they will be necessary to build the site from GitHub. - You can stage them all together as one commit.\nYou will not be able to see diffs in the commit window until they have been saved.\n\nSee more on committing and best practices from the R packages book.\n\n\n\nHow committing goes‚Ä¶ Credit: xkcd comics\n\n\n\n\n5. Push the changes to GitHub\nThe changes and commits we have made are local, but we need to get them onto the GitHub repo and then the website. This is where we need to push.\nIn the Git window, you will see a blue down arrow for pulling and a green up arrow for pushing. You will also see a message along the lines of Your branch is ahead of 'origin/main' by X commits under those buttons.\nFor the purposes of contributing a post to a blogdown website, we will not worry about pulls and fetching upstream. This basically means keeping your origin/master repo synced with the original upstream repo that you forked.\nIf you stick to creating a new post bundle and only modifying files within the post bundles it should be okay without fetching upstream. BUT know that if you are using GitHub to work collaboratively, staying current with the original repo is important and in general it is a good practice to always pull before you push. Recommend Happy Git and GitHub for the useR as a trusty guide.\nIf you want to try fetching upstream it is easiest to do via GitHub. Followed by a pull in RStudio.\n\nLog into your GitHub account online, and navigate to your YOU/blog_website repo.\nUnder the green Code button, there should be a Fetch upstream button that will sync your forked repo with the original upstream repo.\nThere is information about the status of your branch compared to the original upstream repo e.g., ‚Äòup to date‚Äô or ‚ÄòX commits ahead/behind‚Äô to give you an idea if you need to fetch or not.\nNow in RStudio, you should be good to pull.\nRevisiting this diagram, the fetch upstream is updating your forked repo from the original yellow repo and then the pull is updating your local repo from your forked repo.\n\n\n\n\nDiagram of fetching upstream and pulling Credit: modified from Happy Git for the useR\n\n\nNow we will push our commits from or local repo to our remote origin/master repo on GitHub.\nIf this is your first time using Git with RStudio, you will have to set-up a personal access token or PAT in GitHub. For detailed directions, go to the GitHub page.\n\nGo to your GitHub account online and click on your profile photo in the upper-right and go to Settings.\nIn the left sidebar, click on Developer settings then Personal Access Tokens.\nClick the Generate a new token button and give a descriptive name and expiration.\nSelect scopes or privacy settings (defaults are generally fine) and the generate the token.\nCopy your PAT and put it in the password field for any pop-ups asking for your GitHub credentials when you push.\n\nIf you see HEAD -&gt; main then all good.\n\n\n\nScreenshot of push window in RStudio.\n\n\nNow if you go back to your GitHub account and forked repo online, you should see the changes you made locally are now in the remote online repo and your commit message.\n\n\n\nScreenshot of pushed changes on forked GitHub repo.\n\n\nIn general, you should commit often and then push.\n\n\n6. Serve the site\nIn the console, run blogdown::serve_site(). Alternatively, can click on RStudio ‚ÄòAddins‚Äô and select ‚ÄòServe Site‚Äô. Be patient, but what happens?\n\n\n\nScreenshot of served site in RStudio.\n\n\nSome important information on what is going on from blogdown: Creating Websites with R Markdown:\n\nServing the site did the following: 1. Started a local Hugo server to help you preview your website, and 2. Knitted a sample .Rmd post to an .html page. You can see this from the progress message that printed to your console: Rendering content/english/post/2021-11-23-creating-a-post/index.Rmd... Done\n\nYou can also view the locally served website in a browser by clicking on the ‚ÄúShow in new window‚Äù button at the top left of the RStudio Viewer pane to the right of the broom.\nServing the site is using something called LiveReload:\n\nLet‚Äôs introduce an important and helpful technology that you just used: LiveReload. Serving your site uses LiveReload, which means your website will be automatically rebuilt and reloaded in your web browser when you modify any source file of your website and save it. Basically, once you launch the website in a web browser, you do not need to rebuild it explicitly anymore. All you need to do is edit the source files, such as R Markdown documents, and save them. There is no need to click any buttons or run any commands. LiveReload is implemented via blogdown::serve_site() and Hugo, and you will only need to use it once per work session.\n\nRemember, every time you save your .Rmd file will activate the LiveReload. To stop serving the site locally run blogdown::stop_server() in the console.\n\n\n7. Create your content\nOnce the website is set-up, forked, and cloned‚Ä¶ you can get on with creating a new post with minimal coding. The main thing you will need to use is:\n\nR Markdown\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML documents that we can incorporate into the website. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. For a website post, knitting is not as important because we can serve our site locally which automatically knits anything new and view the changes as we just learned üëÜ.\nNow let‚Äôs add some information about R Markdown:\n\nYAML header\nThe YAML, or Yet Another Markdown Language, header at the top of the .Rmd is set between --- tags. Here is where information like the Title, Date, Author of the document go and will appear in the post.\nHave a look at previous posts and add any relevant tags or categories as you like.\nThe default .Rmd has some redundant settings (tags vs Tags) so if you use them stick with the lower case settings.\n\n\nFormatting\nCan bold and italicize text.\nHeadings:\nCan specify headings using # marks. The number of has symbols corresponds to the level of the header (2 hashs = level 2 header)\nThis will also create a structure outline of your document you can navigate either by using the ‚ÄòFormatting‚Äô button at the bottom of the .Rmd or the right most button in the top right of the .Rmd.\n\n\n\nScreenshot of buttons to view document outline\n\n\nMake lists:\n\none\ntwo\nthree\n\nfull indent for sub-bullet\n\n\nOrdered lists:\n\nlists\nneed spaces\nbefore and after\n\nFor a return to start a new line, leave two spaces at the end of the line.\nLike this.\n\n\nIncluding code\nYou can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nThere is also inline code: The mean of speed in the cars data set is 15.4.\n\n\nInclude mathematical notation\nMathematical notation can be enabled using third party JavaScript libraries like KaTeX. See resource of supported TeX functions. For these to render correctly you must add math: true to the YAML header at the top of the .Rmd.\nTo enter equations like a code chunk or block math, use two $ on separate lines surrounding your equations.\nPut two \\ after a line for a full return.\n\\[\ny = mx + \\beta\\\\\nE = mc^2\n\\]\n\\[ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } \\] You can also use inline math notation by sandwiching it between $ without spaces. Like so \\(\\mu = 0.2566\\).\nAnother inline way: (= = 1.6180339887‚Ä¶)\n\n\nIncluding Plots\nYou can also embed plots, for example:\n\n\n\n\n\n\n\nChunk options\nChunks are the gray areas in the .Rmd file where you can add code that will be run. These are defined by three back ticks (not a single quote, the key to the left of the 1). You can insert R code chunks but also in other languages! See the green ‚ÄòInsert‚Äô code chunk button to see different options. An R code chunk will have the an {r  } after the opening back ticks.\nThe keyboard short cut to add an R code chunk is Ctrl + Alt + I\nYou can also set options in the {} of a chunk like hide the code chunk (echo = FALSE), suppress warnings (warnings = FALSE), and cache the chunk (cache = TRUE) if you have something that takes a while to run.\nLet‚Äôs set echo = FALSE for our plot chunk above. We are only interested in seeing the plot, not the code that produces the plot.\nYou can add a code chunk at the beginning of the .Rmd file and set global options that will apply to the whole document.\n\nknitr::opts_chunk$set(echo = TRUE)\n\nSee more at:\n\nRStudio - https://rmarkdown.rstudio.com/lesson-3.html\nR Markdown Cookbook - https://bookdown.org/yihui/rmarkdown-cookbook/\n\nIt is also a good idea to name your chunks as chunks are included in the document outline. Chunks cannot have the same name - you will get an error.\n\n\nInsert objects\nYou can add pictures, weblinks, and GIFs in R Markdown. They all follow the similar hyperlink formats.\nFor a hyperlink to a website you put the word you want to hyperlink in square brackets [] followed immediately (no spaces or characters) by round parentheses (). E.g. [GitHub](www.github.com)\nTo insert an image or gif from a website you add a ! before the square brackets like so: ![description](https://media.giphy.com/media/sJWNLTclcvVmw/giphy.gif). The description in the [] will appear as a caption and the link must end in the appropriate file extension (.gif, .jpg, .png, etc) to work.\n\n\n\nFunny Yawn Credit: https://www.reddit.com/r/gifs/comments/54q75s/goodnight_tongue/\n\n\nYou can also insert pictures using the RStudio ‚ÄòAddins‚Äô &gt; ‚ÄòInsert Image‚Äô and uploading an image saved on your computer with a few other options like alt text. This will result in the same hyperlink code as inserting an image, but with a relative path instead of the url.\nExample use Addin to insert image\nYou can also save files (like images, html presentations) in your post bundle to link using relative paths on your own.\n\n\n\n\n7. Pull Request\nOkay, so as you were creating the content of your post you should have been committing regularly and then pushing, right?\nLet‚Äôs say we are finished with our beautiful post and read to incorporate it into the original upstream repo geocommunity/blog_website that we forked from. Remember, when we push we are pushing the commits we made locally on our computer to our YOU/blog_website repo that we forked from the original repo.\nBecause we are not owner/developers of the upstream geocommunity/blog_website repo we need to submit a pull request to submit our new blog post for approval into the upstream repo.\n\nOn your YOU/blog_website repo in your GitHub account, click on Pull requests.\nOn the right of the screen, there should be a green New pull request button. This will take to you a ‚ÄòComparing changes‚Äô window outlining the files and changes you have made. This will alert you to any merge conflicts with the original upstream repo. Again, sticking to creating a new post bundle/content should avoid any merge conflicts.\nClick the green Create pull request button on the right. This will take you to a ‚ÄòOpen a pull request‚Äô window that will have your last commit and space to add a larger message with your pull request or PR.\nOnce you are happy, click the green button at the end and wait for approval. You can have a bit of a conversation to hash out any issues as well over the approval process.\n\n\n\n\nScreenshot of GitHub open a pull request.\n\n\nCongratulations - now you have submitted your blog post to a blogdown site!\n\n\n\nCheers Credit: Sony"
  },
  {
    "objectID": "posts/2021-11-23-creating-a-post/index.html#troubleshooting",
    "href": "posts/2021-11-23-creating-a-post/index.html#troubleshooting",
    "title": "How to contribute a post (using blogdown on our old site)",
    "section": "Troubleshooting‚Ä¶",
    "text": "Troubleshooting‚Ä¶\nLet‚Äôs face it, the likelihood of something going awry following this tutorial is not 0‚Ä¶ Few things that might help along the way:\n\nSometimes it is difficult to tell when the ‚ÄòLiveReload‚Äô has finished or if you are used to saving regularly every few minutes that constant updating of ‚ÄòLiveReload‚Äô can freeze RStudio.\n\nSolution: The good ‚Äôole Restart R (Session &gt; Restart R) or close and re-open.\n\nFormatting wise, it‚Äôs a good idea to put full line returns before/after formatting bits like lists and inserting images. Something to check if your content is not formatting as you expect.\nOTHERS??"
  },
  {
    "objectID": "posts/2021-11-23-creating-a-post/index.html#resources-mentioned",
    "href": "posts/2021-11-23-creating-a-post/index.html#resources-mentioned",
    "title": "How to contribute a post (using blogdown on our old site)",
    "section": "Resources mentioned:",
    "text": "Resources mentioned:\n\nCreating Websites with R Markdown by Yihui Xi, Amber Thomas, and Alison Presmanes Hill.\n‚ÄúUp & running with blogdown in 2021‚Äù Alison Hill.\n‚ÄúCreating a Geospatial Blog with blogdown‚Äù on the UQGSAC blog by Mitch Rudge and St√©phane Guillou.\nExcuse me, do you have a moment to talk about version control? by Jenny Bryan\nHappy Git and GitHub for the useR by Jenny Bryan.\nGetting Started with GitHub R Ladies Brisbane presentation by Caitie Kuemple.\ngeocommunity/website GitHub repo\n18.6 Commit best practices from the R packages book by Hadley Wickham and Jenny Bryan.\nR Markdown\nKaTeX - Supported Functions\nCode Chuncks\nR Markdown Cookbook by Yihui Xie, Christophe Dervieux, Emily Riederer."
  },
  {
    "objectID": "posts/2022-02-23-raster-analysis/index.html",
    "href": "posts/2022-02-23-raster-analysis/index.html",
    "title": "Basic Raster Analysis with R",
    "section": "",
    "text": "Presented by Mitch Rudge\nPhD Candidate, UQ Sustainable Minerals Institute\nTwitter:"
  },
  {
    "objectID": "posts/2022-02-23-raster-analysis/index.html#what-we-will-cover",
    "href": "posts/2022-02-23-raster-analysis/index.html#what-we-will-cover",
    "title": "Basic Raster Analysis with R",
    "section": "What we will cover:",
    "text": "What we will cover:\n\nRaster and vector basics\nIntroducing the Terra package\nCreating, Importing and exporting rasters\nDealing with coordinate reference systems\nNaming and sub-setting SpatRaster layers\nRaster summaries\nRaster data manipulation\nCompatibility between Raster and Terra\nOptions to allow the processing of large files\nA real world example with drone data\nA fun new package called Layer\n\nYou will need installations of R, RStudio and Terra."
  },
  {
    "objectID": "posts/2022-02-23-raster-analysis/index.html#raster-and-vector-basics",
    "href": "posts/2022-02-23-raster-analysis/index.html#raster-and-vector-basics",
    "title": "Basic Raster Analysis with R",
    "section": "1. Raster and vector basics",
    "text": "1. Raster and vector basics\nRasters divide areas into a grid of rectangles of equal size. Each rectangle holds one or more value for a variable of interest.\nVectors consist of a series of coordinates that make points, lines or polygons.\n http://gsp.humboldt.edu/"
  },
  {
    "objectID": "posts/2022-02-23-raster-analysis/index.html#introducing-the-terra-package",
    "href": "posts/2022-02-23-raster-analysis/index.html#introducing-the-terra-package",
    "title": "Basic Raster Analysis with R",
    "section": "2. Introducing the Terra package",
    "text": "2. Introducing the Terra package\n\nTerra is set to replace the extremely popular Raster package. It is written by the same group of developers at r spatial, led by Robert Hijmans. This means that Terra is compatible with Raster - with some serious advantages.\nAdvantages of Terra over Raster. It‚Äôs faster - unlike Raster, Terra is mostly written in C++, making it much faster for many operations.\nIt‚Äôs simpler - Terra does away with the complex data structure of Raster like RasterLayer, RasterStack and RasterBrick. We will go into this later.\nFor now, lets install and load Terra.\n\ninstall.packages(\"terra\")\nlibrary(terra)"
  },
  {
    "objectID": "posts/2022-02-23-raster-analysis/index.html#creating-importing-and-exporting-rasters",
    "href": "posts/2022-02-23-raster-analysis/index.html#creating-importing-and-exporting-rasters",
    "title": "Basic Raster Analysis with R",
    "section": "3. Creating, Importing and exporting rasters",
    "text": "3. Creating, Importing and exporting rasters\nThe following examples are largely based on examples from the official manual\nThe rast() function is used to create and import SpatRasters.\nTo create a SpatRaster from scratch:\n\nx &lt;- rast(nrows=108, ncols=108, xmin=0, xmax=10, ymin = 0, ymax = 10)\nvalues(x) &lt;- 1:ncell(x)\n\nAnd, probably more useful, to import a raster from a file:\n\nf &lt;- system.file(\"ex/meuse.tif\", package=\"terra\") #example data within terra\nr &lt;- rast(f)\n\nBut instead of system.file() (which is looking inside the terra package), point directly to a raster file.\n\nplot(r)\n\n\n\n\nTo write a SpatRaster to file, we can use the the writeRaster function:\n\nx &lt;- writeRaster(r, \"output.tif\", overwrite=TRUE)\nx"
  },
  {
    "objectID": "posts/2022-02-23-raster-analysis/index.html#dealing-with-coordinate-reference-systems",
    "href": "posts/2022-02-23-raster-analysis/index.html#dealing-with-coordinate-reference-systems",
    "title": "Basic Raster Analysis with R",
    "section": "4. Dealing with coordinate reference systems",
    "text": "4. Dealing with coordinate reference systems\nGetting the coordinate reference systems correct is a very important, and sometimes tricky, aspect of geospatial analysis.\nThere are two main classes of coordinate reference systems, have a look here for a pretty good.\nAngular coordinate reference systems - these represent the vertical and horizontal angles between the point on the surface and the center of the earth (see figure).\n Image reference: https://rspatial.org/terra/\nTo get location using an angular CRS, we require a pair of coordinates and a reference datum; a model of the shape of the earth. WGS84 is probably the most widely used global datum, where GDA94 / 2020 are commonly used Australian datums.\nProjected coordinate reference system - here, angular coordinates have been converted to a Cartesian system, making it is easier to make maps and calculate area etc. These require a projection, a datum and a set of parameters. Projections include Mercator, UTM and Lambert.\nDefining a CRS in Terra\nTerra recommends using the EPSG database, as PROJ.4 is no longer fully supported.\nTo look up an EPSG code, go to https://epsg.org/ and find your CRS.\nWhen you know the EPSG code associated with your data, you can assign it:\n\ncrs(x) &lt;- \"EPSG:27561\" \n\nBe careful, this defines the CRS but doesn‚Äôt change the underlying data. It is not generally recommended that you project raster data because it results in a loss of precision."
  },
  {
    "objectID": "posts/2022-02-23-raster-analysis/index.html#naming-and-subsetting-spatraster-layers",
    "href": "posts/2022-02-23-raster-analysis/index.html#naming-and-subsetting-spatraster-layers",
    "title": "Basic Raster Analysis with R",
    "section": "5. Naming and subsetting SpatRaster layers",
    "text": "5. Naming and subsetting SpatRaster layers\nNaming the layers of a SpatRaster is pretty straightforward, using the names() function.\n\ns &lt;- rast(system.file(\"ex/logo.tif\", package=\"terra\"))\nnames(s)\n\n[1] \"red\"   \"green\" \"blue\" \n\nnames(s) &lt;- c(\"a\", \"b\", \"c\")\nnames(s)\n\n[1] \"a\" \"b\" \"c\"\n\n\nSub-setting the layers of a SpatRaster is also a pretty simple operation.\nYou can either use square bracket notation [], the subset() function, or $ notation.\nFrom the manual, here is an example using the R logo:\n\nsubset(s, 2:3) #will select band 2 and 3\n#because we changed the names from red, green, blue to a, b c. \ns[[c(\"a\", \"b\")]] #will also select band 2 and 3\ns$a #will select the a (red) band\n\nNote that unlike with Raster, there is no need for different classes like Raster stacks/layers/bricks."
  },
  {
    "objectID": "posts/2022-02-23-raster-analysis/index.html#raster-summaries",
    "href": "posts/2022-02-23-raster-analysis/index.html#raster-summaries",
    "title": "Basic Raster Analysis with R",
    "section": "6. Raster summaries",
    "text": "6. Raster summaries\nNow that we know how to import a Raster, define its CRS, and select the bands we are interested in, its a good time to start investigating its values.\nThe global() function can be used to extract values like the average, mean and max cell values. Using the elevation data we imported earlier, we could work out the highest cell on the map with:\n\nmax_h &lt;- global(r, \"max\", na.rm=TRUE)\nmax_h\n\n      max\ntest 1736\n\n\nThe humble histogram is another useful tool when getting a handle on raster data. Terra allows you to create a frequency distribution histogram with hist():\n\nhist(r)\n\n\n\n\nOr if a boxplot is more your style, you can use boxplot():\n\nboxplot(r)"
  },
  {
    "objectID": "posts/2022-02-23-raster-analysis/index.html#raster-data-manipulation",
    "href": "posts/2022-02-23-raster-analysis/index.html#raster-data-manipulation",
    "title": "Basic Raster Analysis with R",
    "section": "7. Raster data manipulation",
    "text": "7. Raster data manipulation\nThere are a huge number of functions within Terra for data manipulation, but here are a few that might be useful. ### Extend and trim Often, we will find a lot of white space consisting of NA‚Äôs around our Raster. To remove this, we can use the trim() function. Conversely, to add white space to a raster (say, to match the extent of another Raster), we can run the extend() function.\nTo demonstrate this functionality, we will first add a lot of white space to our elevation raster using extend, before removing it with trim.\n\nextended &lt;- extend(r, 50)\ntrimmed &lt;- trim(extended)\n\n\nAggregate and Resample\nThese are used to change the resolution of a SpatRaster.\nAggregate creates a new SpatRaster with a lower resolution. Users need to provide the factor by which the raster will be reduced.\n\nagg &lt;- aggregate(r, fact = 5, fun = 'mean')\nplot(agg)\n\n\n\n\nYou can see that the resolution has been reduced, by a factor of five in this case.\nIn reality, we will often need to combine rasters from different sources that have different origins and resolutions; this will require us to match the resolution, the origin and the extent. For this, the resample function is the way to go. To demonstrate this, we can first change the origin of the raster we just aggregated.\n\norigin(agg) &lt;- c(10.5, 10.5)\n\nThen we can resample the original raster, r, using the new agg raster with different resolution and origin.\n\nrsm &lt;- resample(r, agg, method= 'bilinear')\n\n\n\nCrop\nCropping is one of the most widely used operations when working with Raster data. To demonstrate a simple crop, we will need to use a SpatVector: the other major data class used by Terra.\nOn the data set we have been working on, we will first use the spatSample() function, another handy tool.\nHere, we will randomly generate one point on the elevation raster.\n\nsamp &lt;- spatSample(r, 1, as.points=TRUE, na.rm=TRUE)\n\nNow we can make a buffer centered on this point using the buffer() function.\n\nbuf &lt;- buffer(samp, width = 200)\nplot(r)\nplot(samp, add=TRUE)\nplot(buf, add=TRUE)\n\n\nNow we can crop the elevation raster to the buffered area.\n\ncropped &lt;- crop(r, buf)\nplot(cropped)\n\n\n\n\nMask\nNotice that the buffer was a circle, but the cropped area is square. Why? Because the crop command uses the extent of the object which is always a rectangle. If you wanted to maintain the shape of the buffer, you will want to use mask()\n\nmask &lt;- mask(r, buf)\nmask &lt;- trim(mask) #we can trim down all the NA values using the trim function\nplot(mask)\n\n\n\n\nStretch\nAnother task is to stretch values to a given range. For example, classification can require data that is normalised to 8bit (0-255). This can be handy if you want to normalise rasters on different scales, such as elevation in m AGL and reflectance in DN.\nIn terra, this is as easy as:\n\nstr &lt;- stretch(r) #defaults to 0-255\nglobal(str, \"range\", na.rm=TRUE)\n\n     range max\ntest     0 255\n\n\n\n\nFocal\nThe focal() function can be used to clean and smooth rasters. Focal() uses a moving window with size w and a function to average neighboring cells. Lets do that with the elevation dataset:\n\nf &lt;- focal(r, w=5, fun=\"mean\")\nplot(f)"
  },
  {
    "objectID": "posts/2022-02-23-raster-analysis/index.html#compatibility-between-raster-and-terra",
    "href": "posts/2022-02-23-raster-analysis/index.html#compatibility-between-raster-and-terra",
    "title": "Basic Raster Analysis with R",
    "section": "8. Compatibility between Raster and Terra",
    "text": "8. Compatibility between Raster and Terra\nIf you have grow acustomed to using Raster, don‚Äôt worry, it is not difficult switch between Raster classes (RasterLayer, RasterStack etc) and SpatRaster using the rast() function.\nHere is an example.\nFirst install the Raster packages, which is available on CRAN.\n\ninstall.packages(\"raster\")\nlibrary(raster)\n\nNow, using the sample data loaded into Raster, we can create a Raster stack of the r logo.\n\nstac &lt;- stack(system.file(\"ex/logo.tif\", package=\"terra\")) #This is a raster stack\nrst &lt;- rast(stac) #now this is a SpatRaster\n\nIts that simple. And to change this back to a Raster object:\n\nstac &lt;- raster(rst) #now this is a RasterStack"
  },
  {
    "objectID": "posts/2022-02-23-raster-analysis/index.html#options-to-allow-the-processing-of-large-files",
    "href": "posts/2022-02-23-raster-analysis/index.html#options-to-allow-the-processing-of-large-files",
    "title": "Basic Raster Analysis with R",
    "section": "9. Options to allow the processing of large files",
    "text": "9. Options to allow the processing of large files\nTerra has some settable options that can help to improve performance. Have a look through them with ?terraOptions\nParticularly useful options include tempdir, which provides a default location for files to be written. This can help prevent your C drive being clogged with temp files.\nAlso handy is memfrac, which lets us stipulate how much RAM terra is can use - from 0-0.9.\nCheck the current options with terraOptions()\nSet options using terraOptions(memfrac=0.2, tempdir = \"C:/temp/terrafiles\")."
  },
  {
    "objectID": "posts/2022-02-23-raster-analysis/index.html#a-real-world-example-with-drone-data",
    "href": "posts/2022-02-23-raster-analysis/index.html#a-real-world-example-with-drone-data",
    "title": "Basic Raster Analysis with R",
    "section": "10. A real world example with drone data",
    "text": "10. A real world example with drone data\nIn this example, we will use some of my drone data collected from savanna woodlands in north Australia.This is a typical example where we have an orthomosaic, which was derived from a camera (RGB), and a canopy height model, which was derived from LiDAR point cloud.\nFirst, we need to reduce the size of the orthomosaic (1.3+ GB is too big). But in reality, we might want that much resolution depending on what we are trying to do.\nYou don‚Äôt need to run the below code, but this is what I did to reduce down the large orthomosaic.\n\northopath &lt;- \".../hacky_sac/2022-02-23 Basic raster analysis with R/plot1_ortho.tif\"\northo_reduced_path &lt;- \"...2022-02-23 Basic raster analysis with R/plot1_ortho_reduced.tif\"\northo &lt;- rast(orthopath)\nres(ortho) #check the resolution \n\n#now lets reduce it down with aggregate \n\n#note the filename variable lets us write directly to file\naggregate(ortho, fact = 10, fun = 'mean', filename = ortho_reduced_path)\northo_reduced &lt;- rast(ortho_reduced_path)\nres(ortho_reduced)\n\nDownload the reduced orthomosaic here\nAnd the chm here\nThe first thing to do is load the .tifs as SpatRasters in R.\n\nhomefolder &lt;- \"..../Downloads\" #where did you download the files to?\nchm_path &lt;- paste0(homefolder, \"/\", \"plot1_chm.tif\")\northo_path &lt;- paste0(homefolder, \"/\", \"plot1_ortho_reduced.tif\")\n\n#now read in the files using terra\nchm &lt;- rast(chm_path)\northo &lt;- rast(ortho_path)\n\nThis is an example of needing to merge rasters with different geometries into a single raster. Before we can do this, we need to make sure the crs, resolution, origin and extent match.\n\n#do the crs match?\ncrs(chm) == crs(ortho)\n#do the origins match?\norigin(chm) == origin(ortho)\n#do the resolutions match?\nres(chm) == res(ortho)\n#do the extents match?\next(chm) == ext(ortho)\n\nWe should see that the CRS matches, but nothing else.\nThe resample() function can match the geometries. If we wanted to keep the higher resolution of the orthomosaic, we can run:\n\nchm_resampled &lt;- resample(chm, ortho, method = \"bilinear\")\n#now, geometry should match\nres(chm_resampled) == res(ortho)\norigin(chm_resampled) == origin(ortho)\next(chm_resampled) == ext(ortho)\n\nOn the right track. lets plot them to have a look.\n\nplot(chm_resampled)\n\n\n\nplot(ortho)\n\n\nWe can see that even though they have the same extent - as enforced by the resample() - there are a lot of blank cells around the CHM.\nLets get rid of those blank (NA) cells using the trim() function.\n\nchm_resampled &lt;- trim(chm_resampled)\nplot(chm_resampled)\n\n\nThat‚Äôs better, now lets crop the orthomosaic using the CHM.\n\northo_cropped &lt;- crop(ortho, chm_resampled)\n\nNow everything should match, and we can finally combine these layers into a single SpatRaster object. For this, you can simply concatenate the layers: c(layer1, layer2, layer3).\n\ncombined &lt;- c(ortho_cropped, chm_resampled)\nplot(combined)\n\n\nThe band names don‚Äôt make a lot of sense at this point, so we can rename them using the names() function.\n\nnames(combined) &lt;- c('red', 'green', 'blue', 'chm')\n\nFinally, prior to doing some analysis, we might want to stretch the chm to 8bit - so its not under-weighted compared to the RGB data. This calls for the stretch() function.\n\ncombined$chm &lt;- stretch(combined$chm)\nplot(combined)\n\n\nNow we have a raster stack we can work with!"
  },
  {
    "objectID": "posts/2022-02-23-raster-analysis/index.html#a-fun-new-package-called-layer",
    "href": "posts/2022-02-23-raster-analysis/index.html#a-fun-new-package-called-layer",
    "title": "Basic Raster Analysis with R",
    "section": "11. A fun new package called Layer",
    "text": "11. A fun new package called Layer\nOne fun little package I stumbled on recently is called layer. It doesn‚Äôt really serve a purpose in terms of analysis, although I‚Äôll use it to make figures that demonstrate the raster data-sets used in analysis.\n\nUnfortunately, Layer calls for Raster data, but as we know, it‚Äôs easy to convert Raster to SpatRaster objects.\nBut before we get into that, lets further reduce the resolution of the combined SpatRaster, as there is a lot going on in the background of Layer and its pretty slow.\n\ncombined_lowres &lt;- aggregate(combined, fact = 10) #10x less rows and columns\n\nNow we can load the layer and raster packages:\n\ninstall.packages(\"layer\")\nlibrary(layer)\n\nNow we can provide the layers to tilt_map, then plot_tilted_map, to create a nice looking tilted stack that illustrates our data. Note within the tilt_map function, we convert the SpatRaster to Raster with the raster() function.\n\ntilt_map_1 &lt;- tilt_map(raster(combined_lowres$red))\ntilt_map_2 &lt;- tilt_map(raster(combined_lowres$green), x_shift = 0, y_shift = 50)\ntilt_map_3 &lt;- tilt_map(raster(combined_lowres$blue), x_shift = 0, y_shift = 100)\ntilt_map_4 &lt;- tilt_map(raster(combined_lowres$chm), x_shift = 0, y_shift = 150)\n\nmap_list &lt;- list(tilt_map_1, tilt_map_2, tilt_map_3, tilt_map_4)\n\nplot_tiltedmaps(map_list, \n                layer = c(\"value\", \"value\", \"value\", \"value\"),\n                palette = c(\"bilbao\", \"mako\", \"rocket\", \"turbo\"),\n                color = \"grey40\")"
  },
  {
    "objectID": "posts/2022-03-31-spatial-networks/index.html",
    "href": "posts/2022-03-31-spatial-networks/index.html",
    "title": "Introduction to tidy spatial networks",
    "section": "",
    "text": "Written and presented by St√©phane Guillou Technology Trainer, UQ Library Mastodon: @stragu@mastodon.indie.host\nThis tutorial is an introduction to dealing with tidy spatial networks in R, demonstrating a full process of data acquisition from the open spatial database OpenStreetMap, data preparation, and basic network analysis like isodistance and shortest path calculation. Along the way, we use the default plotting methods for spatial network objects, but also make use of the ggplot2 and tmap packages as alternatives."
  },
  {
    "objectID": "posts/2022-03-31-spatial-networks/index.html#setting-up",
    "href": "posts/2022-03-31-spatial-networks/index.html#setting-up",
    "title": "Introduction to tidy spatial networks",
    "section": "Setting up",
    "text": "Setting up\nTo follow this tutorial, you will have to have a number of packages available, which can be best sorted out with the following command:\n\ninstall.packages(c(\"dplyr\", \"ggplot2\", \"sfnetworks\", \"tmap\", \"osmdata\"))\n\nIf you run into issues installing and running sf (which relies on spatial libraries external to R), please refer to their installation instructions."
  },
  {
    "objectID": "posts/2022-03-31-spatial-networks/index.html#sfnetworks",
    "href": "posts/2022-03-31-spatial-networks/index.html#sfnetworks",
    "title": "Introduction to tidy spatial networks",
    "section": "sfnetworks",
    "text": "sfnetworks\nThe main package this tutorial centres around is the sfnetworks package (Lucas van der Meer, Lorena Abad et al.), which is the result of joining what the tidygraph package does for networks with what the sf package does for spatial vector data.\nThe data structure central to this package is a combination of two spatial objects, one describing the nodes (or points) and another describing the edges (or lines connecting the points). In the words of the developers:\n\n‚ÄúA close approximation of tidyness for relational geospatial data is two sf objects, one describing the node data and one describing the edge data.‚Äù\n\nIn other words, an sfnetwork object is made up of one sf object for nodes and one sf object for edges. (As opposed to two simple data.frames for a tidygraph object.)\n\n\n\nA visual explanation of sf objects by Allison Horst (copyright hers): a dataframe with sticky geometries in a list column. The geometries are ‚Äústicky‚Äù because they don‚Äôt vanish when, say, you select() columns.\n\n\nIn the edges component of a network, it is also possible (or necessary) to specify where the edge starts and finishes, using the from and to columns. Such information can be interpreted as the direction of the edge in a ‚Äúdirected‚Äù network, or simply as its two extremities in the case of an ‚Äúundirected‚Äù network.\nFinally, another important characteristic of the edges is their weight. This can relate to any kind of information, but if we are interested in distances between points, it will typically be the length of the edges."
  },
  {
    "objectID": "posts/2022-03-31-spatial-networks/index.html#from-osm-data-to-an-sfnetwork",
    "href": "posts/2022-03-31-spatial-networks/index.html#from-osm-data-to-an-sfnetwork",
    "title": "Introduction to tidy spatial networks",
    "section": "From OSM data to an sfnetwork",
    "text": "From OSM data to an sfnetwork\nAlthough the structure of an sfnetwork seems to be quite complex, it is possible ‚Äì and useful! ‚Äì to use a shortcut and create one from a simple collection of linestrings.\nWe can download download data from a spatial vector database like OpenStreetMap (OSM), focus on the lines (or ‚Äúways‚Äù) and convert that sf object into an sfnetwork that will use the lines as edges, and their endpoints as nodes.\nLet‚Äôs first download from OSM all the ways that are primary used by pedestrians in the suburb of West End. (To understand how these features are tagged in the database, a good starting point is the Map Features page on the OSM wiki.)\n\nlibrary(osmdata)\n\n\n# build the query\nwe_foot &lt;- opq(\"west end, meanjin\") %&gt;% \n  add_osm_features(features = c('\"highway\"=\"footway\"',\n                                '\"highway\"=\"steps\"',\n                                '\"foot\"=\"yes\"',\n                                '\"highway\"=\"living_street\"')) %&gt;% \n  osmdata_sf()\n\nThe osmdata package allows building Overpass queries to download OSM vector data that matches our criteria:\n\nFeatures tagged as highway=footway, including footpaths, crossings‚Ä¶\nFeatures tagged as highway=steps, for stairs\nFeatures tagged as foot=yes, for example bikeways on which foot traffic is allowed (see this shared bikeway for an example)\nFeatures tagged as highway=living_street, a shared road on which pedestrians often have right of way\n\nFinding the right combination of OSM tags to use is always an iterative process, refining the selection at each step.\nWe choose to return an sf object, which might contain points, lines and polygons. We are only interested in the lines:\n\nnames(we_foot)\n\n[1] \"bbox\"              \"overpass_call\"     \"meta\"             \n[4] \"osm_points\"        \"osm_lines\"         \"osm_polygons\"     \n[7] \"osm_multilines\"    \"osm_multipolygons\"\n\nwe_foot_lines &lt;- we_foot$osm_lines\n\nWe can visualise the resulting object with the sf package:\n\nlibrary(sf)\nwe_foot_lines %&gt;%\n  st_geometry() %&gt;% \n  plot()\n\n\n\n\nIt seems to have kept a ferry route, which was tagged with foot=yes and route=ferry on OSM. We can remove it:\n\nlibrary(dplyr)\nwe_foot_lines &lt;- we_foot_lines %&gt;% \n  filter(is.na(route))\n\nAnother caveat in our process is that some footpaths are mapped as closed ways on OSM, and are therefore returned as a polygon:\n\nwe_foot$osm_polygons %&gt;% \n  st_geometry() %&gt;% \n  plot()\n\n\n\n\nThe smaller ones are proper pedestrian areas, but the bigger ones are probably footpaths encircling a whole residential block. It is possible to ‚Äúcast‚Äù these polygons as ways and include them in the network:\n\n# cast polygons to lines\npoly_to_lines &lt;- st_cast(we_foot$osm_polygons, \"LINESTRING\")\n# bind all lines together\nlibrary(dplyr)\nwe_foot_lines &lt;- bind_rows(we_foot_lines, poly_to_lines)\n\nThis is what we are left with:\n\n# plot it\nwe_foot_lines %&gt;% \n  st_geometry() %&gt;% \n  plot()\n\n\n\n\nWe can now convert the object to an sfnetwork object, making sure we set it as undirected (the default is directed = TRUE):\n\nlibrary(sfnetworks)\nfoot_net &lt;- as_sfnetwork(we_foot_lines, directed = FALSE)\nplot(foot_net)"
  },
  {
    "objectID": "posts/2022-03-31-spatial-networks/index.html#prepare-the-network",
    "href": "posts/2022-03-31-spatial-networks/index.html#prepare-the-network",
    "title": "Introduction to tidy spatial networks",
    "section": "Prepare the network",
    "text": "Prepare the network\n\nCRS\nThe current coordinate system for the network is a global one, EPSG:4326.\n\nst_crs(foot_net)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nst_transform() allows us to transform the coordinates to a different projection. For this part of the world, a recent Projected Reference System is EPSG:7856 (or ‚ÄúGDA2020 / MGA zone 56‚Äù).\n\nfoot_net &lt;- st_transform(foot_net, 7856)\n\nIf the command above gives you a GDAL error, reassign the original CRS first: st_crs(foot_net) &lt;- 4326\n\n\nClean up\nsfnetworks and tidygraph include many pre-processing and cleaning functions for graphs, some of them detailed in this article.\nOne relevant example for this dataset is the subdivision of edges: because some edges have interior nodes that are endpoints of other edges, they will not be connected to each other when analysing the network.\n We need to combine tidygraph‚Äôs convert() function with a ‚Äúspatial morpher‚Äù function from sfnetworks:\n\nlibrary(tidygraph)\nfoot_net &lt;- convert(foot_net, to_spatial_subdivision)\n\nThe network should now contain more edges.\nNote that this will only happen for features sharing a node: if two edges overlap, like for bridges and underpaths, no extra node will be created where they cross, and therefore no new connecting endpoints will be created for them ‚Äì which is good news!\nAlso note that using to_spatial_subdivision() will copy tags into each of the subdivisions of an edge. This usually isn‚Äôt a problem (think speed limits, surface, lighting, width‚Ä¶) but can be in some cases (for example if the length of the edge had already been added). The order of processing often matters.\nAnother example is the removal of nodes that have only two edges connected, also called ‚Äúsmoothing of pseudo-nodes‚Äù. This would be useful to simplify a network and reduce the processing times needed. We again use a combination of convert() with the relevant spatial morpher, to_spatial_smooth():\n\nfoot_simple &lt;- convert(foot_net, to_spatial_smooth)\nplot(foot_simple)\n\n\n\n\nFor our example, simplifying the network might not be useful:\n\nIf we calculate isodistances or isochrones, reducing the number of nodes will reduce the precision;\nOnce again, one needs to take care of the potential loss of valuable data. For example, do nodes contain relevant tags, like ones describing physical barriers? And how will the combined edges‚Äô tags be stored?\n\nMore information about ‚Äúspatial morphers‚Äù, including what options exist for dealing with attributes when multiple features are merged, is available in the documentation: ?spatial_morphers\n\n\nWeights\nAs mentioned before, a common weight associated with each edge of the network is the edge‚Äôs length. We can add this weight to our network, but we need to first ‚Äúactivate‚Äù the part of the object we want to modify:\n\nfoot_net &lt;- foot_net %&gt;% \n  activate(\"edges\") %&gt;% \n  mutate(weight = edge_length())\n\nUsing st_as_sf(), we can extract the components of an sfnetwork object and use them in a familiar plotting system like ggplot2:\n\nlibrary(ggplot2)\nggplot() +\n  geom_sf(data = st_as_sf(foot_net, \"edges\"),\n          mapping = aes(colour = as.numeric(weight))) +\n  labs(colour = \"Edge length (m)\")"
  },
  {
    "objectID": "posts/2022-03-31-spatial-networks/index.html#interactive-map",
    "href": "posts/2022-03-31-spatial-networks/index.html#interactive-map",
    "title": "Introduction to tidy spatial networks",
    "section": "Interactive map",
    "text": "Interactive map\nAt this point, it might be interesting to create an interactive visualisation to zoom into, which might help spot issues with the data.\n\nlibrary(tmap)\ntmap_mode(\"view\") # set to interactive mode\ntm_tiles(\"CartoDB.Positron\") +\ntm_shape(st_as_sf(foot_net, \"edges\")) +\n  tm_lines(col = \"footway\", palette = \"Accent\", colorNA = \"red\") +\ntm_shape(st_as_sf(foot_net, \"nodes\")) +\n  tm_dots()\n\n\n\n\n\n\n\nRemove small disconnected neighbourhoods\nThere are a few ‚Äúislands‚Äù in the network, in which not many points are connected to each other. We can remove those, by:\n\nChoosing how far we look to determine the size of a neighbourhood, with tidygraph‚Äôs local_size() function\nOnly keeping the neighbourhoods that have reached that threshold.\n\n\nfoot_net &lt;- foot_net %&gt;% \n  activate(nodes) %&gt;% \n  mutate(neighbourhood = local_size(order = 6)) %&gt;% \n  filter(neighbourhood &gt; 5)"
  },
  {
    "objectID": "posts/2022-03-31-spatial-networks/index.html#isodistances",
    "href": "posts/2022-03-31-spatial-networks/index.html#isodistances",
    "title": "Introduction to tidy spatial networks",
    "section": "Isodistances",
    "text": "Isodistances\nLet‚Äôs now draw an isodistance around the Kurilpa Library in West End. First, download the library‚Äôs feature from OSM, and calculate its centroid (because it is mapped as a building):\n\nkurilpa_lib &lt;- opq_osm_id(id = 523925261, type = \"way\") %&gt;% \n  osmdata_sf() %&gt;% \n  .$osm_polygons %&gt;% \n  st_centroid() %&gt;% \n  st_set_crs(4326) %&gt;% # (if the following step generate GDAL error)\n  st_transform(crs = 7856)\n\nThen, calculate the isodistance for a distance smaller or equal to 1 km (more or less a 15-minute walk), using the node_distance_from() function from tidygraph:\n\nfoot_net &lt;- activate(foot_net, \"nodes\")\niso &lt;- foot_net %&gt;%\n  dplyr::filter(node_distance_from(st_nearest_feature(kurilpa_lib, foot_net), weights = as.numeric(weight)) &lt;= 1000)\n\nFinally, draw a polygon around the isodistance, and plot everything:\n\niso_poly &lt;- iso %&gt;%\n  st_geometry() %&gt;%\n  st_combine() %&gt;%\n  st_convex_hull()\nplot(foot_net, col = \"grey\")\nplot(iso_poly, col = NA, border = \"black\", lwd = 3, add = TRUE)\nplot(iso, col = \"lightgreen\", add = TRUE)\nplot(kurilpa_lib, col = \"red\", pch = 8, cex = 2, lwd = 2, add = TRUE)"
  },
  {
    "objectID": "posts/2022-03-31-spatial-networks/index.html#shortest-path",
    "href": "posts/2022-03-31-spatial-networks/index.html#shortest-path",
    "title": "Introduction to tidy spatial networks",
    "section": "Shortest path",
    "text": "Shortest path\nLet‚Äôs now calculate the shortest distance from the South Brisbane Sailing Club to the library.\n\n# get the location of the Sailing Club's entrance\nsailing_club &lt;- opq_osm_id(id = 7622867925, type = \"node\") %&gt;% \n  osmdata_sf() %&gt;% \n  .$osm_points %&gt;%\n  st_set_crs(4326) %&gt;% # (if the following step generates a GDAL error)\n  st_transform(crs = 7856)\n\n# calculate the shortest path\nshortest &lt;- foot_net %&gt;% \n  activate(edges) %&gt;% \n  st_network_paths(from = sailing_club, to = kurilpa_lib)\n\n# extract the node IDs\nnode_path &lt;- shortest %&gt;%\n  slice(1) %&gt;%\n  pull(node_paths) %&gt;%\n  unlist()\n\n# only keep the network for these nodeIDs\npath_sf &lt;- foot_net %&gt;% \n  activate(nodes) %&gt;% \n  slice(node_path) %&gt;% \n  st_as_sf(\"edges\")\n\n# visualise\ntm_tiles(\"CartoDB.Positron\") +\ntm_shape(path_sf) +\n  tm_lines(col = \"red\") +\ntm_shape(sailing_club) +\n  tm_dots(col = \"blue\", size = 0.1) +\ntm_shape(kurilpa_lib) +\n  tm_dots(col = \"green\", size = 0.1)\n\n\n\n\n\n\nThe granularity of the OSM data available for this area allows us to create a precise path for pedestrians, switching sides of roads only when a crossing is reached. However, every new analysis of the network might reveal more interesting information about the data we have used. For example, the shortest path calculated might go through private residential paths (usually blocked by gates), and avoiding those would mean extra steps in pre-processing (for example keeping track of existing OSM tags like barrier=* on nodes and access=private on ways)."
  },
  {
    "objectID": "posts/2022-03-31-spatial-networks/index.html#further-resources",
    "href": "posts/2022-03-31-spatial-networks/index.html#further-resources",
    "title": "Introduction to tidy spatial networks",
    "section": "Further resources",
    "text": "Further resources\n\nsfnetworks‚Äô documentation, including its 5 articles, which inspired much of this tutorial\ntidygraph‚Äôs gigantic collection of graph manipulation functions\nTransportation chapter in the free Geocomputation with R book (Lovelace, Nowosad, Muenchow)\nSpatial network analysis with the {sfnetworks} package, video on analysing OSM data with sfnetworks, by Renate Thiede, for R-Ladies Johannesburg (2021)\nSpatial networks in R with sf and tidygraph, article by van der Meer, Abad and Lovelace (2019)\nOther related packages:\n\nggraph for graph visualisation\nstplanr for sustainable transport planning and modeling\ncppRouting for calculating distances, shortest paths and isochrones/isodistances\ndodgr for distance and time calculations on directed graphs (and its vignette on street network routing based on OSM data)\nosrm for using a routing API based on OSM data"
  },
  {
    "objectID": "posts/2022-03-31-spatial-networks/index.html#legal",
    "href": "posts/2022-03-31-spatial-networks/index.html#legal",
    "title": "Introduction to tidy spatial networks",
    "section": "Legal",
    "text": "Legal\nApart for illustrations in which the copyright is mentioned, this article is released under a CC-BY 4.0 licence. All data used for the data visualisations use OpenStreetMap data, which is ¬© OpenStreetMap contributors but released under an ODBL licence."
  },
  {
    "objectID": "posts/2022-03-31-spatial-networks/index.html#updates",
    "href": "posts/2022-03-31-spatial-networks/index.html#updates",
    "title": "Introduction to tidy spatial networks",
    "section": "Updates",
    "text": "Updates\nThis article was updated after it was presented:\n\n2022-04-27:\n\nClarify information about overlapping ways being left alone by to_spatial_subdivision()\nAdd Renate Thiede‚Äôs excellent video on the exact same topic, discovered after finishing writing\nAdd author‚Äôs role and contact"
  },
  {
    "objectID": "posts/2022-03-31-spatial-networks/index.html#about-the-author",
    "href": "posts/2022-03-31-spatial-networks/index.html#about-the-author",
    "title": "Introduction to tidy spatial networks",
    "section": "About the Author",
    "text": "About the Author\n\n \n\nSt√©phane is a software trainer at the UQ Library, specialising in open research software and data analysis support. After a few years working in plant science and agricultural research, he joined the great Library team where he is able to promote Open Research practices to the wider UQ community."
  },
  {
    "objectID": "posts/2022-04-28-cloud-computing-with-open-data-cube-and-python/index.html",
    "href": "posts/2022-04-28-cloud-computing-with-open-data-cube-and-python/index.html",
    "title": "Cloud computing with Open Data Cube and Python",
    "section": "",
    "text": "This workshop broadly covered remote sensing data analysis using the Open Data Cube (ODC), developed by Geoscience Australia. We went over some important Python packages that the ODC is built upon: Numpy, Xarray, and Dask, and explored how they enable fast and scalable computation.\nAll the learning material used in this tutorial is available on the Digital Earth Australia Sandbox as Jupyter Notebooks. An account was required to participate, which can easily be created here.\nAdditional documentation for the DEA Sandbox is available here which includes useful guides, a dataset catalogue and examples.\nIf you missed the demonstration, the recording is available on our cloudstor site, email mitchel.rudge@uq.edu.au to get access.\n\nAbout the presenter\nThe workshop was be presented by Tim Devereux.\n\n\n\n \n\nTim is a PhD candidate with the UQ Remote Sensing Research Centre (RSRC), and has a background in Environmental and Computational Sciences. His research is focused on the development of high fidelity digital representations of Australian forests for next generation simulations. He is also a demonstrator for the SEES advanced remote sensing course at UQ."
  },
  {
    "objectID": "posts/2022-05-26-problem-solving-session/index.html",
    "href": "posts/2022-05-26-problem-solving-session/index.html",
    "title": "Problem Solving Session I",
    "section": "",
    "text": "We did something different for May: a problem solving session!\n\nHere, people emailed in some questions which we went through and discussed in the workshop.\nIt seemed to go pretty well, so we will probably use this format from time to time.\nBelow, we have the two main questions that were discussed, attempted solutions"
  },
  {
    "objectID": "posts/2022-05-26-problem-solving-session/index.html#about-the-session",
    "href": "posts/2022-05-26-problem-solving-session/index.html#about-the-session",
    "title": "Problem Solving Session I",
    "section": "",
    "text": "We did something different for May: a problem solving session!\n\nHere, people emailed in some questions which we went through and discussed in the workshop.\nIt seemed to go pretty well, so we will probably use this format from time to time.\nBelow, we have the two main questions that were discussed, attempted solutions"
  },
  {
    "objectID": "posts/2022-05-26-problem-solving-session/index.html#question-1---rasterizing-field-size-information-per-farm.",
    "href": "posts/2022-05-26-problem-solving-session/index.html#question-1---rasterizing-field-size-information-per-farm.",
    "title": "Problem Solving Session I",
    "section": "Question 1 - Rasterizing field size information per farm.",
    "text": "Question 1 - Rasterizing field size information per farm.\n\nThe question.\nI have a geospatial problem to crop field mapping.I am using household survey data that includes a section on crop fields, their location, their area, use, distance to homestead and other things. An example for one country, Ethiopia, is here.\nFor privacy reasons I cannot get the GPS coordinates of each crop field although this data was collected. The household GPS locations are easier to access.\nI try to think through a process of calculating field size per raster grid cell that intersects with a circle around a household that is defined by the maximum field-homestead distance. I know that all the fields have to be within that circle and I know the mean field size and the distribution of field sizes within that circle. I was thinking to just assume that every grid cell in the circle has the mean field size of that circle with an error range based on the field size distribution. But there are also overlaps between circles and I don‚Äôt just want to calculate averages in this case.\nDo you know of a similar geospatial problem that someone has solved already that I could look into to get some ideas? From another field outside agriculture even?\n\n\n\nCreate some dummy data\nIn this problem, we have point data (the households), with attributes related to the crop fields that are associated with that household, such as their area, use and distance to homestead. One of those attributes, maximum distance of field to homestead, was used to generate the buffers (red circles). But because maximum distance doesn‚Äôt perfectly capture the distance of fields to homesteads, they overlap. This complicates the process of rasterising the buffers.\nTo explore our options, we will use the Terra R package.\nFirst, we will create some dummy data.\n\nlibrary(terra)\n\nterra 1.7.18\n\nr &lt;- rast(ncols=10, nrows=10, xmin = 1, xmax = 1.1, ymin = 1, ymax = 1.1)\nvalues(r) &lt;- sample(1:ncell(r), ncell(r))\n\nThen create some random points within our dummy raster (r). These represent the household locations.\n\nsamp &lt;- spatSample(r, 25, as.points=TRUE, method = \"random\")\nwidths &lt;- sample(100:1000, length(samp), replace = T)\n\nFinally, we will add buffers to each of the points, randomly sized to represent the variability in maximum distance to field from household.\n\nbuf&lt;-list()\nfor(i in 1:length(samp)) {buf[[i]] &lt;- buffer(samp[i], widths[i])}\nfields &lt;- vect(buf)\n\nAlso, add in other attributes (dmax, dmin and dmed) representing other field attributes like mean field size etc.\n\nfields$ID &lt;- 1:25 #field IDs\nfields$dmax &lt;- expanse(fields) / 1000 #based on area covered by vector. \n\nWarning: [expanse] unknown CRS. Results can be wrong\n\nfields$dmin &lt;- fields$dmax / 4 \nfields$dmed &lt;- fields$dmax / 2 \n\nSo now when we plot everything, we have essentially recreated the problem.\n\nplot(r) #plot the dummy raster\nplot(fields, add = T)\n\n\n\n\nAnd when we look at the attributes of the fields dataset, we have attributes for each field.\n\nfields\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 25, 5  (geometries, attributes)\n extent      : 0.9961141, 1.096878, 0.9973581, 1.102877  (xmin, xmax, ymin, ymax)\n coord. ref. :  \n names       : lyr.1    ID      dmax      dmin      dmed\n type        : &lt;int&gt; &lt;int&gt;     &lt;num&gt;     &lt;num&gt;     &lt;num&gt;\n values      :     5     1 1.071e-07 2.677e-08 5.354e-08\n                 100     2 6.054e-08 1.514e-08 3.027e-08\n                  23     3 1.221e-07 3.052e-08 6.104e-08\n\n\n\n\nMethod 1. go back to point data, and add rasterise points using the grid.\nThis ensures that polygons are not overlapping - 1 point per grid cell. but it removes the spatial representation of the radius.\n\ncentres &lt;- terra::centroids(fields)\nplot(r)\nplot(centres, add=TRUE)\n\n\n\n\nNow that we have converted the buffers to points, we can use the rasterize function to add the point data to the rasters - one layer per attribute.\n\ndmax_cent &lt;- rasterize(centres, r, \"dmax\", \"mean\") #rasterize the dmax attribute of the points\nnames(dmax_cent) &lt;- \"dmax_cent\" #name the dmax layer\n\ndmin_cent &lt;- rasterize(centres, r, \"dmed\", \"mean\") # rasterize the median field size attribute from the points\nnames(dmin_cent) &lt;- \"dmin_cent\" #name the dmin layer\n\ndmed_cent &lt;- rasterize(centres, r, \"dmin\", \"mean\") #rasterize the min field size from the point layers\nnames(dmed_cent) &lt;- \"dmed_cent\" #name the dmed layer\n\ncent_combined &lt;- c(dmax_cent, dmin_cent, dmed_cent) #combine the rasters into a single spatVector\n\n\npar(mfrow=c(1,3))\n\nplot(dmin_cent, xlim = c(1, 1.1))\nplot(centres, add=T)\n\nplot(dmax_cent)\nplot(centres, add=T)\n\nplot(dmax_cent)\nplot(centres, add=T)\n\n\n\n\nWe can see that each household, represented by the points, relates to a single raster grid cell. This might be enough, but the spatial information embedded in the buffers - where the buffer size is proportional to maximum distance - is lost.\nSo what options are there to preserve this information when rasterizing?\n\n\nMethod 2. add values of buffer to grid cell, and account for the proportion of overlap.\nThere has been a similar issue posted on stackoverflow which gives us some clues.\nOk, going back to our fields and our raster:\n\nplot(r)\nplot(fields, add=T)\n\n\n\n\nOne idea was to scale the field size by the fraction of the grid cell that it covers, such that a 100m max field size, that covered 5% of a grid would become 5m. Unfortunately this doesn‚Äôt make a lot of sense, as the actual size of the field hasnt changed.\nSo really what we want to do is just add the field size onto the raster. so even if its a small fraction, the average size would still be 100m.\nBut its not so simple when field buffers overlap: what happens when a 100m field and 50m field overlap within the same grid cell?\nOne approach is to reduce the cell size of the original raster, then individually rasterize all of the field polygons using this fine resolution raster, before merging the musing the mosaic function to make the grid cells much smaller than they currently are.\n\ndis &lt;- terra::disagg(r, 20) # 20x smaller cells. \ndvalue &lt;- lapply(1:nrow(fields), \\(i) rasterize(fields[i,], dis, field = \"dmax\")) #rasterize each of the fields using the fine raster  \ndvalue &lt;- terra::sprc(dvalue) #make a spatraster collection out of the list of rasterised fields \ndvalue &lt;- mosaic(dvalue) #mosaic them all together, which by default will average overlapping cells\nplot(dvalue)\nplot(fields, add=TRUE)\n\n\n\n\nNow, the original question was what is the average field size per grid cell. When we re-aggregate the data\n\npar(mfrow=c(1,2))\n\nplot(dmax_cent)\nplot(fields, add=T)\n\nplot(aggregate(dvalue, 20, mean, na.rm = TRUE))\nplot(fields, add=T)\n\n\n\n\nSo now, each cell should represent the average field size within it. This is not perfect: It might tell you where there are trends toward bigger or smaller fields, but the raster cell is not proportional to field size so be careful with summary statistics."
  },
  {
    "objectID": "posts/2022-05-26-problem-solving-session/index.html#question-2.-take-a-sample-of-a-continuous-raster-using-a-random-sample-of-a-categorical-raster.",
    "href": "posts/2022-05-26-problem-solving-session/index.html#question-2.-take-a-sample-of-a-continuous-raster-using-a-random-sample-of-a-categorical-raster.",
    "title": "Problem Solving Session I",
    "section": "Question 2. Take a sample of a continuous raster using a random sample of a categorical raster.",
    "text": "Question 2. Take a sample of a continuous raster using a random sample of a categorical raster.\n\nThe problem\nHave 2 rasters, one categorical representing classes of coral reef cover, and one continuous representing wave action.\nWanting to take a random sample of the reef cover raster (excluding NAs), then use that sample to return corresponding values of the continuous raster. The two rasters have different resolutions.\n\n\nAttempted solution\nAs always, we will start by trying to create a dummy dataset. Starting with the reef cover raster.\n\nlibrary(terra) #load the terra library, the best for raster analysis in R \n\n#make a coral cover raster\ncoral &lt;- rast(ncol=10, nrow=10, names=\"stratum\")  # a 10x10 raster\nset.seed(1)\nvalues(coral) &lt;- round(runif(ncell(coral), 1, 5)) #values 1-5, setting 5 as NA\ncoral &lt;- terra::classify(coral, cbind(5, NA)) # #make 5 NA, so the dataset has some NA\n\n#make a wave action raster\nwave &lt;- rast(ncol=20, nrow=20) \nset.seed(1)\nvalues(wave) &lt;- runif(ncell(wave), 1, 25)\nwave &lt;- terra::classify(wave, cbind(5, NA))\n\npar(mfrow=c(1,2)) #plot them side-by-side\nplot(coral)\nplot(wave)\n\n\n\n\nNow we can take a random sample of the coral cover raster using the spatSample function.\n\nrand_pts &lt;- spatSample(coral, 10, \"random\", as.points=TRUE, na.rm = TRUE) #this will sample the number from among the strata (not #NA) \nplot(coral)\nplot(rand_pts, 1, add=TRUE, plg=list(x=185, y=1, title=\"points\"))\n\n\n\n\nAnd we can see that each point relates to the stratum integer of the raster:\n\nrand_pts$stratum\n\n [1] 3 3 3 4 1 4 4 3 3 4\n\n\nAs an aside, this doesn‚Äôt seem to work when we have a named categorical raster, as opposed to a categorical raster represented by integers. As we will see if we label the categories using the levels function:\n\ncoral_named = coral-1 #values need to start at zero, not one, for the levels function. \n#the way we made the raster, they started at one. \n\nlevels(coral_named) &lt;- c(\"Rubble\", \"Coral/Algae\", \"Sand\", \"Rock\") #now we Can add our category labels in \n\nWarning: [set.cats] setting categories like this is deprecated; use a two-column\ndata.frame instead\n\nplot(coral_named)\n\n\n\n\nWe would obviously prefer our categorical raster to look like this, with names rather then integers representing the categories. But when we randomly sample it with spatSample (using the exact same code as above), we get a whole pile of NAs.\n\nrand_pts_named &lt;- spatSample(coral_named, 10, \"random\", as.points=TRUE, na.rm = TRUE) #this will sample the number from among the strata (not NA) \nrand_pts_named$stratum\n\n [1] Coral/Algae Rock        Rock        Sand        Rock        Coral/Algae\n [7] Rock        Sand        Sand        Rubble     \nLevels: Rubble Coral/Algae Sand Rock\n\n\nNot sure why this is the case and it might be a good one to put on stack overflow‚Ä¶ but for now, lets just move on knowing the integers represent the cover classes.\nSo getting back to the task - we can use the random sample from the coral layer stored in the rand_pts spatVector using rasterize:\n\ncoral_sample &lt;- rasterize(rand_pts, wave, field = \"stratum\") #rasterize the points using the wave raster as a template\n\nAnd now that we are done sampling, we can name our coral categories.\n\ncoral_sample &lt;- coral_sample -1\nlevels(coral_sample) &lt;- c(\"Rubble\", \"Coral/Algae\", \"Sand\", \"Rock\") #now we can add our category labels in\n\nWarning: [set.cats] setting categories like this is deprecated; use a two-column\ndata.frame instead\n\n\nFinally, we can use the mask function to grab the cells where we have sample values of coral:\n\nwaves_sample &lt;- mask(wave, coral_sample) #get the waves cells where the points were \n\nLets have a look at the two rasters to see if they it all makes sense.\n\npar(mfrow=c(1,2)) #plot them side-by-side\nplot(coral_sample)\nplot(waves_sample)"
  },
  {
    "objectID": "posts/2022-07-07-problem-solving-session-ii/index.html",
    "href": "posts/2022-07-07-problem-solving-session-ii/index.html",
    "title": "Problem Solving Session II",
    "section": "",
    "text": "Following the success of our last problem solving session, we ran another one in June.\n\n\nWe had interesting and wide-ranging discussions about the issues people are having with geospatial analysis.\nBelow are some of the key issues / contributions / suggestions that came up.\n\n\nLorna has been processing some massive raster datasets in R, and has been running into persistent RAM issues. The code below contains a few tricks of the trade to try to help get you up and running if you are running into RAM issues.\n\n\nThis includes a section about changing the RAM allocated to java - which is required for the species distribution modelling package MaxEnt.\n\n#increase RAM allocation; once increased you cannot decrease it until you restart your session\nmemory.limit(8000)\nmemory.limit(5000) #is ignored\n###check current RAM allocation\nmemory.limit()\n\n#if you restart r session it goes back to original RAM allocation\nmemory.limit()\n####\n# change RAM allocated to java (heap space)\n###\n#more niche, but java is needed to run MaxEnt (species distribution modelling)\n#settings need to be changed before opening the library\noptions(java.parameters = c(\"-XX:+UseConcMarkSweepGC\", \"-Xmx16000m\"))\n\n\n\n\n\nYou can also increase the RAM available to specific packages - namely the raster and terra packages:\n\n###with raster package\n\n#check allocated fraction (and other raster settings, including temporary directory)\nrasterOptions()\n\n#can increase or decrease in same session\nrasterOptions(memfrac=0.9)\nrasterOptions(memfrac=0.3)\n\n#restarting session goes back to original allocation\n\nwith terra package; Mitch showed us: https://geospatial-community.netlify.app/post/2022-02-23-raster-analysis/\n\nlibrary(terra)\nterraOptions()\nterraOptions(memfrac=0.7)\n\n#and you can also change the temp directory directly for terra... but only terra\nterraOptions(memfrac=0.2, tempdir = \"R:/big_drive/Trial\")\n\n\n\n\nAs described here, you can change the temp directory so it doesn‚Äôt fill up your computer.\n\nSys.getenv() #to check all setting of the environment\n\n#check the directories now\nSys.getenv(\"TMPDIR\",\"TMP\",\"TEMP\")\n\n#each path needs to be specifically written (otherwise it doesnt work, because of the quotations) \ntochange&lt;-c(\"TMPDIR = 'R:/large_disk/RtempFiles'\",\n            \"TMP = 'R:/large_disk/RtempFiles'\",\n            \"TEMP = 'R:/large_disk/RtempFiles'\")\n#\"R:/large_disk/Trial\"\n\n#write the file as .Renviron in your associated user path (R_USER)\nwrite(tochange, sep=\"c\", file=file.path(Sys.getenv('R_USER'), '.Renviron'))\nSys.getenv('R_USER') #the path where the file was written\n\n#restart session, can be without closing R\nSys.getenv(\"TMPDIR\",\"TMP\",\"TEMP\")\n\n\n\n\nBy default, R uses only one core;but most computers these days have multiple cores\nHere is a summary.\nHere are 2 ways to do this. ###1.\n\n#specific to an operation, but cannot do many raster operations\nclusterR(x, fun, args=NULL, cl=mycluster)\n\n###2.\nThis is the easiest, because you can put anything you need in between\n\nbeginCluster()\nendCluster()\n\nAs an example:\n\n#e.g.\nParentDir&lt;-\"R:\\\\FITZBIO-A6478\\\\4_Analysis\\\\R\\\\1_HSModel\\\\2_RasterTifs\\\\F3_90m\"\nLayer1 &lt;-raster(file.path(paste0(ParentDir,\"\\\\Bio1.tif\")))\nLayer2 &lt;-raster(file.path(paste0(ParentDir,\"\\\\Bio2.tif\")))\n#plot(Layer1) \n#plot(Layer2)\n#without using multiple cores\nstart_time &lt;- Sys.time()\ns&lt;-stack(Layer1,Layer2)\nLay1xLay2&lt;-overlay(s, fun=function(x,y) x*y )\nend_time &lt;- Sys.time()\nNoCore&lt;-end_time - start_time\n\n#using multiple cores, but measuring time only of the process itself\nbeginCluster()\nstart_time &lt;- Sys.time()\ns&lt;-stack(Layer1,Layer2)\nLay1xLay2&lt;-overlay(s, fun=function(x,y) x*y )\nend_time &lt;- Sys.time()\nendCluster()\nTimeExlcudeCore&lt;-end_time - start_time\n\n#using multiple cores, measuring time of whole process (including parallelizing)\nstart_time &lt;- Sys.time()\nbeginCluster()\ns&lt;-stack(Layer1,Layer2)\nLay1xLay2&lt;-overlay(s, fun=function(x,y) x*y )\nendCluster()\nend_time &lt;- Sys.time()\nTimeIncludeCore&lt;-end_time - start_time\n\n#check time results\nNoCore\nTimeExlcudeCore\nTimeIncludeCore\n#when preparing this, sometimes I got same results sometimes I didn't\n\n\n\n\nRunning gc() gets rid of temp files, so be sure to save results needed and not do it half way through a process that relies on temp files\n\n\n\nIf all else fails, you could think about how much resolution you actually need, and whether it could be reduced.\n\n#rescale base layer; basis for the rest\nBaseLayerFX&lt;-aggregate(Layer1, fact=factor) #factor defined where paths are loaded\n\nLayer2_coarse&lt;-resample(Layer2,BaseLayerFX) #it will match the extent of the baselayer\n#if you aggregate Layer2, there might be slight differences in pixel positions/extents etc-so they may not match\n#it is best to resample using the first one as basis\n\n####\n# save layers you create in between\n###\n#so next time you don't have to re-do them... \n#it saves times for you and it frees up memory both for new processes -and garbage collection gc()\n\n#e.g.\n#writeRaster(Layer2_coarse, filename=file.path(ProcessOut,paste0(\"uchas.tif\")), format=\"GTiff\", overwrite=FALSE)\n\n\n\n\nStill on the subject of memory, Tim showed us how to use the doParallel package in R, with a reproducible example.\nThis is a very basic example of what is called an embarrassingly parallel task, but should be a nice short intro for absolute beginners to parallel programming.\nParallel computing is a complex field of computer science. Performance is dependent on many factors including algorithm complexity, hardware IO performance and input data interdependence. Wikipedia has a nice overview of these topics here: https://en.wikipedia.org/wiki/Parallel_computing#Types_of_parallelism\n\n# First we define our function. For the sake of clear example this function just sleeps for n seconds which is given as input.\n\ndo_something &lt;- function(n)\n{\n    Sys.sleep(n)\n}\n\n\n### Base R for loop ###\n\n# Define number of iterations\niterations = 6\n\n# Start process timer\nstart &lt;- proc.time()\n\n# Call our function using a base R for loop, this is executed sequentially using one process. \nfor (x in 1:iterations){\n  do_something(1)\n}\n\n# Stop process timer\nbase_loop &lt;- proc.time()-start\n\n\n# Print duration to console\nprint(base_loop)\n\n   user  system elapsed \n   0.00    0.00    6.14 \n\n\n\n# Import doParallel \nlibrary(doParallel)\n\nLoading required package: foreach\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\n\n\n# Using doParallel.detectCores(), detect the number of cores on your machine.\ndetectCores()\n\n[1] 8\n\n\n\n# Using doParallel.registerDoParallel(), allocate a number of cores available for processing in parallel.\nregisterDoParallel(6)\n\n\n### doParaellel %do% ###\n\n# Start process timer\nstart &lt;- proc.time()\n\n# Call our function using a doParaellel %do% loop, this is also executed sequentially using one process. \n\nr &lt;- foreach(icount(iterations)) %do% {\n  do_something(1)\n}\n\n# Stop process timer\ndo_loop &lt;- proc.time()-start\n\n# Print duration to console\nprint(do_loop)\n\n   user  system elapsed \n   0.04    0.00    6.15 \n\n\nThis takes a similar amount of time to process as base R. Some functions do not gain performance when parallelised or the output of a function may be more reliable when executed as a single process. So in some situations parallelism is not always desired.\n\n### doParaellel %dopar% ###\n  \n# Start process timer\nstart &lt;- proc.time()\n\n# Call our function using a doParaellel %dopar% loop, this is executed in parallel.\nr &lt;- foreach(icount(6)) %dopar% {\n  do_something(1)\n}\n\n# Stop process timer\ndopar_loop &lt;- proc.time()-start\n\n# Print duration to console\nprint(dopar_loop)\n\n   user  system elapsed \n   0.02    0.00    1.08 \n\n\nIt takes ~1 seconds to run the our function on 6 cores. How long would it take on 3 cores, or 12 cores? What happens if we change the number of iterations?\n\n# Print concatenated results to console. \nprint(rbind(base_loop,do_loop,dopar_loop)[,1:3])\n\n           user.self sys.self elapsed\nbase_loop       0.00        0    6.14\ndo_loop         0.04        0    6.15\ndopar_loop      0.02        0    1.08\n\n\n\n\n\nRichard is running some geospatial analysis using the terra package.He ultimately wants this code to be free of warnings etc, so he can turn it into a shiny app.\nBut is constantly getting this warning: Error in (function (x) : attempt to apply non-function\nThis is discussed here.\nWhere Robert Hijmans (the main terra developer), seems to say that it is an annoying problem with no solution.\nApril 2022, from Robert Hijmans: &gt;You can ignore these messages from the garbage collector. They do not affect your data. They are very annoying. I have done a lot of things to get to the bottom of this, but sofar to no avail. I have much simpler packages that also show these messages and I need to go back to one of these to create a reproducible example for others to look at (even it only happens on the first run) that does not require installation of GDAL etc.\nIt appears to be related to the garbage collection R function gc()\nOne person suggested this as a ‚Äúdirty workaround‚Äù try(terra::XXX, silent = TRUE)\nSo it seems that there is no easy solution to this particular error.\n\n\n\nThis is currently unsolved, so please get in touch if you have a solution\nCatherine is having an issue with reprojecting between rasters with certain EPSG codes in terra.\nProject CRS is epsg:28355, but also has one raster in epsg:9001. So trying to convert the 9001 to 28355 and it is not working.\nAlso found that when trying to make a reproducible example, got NAs for the reprojected raster - whether reprojecting from 9001 or 28355‚Ä¶\nTesting reproject from EPSG 9001 to 4326\n\nlibrary(terra)\n\nterra 1.6.3\n\n    # Create a raster with EPSG:4326 projection\n    target &lt;- rast(nrows=108, ncols=21, xmin=0, xmax=50,\n              vals = rep(1:21, each = 2),\n              crs = \"EPSG:4326\")\n\nWarning: [setValues] values were recycled\n\n    plot(target)\n\n\n\n    crs(target)\n\n[1] \"GEOGCRS[\\\"WGS 84\\\",\\n    DATUM[\\\"World Geodetic System 1984\\\",\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"World.\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"EPSG\\\",4326]]\"\n\n\n\n# Create a raster with EPSG:4326 projection \n    y &lt;- rast(nrows = 54, ncols = 21, xmin = 0, xmax = 50,\n              vals = rep(1:21, each = 2),\n              crs = \"EPSG:9001\")\n\nWarning: [setValues] values were recycled\n\n    y\n\nclass       : SpatRaster \ndimensions  : 54, 21, 1  (nrow, ncol, nlyr)\nresolution  : 2.380952, 3.333333  (x, y)\nextent      : 0, 50, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=geocent +ellps=GRS80 +units=m +no_defs \nsource      : memory \nname        : lyr.1 \nmin value   :     1 \nmax value   :    21 \n\n    plot(y)\n\n\n\n    # project to target raster\n    z &lt;- project(y, target, method = \"near\")\n    z\n\nclass       : SpatRaster \ndimensions  : 108, 21, 1  (nrow, ncol, nlyr)\nresolution  : 2.380952, 1.666667  (x, y)\nextent      : 0, 50, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : memory \nname        : lyr.1 \nmin value   :   NaN \nmax value   :   NaN \n\n    # the min/max values are NaN?\n    plot(z) # blank...\n\n\n\n\n\n\n\nPaul is wanting to make some interactive maps as part of his thesis. Some discussion about the options that are out there. ### Blogdown. One option is to use something like blogdown. There are examples on this website, like Stephanes excellent work describing tidy networks.\n\n\nLeaflet is another option.\nThis might give more flexibility in terms of map making, but doesn‚Äôt give you the document format.\nWe will stay posted with what Paul ends up doing!\n\n\n\n\nDeqiang is wanting to know how to run arcpy on the supercomputer.\nBecause Arcpy uses windows, it is not able to be run on any of the university HPCs‚Ä¶\nGabriel had had similar issues with arcpy, and confirmed that the HPC will not support arcpy with because it uses linux based clusters\nGabriel suggested running arcpy using the multiprocessing library\nAn alternative would be to re-code the least cost algorithm using python https://gis.stackexchange.com/questions/28583/gdal-perform-simple-least-cost-path-analysis\n\n\n\nThere was some discussion of debugging in R. This post was shared as a starting point.\nBecause debugging is something most of thought we could do better at, it might be a good candidate for a workshop down the track."
  },
  {
    "objectID": "posts/2022-07-07-problem-solving-session-ii/index.html#six-tips-for-processing-big-rasters-in-r---lorna-hernandez-santin",
    "href": "posts/2022-07-07-problem-solving-session-ii/index.html#six-tips-for-processing-big-rasters-in-r---lorna-hernandez-santin",
    "title": "Problem Solving Session II",
    "section": "",
    "text": "Lorna has been processing some massive raster datasets in R, and has been running into persistent RAM issues. The code below contains a few tricks of the trade to try to help get you up and running if you are running into RAM issues.\n\n\nThis includes a section about changing the RAM allocated to java - which is required for the species distribution modelling package MaxEnt.\n\n#increase RAM allocation; once increased you cannot decrease it until you restart your session\nmemory.limit(8000)\nmemory.limit(5000) #is ignored\n###check current RAM allocation\nmemory.limit()\n\n#if you restart r session it goes back to original RAM allocation\nmemory.limit()\n####\n# change RAM allocated to java (heap space)\n###\n#more niche, but java is needed to run MaxEnt (species distribution modelling)\n#settings need to be changed before opening the library\noptions(java.parameters = c(\"-XX:+UseConcMarkSweepGC\", \"-Xmx16000m\"))"
  },
  {
    "objectID": "posts/2022-07-07-problem-solving-session-ii/index.html#change-ram-allocated-to-process-rasters",
    "href": "posts/2022-07-07-problem-solving-session-ii/index.html#change-ram-allocated-to-process-rasters",
    "title": "Problem Solving Session II",
    "section": "",
    "text": "You can also increase the RAM available to specific packages - namely the raster and terra packages:\n\n###with raster package\n\n#check allocated fraction (and other raster settings, including temporary directory)\nrasterOptions()\n\n#can increase or decrease in same session\nrasterOptions(memfrac=0.9)\nrasterOptions(memfrac=0.3)\n\n#restarting session goes back to original allocation\n\nwith terra package; Mitch showed us: https://geospatial-community.netlify.app/post/2022-02-23-raster-analysis/\n\nlibrary(terra)\nterraOptions()\nterraOptions(memfrac=0.7)\n\n#and you can also change the temp directory directly for terra... but only terra\nterraOptions(memfrac=0.2, tempdir = \"R:/big_drive/Trial\")"
  },
  {
    "objectID": "posts/2022-07-07-problem-solving-session-ii/index.html#move-temporary-directory-to-a-different-folder",
    "href": "posts/2022-07-07-problem-solving-session-ii/index.html#move-temporary-directory-to-a-different-folder",
    "title": "Problem Solving Session II",
    "section": "",
    "text": "As described here, you can change the temp directory so it doesn‚Äôt fill up your computer.\n\nSys.getenv() #to check all setting of the environment\n\n#check the directories now\nSys.getenv(\"TMPDIR\",\"TMP\",\"TEMP\")\n\n#each path needs to be specifically written (otherwise it doesnt work, because of the quotations) \ntochange&lt;-c(\"TMPDIR = 'R:/large_disk/RtempFiles'\",\n            \"TMP = 'R:/large_disk/RtempFiles'\",\n            \"TEMP = 'R:/large_disk/RtempFiles'\")\n#\"R:/large_disk/Trial\"\n\n#write the file as .Renviron in your associated user path (R_USER)\nwrite(tochange, sep=\"c\", file=file.path(Sys.getenv('R_USER'), '.Renviron'))\nSys.getenv('R_USER') #the path where the file was written\n\n#restart session, can be without closing R\nSys.getenv(\"TMPDIR\",\"TMP\",\"TEMP\")"
  },
  {
    "objectID": "posts/2022-07-07-problem-solving-session-ii/index.html#parallel-processing",
    "href": "posts/2022-07-07-problem-solving-session-ii/index.html#parallel-processing",
    "title": "Problem Solving Session II",
    "section": "",
    "text": "By default, R uses only one core;but most computers these days have multiple cores\nHere is a summary.\nHere are 2 ways to do this. ###1.\n\n#specific to an operation, but cannot do many raster operations\nclusterR(x, fun, args=NULL, cl=mycluster)\n\n###2.\nThis is the easiest, because you can put anything you need in between\n\nbeginCluster()\nendCluster()\n\nAs an example:\n\n#e.g.\nParentDir&lt;-\"R:\\\\FITZBIO-A6478\\\\4_Analysis\\\\R\\\\1_HSModel\\\\2_RasterTifs\\\\F3_90m\"\nLayer1 &lt;-raster(file.path(paste0(ParentDir,\"\\\\Bio1.tif\")))\nLayer2 &lt;-raster(file.path(paste0(ParentDir,\"\\\\Bio2.tif\")))\n#plot(Layer1) \n#plot(Layer2)\n#without using multiple cores\nstart_time &lt;- Sys.time()\ns&lt;-stack(Layer1,Layer2)\nLay1xLay2&lt;-overlay(s, fun=function(x,y) x*y )\nend_time &lt;- Sys.time()\nNoCore&lt;-end_time - start_time\n\n#using multiple cores, but measuring time only of the process itself\nbeginCluster()\nstart_time &lt;- Sys.time()\ns&lt;-stack(Layer1,Layer2)\nLay1xLay2&lt;-overlay(s, fun=function(x,y) x*y )\nend_time &lt;- Sys.time()\nendCluster()\nTimeExlcudeCore&lt;-end_time - start_time\n\n#using multiple cores, measuring time of whole process (including parallelizing)\nstart_time &lt;- Sys.time()\nbeginCluster()\ns&lt;-stack(Layer1,Layer2)\nLay1xLay2&lt;-overlay(s, fun=function(x,y) x*y )\nendCluster()\nend_time &lt;- Sys.time()\nTimeIncludeCore&lt;-end_time - start_time\n\n#check time results\nNoCore\nTimeExlcudeCore\nTimeIncludeCore\n#when preparing this, sometimes I got same results sometimes I didn't"
  },
  {
    "objectID": "posts/2022-07-07-problem-solving-session-ii/index.html#free-up-memory",
    "href": "posts/2022-07-07-problem-solving-session-ii/index.html#free-up-memory",
    "title": "Problem Solving Session II",
    "section": "",
    "text": "Running gc() gets rid of temp files, so be sure to save results needed and not do it half way through a process that relies on temp files"
  },
  {
    "objectID": "posts/2022-07-07-problem-solving-session-ii/index.html#change-resolution",
    "href": "posts/2022-07-07-problem-solving-session-ii/index.html#change-resolution",
    "title": "Problem Solving Session II",
    "section": "",
    "text": "If all else fails, you could think about how much resolution you actually need, and whether it could be reduced.\n\n#rescale base layer; basis for the rest\nBaseLayerFX&lt;-aggregate(Layer1, fact=factor) #factor defined where paths are loaded\n\nLayer2_coarse&lt;-resample(Layer2,BaseLayerFX) #it will match the extent of the baselayer\n#if you aggregate Layer2, there might be slight differences in pixel positions/extents etc-so they may not match\n#it is best to resample using the first one as basis\n\n####\n# save layers you create in between\n###\n#so next time you don't have to re-do them... \n#it saves times for you and it frees up memory both for new processes -and garbage collection gc()\n\n#e.g.\n#writeRaster(Layer2_coarse, filename=file.path(ProcessOut,paste0(\"uchas.tif\")), format=\"GTiff\", overwrite=FALSE)"
  },
  {
    "objectID": "posts/2022-07-07-problem-solving-session-ii/index.html#the-doparallel-package---tim-devereux",
    "href": "posts/2022-07-07-problem-solving-session-ii/index.html#the-doparallel-package---tim-devereux",
    "title": "Problem Solving Session II",
    "section": "",
    "text": "Still on the subject of memory, Tim showed us how to use the doParallel package in R, with a reproducible example.\nThis is a very basic example of what is called an embarrassingly parallel task, but should be a nice short intro for absolute beginners to parallel programming.\nParallel computing is a complex field of computer science. Performance is dependent on many factors including algorithm complexity, hardware IO performance and input data interdependence. Wikipedia has a nice overview of these topics here: https://en.wikipedia.org/wiki/Parallel_computing#Types_of_parallelism\n\n# First we define our function. For the sake of clear example this function just sleeps for n seconds which is given as input.\n\ndo_something &lt;- function(n)\n{\n    Sys.sleep(n)\n}\n\n\n### Base R for loop ###\n\n# Define number of iterations\niterations = 6\n\n# Start process timer\nstart &lt;- proc.time()\n\n# Call our function using a base R for loop, this is executed sequentially using one process. \nfor (x in 1:iterations){\n  do_something(1)\n}\n\n# Stop process timer\nbase_loop &lt;- proc.time()-start\n\n\n# Print duration to console\nprint(base_loop)\n\n   user  system elapsed \n   0.00    0.00    6.14 \n\n\n\n# Import doParallel \nlibrary(doParallel)\n\nLoading required package: foreach\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\n\n\n# Using doParallel.detectCores(), detect the number of cores on your machine.\ndetectCores()\n\n[1] 8\n\n\n\n# Using doParallel.registerDoParallel(), allocate a number of cores available for processing in parallel.\nregisterDoParallel(6)\n\n\n### doParaellel %do% ###\n\n# Start process timer\nstart &lt;- proc.time()\n\n# Call our function using a doParaellel %do% loop, this is also executed sequentially using one process. \n\nr &lt;- foreach(icount(iterations)) %do% {\n  do_something(1)\n}\n\n# Stop process timer\ndo_loop &lt;- proc.time()-start\n\n# Print duration to console\nprint(do_loop)\n\n   user  system elapsed \n   0.04    0.00    6.15 \n\n\nThis takes a similar amount of time to process as base R. Some functions do not gain performance when parallelised or the output of a function may be more reliable when executed as a single process. So in some situations parallelism is not always desired.\n\n### doParaellel %dopar% ###\n  \n# Start process timer\nstart &lt;- proc.time()\n\n# Call our function using a doParaellel %dopar% loop, this is executed in parallel.\nr &lt;- foreach(icount(6)) %dopar% {\n  do_something(1)\n}\n\n# Stop process timer\ndopar_loop &lt;- proc.time()-start\n\n# Print duration to console\nprint(dopar_loop)\n\n   user  system elapsed \n   0.02    0.00    1.08 \n\n\nIt takes ~1 seconds to run the our function on 6 cores. How long would it take on 3 cores, or 12 cores? What happens if we change the number of iterations?\n\n# Print concatenated results to console. \nprint(rbind(base_loop,do_loop,dopar_loop)[,1:3])\n\n           user.self sys.self elapsed\nbase_loop       0.00        0    6.14\ndo_loop         0.04        0    6.15\ndopar_loop      0.02        0    1.08"
  },
  {
    "objectID": "posts/2022-07-07-problem-solving-session-ii/index.html#annoying-terra-warning---richard-cottrell",
    "href": "posts/2022-07-07-problem-solving-session-ii/index.html#annoying-terra-warning---richard-cottrell",
    "title": "Problem Solving Session II",
    "section": "",
    "text": "Richard is running some geospatial analysis using the terra package.He ultimately wants this code to be free of warnings etc, so he can turn it into a shiny app.\nBut is constantly getting this warning: Error in (function (x) : attempt to apply non-function\nThis is discussed here.\nWhere Robert Hijmans (the main terra developer), seems to say that it is an annoying problem with no solution.\nApril 2022, from Robert Hijmans: &gt;You can ignore these messages from the garbage collector. They do not affect your data. They are very annoying. I have done a lot of things to get to the bottom of this, but sofar to no avail. I have much simpler packages that also show these messages and I need to go back to one of these to create a reproducible example for others to look at (even it only happens on the first run) that does not require installation of GDAL etc.\nIt appears to be related to the garbage collection R function gc()\nOne person suggested this as a ‚Äúdirty workaround‚Äù try(terra::XXX, silent = TRUE)\nSo it seems that there is no easy solution to this particular error."
  },
  {
    "objectID": "posts/2022-07-07-problem-solving-session-ii/index.html#coordinate-reference-system-issues-in-terra---catherine-kim",
    "href": "posts/2022-07-07-problem-solving-session-ii/index.html#coordinate-reference-system-issues-in-terra---catherine-kim",
    "title": "Problem Solving Session II",
    "section": "",
    "text": "This is currently unsolved, so please get in touch if you have a solution\nCatherine is having an issue with reprojecting between rasters with certain EPSG codes in terra.\nProject CRS is epsg:28355, but also has one raster in epsg:9001. So trying to convert the 9001 to 28355 and it is not working.\nAlso found that when trying to make a reproducible example, got NAs for the reprojected raster - whether reprojecting from 9001 or 28355‚Ä¶\nTesting reproject from EPSG 9001 to 4326\n\nlibrary(terra)\n\nterra 1.6.3\n\n    # Create a raster with EPSG:4326 projection\n    target &lt;- rast(nrows=108, ncols=21, xmin=0, xmax=50,\n              vals = rep(1:21, each = 2),\n              crs = \"EPSG:4326\")\n\nWarning: [setValues] values were recycled\n\n    plot(target)\n\n\n\n    crs(target)\n\n[1] \"GEOGCRS[\\\"WGS 84\\\",\\n    DATUM[\\\"World Geodetic System 1984\\\",\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"World.\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"EPSG\\\",4326]]\"\n\n\n\n# Create a raster with EPSG:4326 projection \n    y &lt;- rast(nrows = 54, ncols = 21, xmin = 0, xmax = 50,\n              vals = rep(1:21, each = 2),\n              crs = \"EPSG:9001\")\n\nWarning: [setValues] values were recycled\n\n    y\n\nclass       : SpatRaster \ndimensions  : 54, 21, 1  (nrow, ncol, nlyr)\nresolution  : 2.380952, 3.333333  (x, y)\nextent      : 0, 50, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=geocent +ellps=GRS80 +units=m +no_defs \nsource      : memory \nname        : lyr.1 \nmin value   :     1 \nmax value   :    21 \n\n    plot(y)\n\n\n\n    # project to target raster\n    z &lt;- project(y, target, method = \"near\")\n    z\n\nclass       : SpatRaster \ndimensions  : 108, 21, 1  (nrow, ncol, nlyr)\nresolution  : 2.380952, 1.666667  (x, y)\nextent      : 0, 50, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : memory \nname        : lyr.1 \nmin value   :   NaN \nmax value   :   NaN \n\n    # the min/max values are NaN?\n    plot(z) # blank..."
  },
  {
    "objectID": "posts/2022-07-07-problem-solving-session-ii/index.html#incorporating-interactive-maps-into-your-thesis---paul-dielmans",
    "href": "posts/2022-07-07-problem-solving-session-ii/index.html#incorporating-interactive-maps-into-your-thesis---paul-dielmans",
    "title": "Problem Solving Session II",
    "section": "",
    "text": "Paul is wanting to make some interactive maps as part of his thesis. Some discussion about the options that are out there. ### Blogdown. One option is to use something like blogdown. There are examples on this website, like Stephanes excellent work describing tidy networks.\n\n\nLeaflet is another option.\nThis might give more flexibility in terms of map making, but doesn‚Äôt give you the document format.\nWe will stay posted with what Paul ends up doing!"
  },
  {
    "objectID": "posts/2022-07-07-problem-solving-session-ii/index.html#running-arcpy-on-a-supercomputer---deqiang-ma",
    "href": "posts/2022-07-07-problem-solving-session-ii/index.html#running-arcpy-on-a-supercomputer---deqiang-ma",
    "title": "Problem Solving Session II",
    "section": "",
    "text": "Deqiang is wanting to know how to run arcpy on the supercomputer.\nBecause Arcpy uses windows, it is not able to be run on any of the university HPCs‚Ä¶\nGabriel had had similar issues with arcpy, and confirmed that the HPC will not support arcpy with because it uses linux based clusters\nGabriel suggested running arcpy using the multiprocessing library\nAn alternative would be to re-code the least cost algorithm using python https://gis.stackexchange.com/questions/28583/gdal-perform-simple-least-cost-path-analysis"
  },
  {
    "objectID": "posts/2022-07-07-problem-solving-session-ii/index.html#how-do-you-debug-r-code-properly",
    "href": "posts/2022-07-07-problem-solving-session-ii/index.html#how-do-you-debug-r-code-properly",
    "title": "Problem Solving Session II",
    "section": "",
    "text": "There was some discussion of debugging in R. This post was shared as a starting point.\nBecause debugging is something most of thought we could do better at, it might be a good candidate for a workshop down the track."
  },
  {
    "objectID": "posts/2022-07-14-newsletter-mapping-interactively-r-for-ecologists-data-wrangling-more/index.html",
    "href": "posts/2022-07-14-newsletter-mapping-interactively-r-for-ecologists-data-wrangling-more/index.html",
    "title": "Newsletter | Mapping interactively + R for ecologists + Data wrangling + more.",
    "section": "",
    "text": "We have put together a summary of last week‚Äôs workshop.\nThanks to everyone who had questions, answers, and contributions. It should all be captured in the post now. If you would like me to make any edits to the post, or better still if there are development on the problems, please let me know and I can easily update the details.\n\n\n\nChristina Buelow from Griffith will be hosting this 1-day course. It will be extremely useful whether you are relatively new to geospatial in R, or looking to hone your skills. https://smp.uq.edu.au/event/session/11370 Cost is $150 for students (hopefully there are still spots available)\n\n\n\nChristina will also be hosting a 100% free Mapping interactively with Rshiny workshop later in the month. Please spread the word, this is sure to be an excellent use of time if you have ever thought about putting your maps and data online.\n\n\n\nAdvanced R workshop with Bill Venables The advanced R workshop with Bill Venables was cancelled but he is doing it for free over several weeks 1 hr on Monday, Wednesday and Friday.\nIf you‚Äôre interested, email roxanne.jemison@uq.edu.au who is organising it.\n\n\n\nEcoCommons, who are creating a platform for digital modelling and analysis (read more here https://www.ecocommons.org.au/about/), have a new educational package: R for Ecologists (https://www.ecocommons.org.au/educational-material/). This is based on this data carpentry module. All videos are 100% free on youtube, and include plenty of spatial content including species distribution modelling.\n\n\n\nWe are putting together an organising committee to help coordinate workshops, this newsletter, and any other new ideas we might have. If you would be interested in volunteering a small amount of time, please get in touch.\n\n\n\nHere are a few job vacancies from across the country that might be of interest. Mainly taken from NRM jobs. If you know of any vacancies, send them in and I‚Äôll include them on the next newsletter.\nSpatial Scientist ‚Äì NSW Department of Planning and Environment (Paramatta, NSW)\nAssistant Director ‚Äì Scientist (Geospatial ‚Äì Land use). Federal department of Agriculture, Fisheries and Forestry. (Canberra)\nSpatial Information Systems Officer. Timberlands Pacific (Launceston, Tasmania)\nGIS analyst. Eco Logical Australia. (Adelaide, SA)\nNatural Hazard and Spatial Analyst. Molino Stewart (Paramatta, NSW)"
  },
  {
    "objectID": "posts/2022-07-14-newsletter-mapping-interactively-r-for-ecologists-data-wrangling-more/index.html#latest-blog-post",
    "href": "posts/2022-07-14-newsletter-mapping-interactively-r-for-ecologists-data-wrangling-more/index.html#latest-blog-post",
    "title": "Newsletter | Mapping interactively + R for ecologists + Data wrangling + more.",
    "section": "",
    "text": "We have put together a summary of last week‚Äôs workshop.\nThanks to everyone who had questions, answers, and contributions. It should all be captured in the post now. If you would like me to make any edits to the post, or better still if there are development on the problems, please let me know and I can easily update the details."
  },
  {
    "objectID": "posts/2022-07-14-newsletter-mapping-interactively-r-for-ecologists-data-wrangling-more/index.html#advanced-data-wrangling-for-spatial-analysis-in-r",
    "href": "posts/2022-07-14-newsletter-mapping-interactively-r-for-ecologists-data-wrangling-more/index.html#advanced-data-wrangling-for-spatial-analysis-in-r",
    "title": "Newsletter | Mapping interactively + R for ecologists + Data wrangling + more.",
    "section": "",
    "text": "Christina Buelow from Griffith will be hosting this 1-day course. It will be extremely useful whether you are relatively new to geospatial in R, or looking to hone your skills. https://smp.uq.edu.au/event/session/11370 Cost is $150 for students (hopefully there are still spots available)"
  },
  {
    "objectID": "posts/2022-07-14-newsletter-mapping-interactively-r-for-ecologists-data-wrangling-more/index.html#interactive-maps-in-rshiny",
    "href": "posts/2022-07-14-newsletter-mapping-interactively-r-for-ecologists-data-wrangling-more/index.html#interactive-maps-in-rshiny",
    "title": "Newsletter | Mapping interactively + R for ecologists + Data wrangling + more.",
    "section": "",
    "text": "Christina will also be hosting a 100% free Mapping interactively with Rshiny workshop later in the month. Please spread the word, this is sure to be an excellent use of time if you have ever thought about putting your maps and data online."
  },
  {
    "objectID": "posts/2022-07-14-newsletter-mapping-interactively-r-for-ecologists-data-wrangling-more/index.html#advanced-r-workshop-with-bill-venables",
    "href": "posts/2022-07-14-newsletter-mapping-interactively-r-for-ecologists-data-wrangling-more/index.html#advanced-r-workshop-with-bill-venables",
    "title": "Newsletter | Mapping interactively + R for ecologists + Data wrangling + more.",
    "section": "",
    "text": "Advanced R workshop with Bill Venables The advanced R workshop with Bill Venables was cancelled but he is doing it for free over several weeks 1 hr on Monday, Wednesday and Friday.\nIf you‚Äôre interested, email roxanne.jemison@uq.edu.au who is organising it."
  },
  {
    "objectID": "posts/2022-07-14-newsletter-mapping-interactively-r-for-ecologists-data-wrangling-more/index.html#r-for-ecologists.",
    "href": "posts/2022-07-14-newsletter-mapping-interactively-r-for-ecologists-data-wrangling-more/index.html#r-for-ecologists.",
    "title": "Newsletter | Mapping interactively + R for ecologists + Data wrangling + more.",
    "section": "",
    "text": "EcoCommons, who are creating a platform for digital modelling and analysis (read more here https://www.ecocommons.org.au/about/), have a new educational package: R for Ecologists (https://www.ecocommons.org.au/educational-material/). This is based on this data carpentry module. All videos are 100% free on youtube, and include plenty of spatial content including species distribution modelling."
  },
  {
    "objectID": "posts/2022-07-14-newsletter-mapping-interactively-r-for-ecologists-data-wrangling-more/index.html#organising-committee",
    "href": "posts/2022-07-14-newsletter-mapping-interactively-r-for-ecologists-data-wrangling-more/index.html#organising-committee",
    "title": "Newsletter | Mapping interactively + R for ecologists + Data wrangling + more.",
    "section": "",
    "text": "We are putting together an organising committee to help coordinate workshops, this newsletter, and any other new ideas we might have. If you would be interested in volunteering a small amount of time, please get in touch."
  },
  {
    "objectID": "posts/2022-07-14-newsletter-mapping-interactively-r-for-ecologists-data-wrangling-more/index.html#spatial-job-vacancies",
    "href": "posts/2022-07-14-newsletter-mapping-interactively-r-for-ecologists-data-wrangling-more/index.html#spatial-job-vacancies",
    "title": "Newsletter | Mapping interactively + R for ecologists + Data wrangling + more.",
    "section": "",
    "text": "Here are a few job vacancies from across the country that might be of interest. Mainly taken from NRM jobs. If you know of any vacancies, send them in and I‚Äôll include them on the next newsletter.\nSpatial Scientist ‚Äì NSW Department of Planning and Environment (Paramatta, NSW)\nAssistant Director ‚Äì Scientist (Geospatial ‚Äì Land use). Federal department of Agriculture, Fisheries and Forestry. (Canberra)\nSpatial Information Systems Officer. Timberlands Pacific (Launceston, Tasmania)\nGIS analyst. Eco Logical Australia. (Adelaide, SA)\nNatural Hazard and Spatial Analyst. Molino Stewart (Paramatta, NSW)"
  },
  {
    "objectID": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html",
    "href": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html",
    "title": "Interactive mapping with RShiny",
    "section": "",
    "text": "In this workshop, we learned how to use R to read in spatial data and make beautiful interactive online maps. # A journey in maps and apps‚Ä¶\nTo follow along in R you will need several packages that can be installed with a single line of code:\ninstall.packages(c('shiny', 'leaflet', 'sf', 'tmap', 'dplyr'))\nNote: This journey is thanks to the awesome book Mastering Shiny"
  },
  {
    "objectID": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#why-interactive-and-why-shiny",
    "href": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#why-interactive-and-why-shiny",
    "title": "Interactive mapping with RShiny",
    "section": "Why interactive and why Shiny?",
    "text": "Why interactive and why Shiny?\nOpen Science\n\nInteractive web apps are a great way to make our research open, transparent, and engaging\nMove beyond static papers\n\nShiny\n\nPerformative apps without HTML, javascript, CSS‚Ä¶\nRange of users - academics to tech companies\nNeed to be hosted on a web server\n\nServer options\n\nshinyapps.io: free or subscription options\nSee here for more info on Rshiny servers\nThere are other ‚Äònon-R‚Äô server options out there too"
  },
  {
    "objectID": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#app-fundamentals",
    "href": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#app-fundamentals",
    "title": "Interactive mapping with RShiny",
    "section": "App fundamentals",
    "text": "App fundamentals\nInteractivity requires Reactivity\nWhat is reactive programming?\n\nOutputs react to inputs\n\nMeans computations are only done when necessary (moving beyond functions to reactions)\nReactive contexts are lazy and cached\n\nIt‚Äôs different from how we normally code our scripts:\n\nCommands vs.¬†recipes\nImperative vs.¬†declarative\nAssertive vs.¬†passive-aggressive"
  },
  {
    "objectID": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#shiny",
    "href": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#shiny",
    "title": "Interactive mapping with RShiny",
    "section": "Shiny",
    "text": "Shiny\n{shiny} provides a framework for reactive programming:\n\nUser-interface (ui): how it looks. Takes inputs and displays outputs\nServer: has the recipe to turn inputs into outputs\n\nhint: if you have {shiny} installed, just type shinyapp in your R script to insert boilerplate shiny app code and get started quickly\n\nlibrary(shiny)\nui &lt;- fluidPage(\n  \n)\nserver &lt;- function(input, output, session) {\n  \n}\nshinyApp(ui, server)"
  },
  {
    "objectID": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#a-simple-histogram",
    "href": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#a-simple-histogram",
    "title": "Interactive mapping with RShiny",
    "section": "A simple histogram",
    "text": "A simple histogram\n\nhist(rnorm(25))"
  },
  {
    "objectID": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#a-reactive-histogram",
    "href": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#a-reactive-histogram",
    "title": "Interactive mapping with RShiny",
    "section": "A reactive histogram",
    "text": "A reactive histogram\n\nlibrary(shiny)\nui &lt;- fluidPage(\n  numericInput(inputId = \"n\", \"Sample size\", value = 25), \n  plotOutput(outputId = \"hist\")\n)\nserver &lt;- function(input, output, session) {\n  output$hist &lt;- renderPlot({\n    hist(rnorm(input$n))\n    })\n}\nshinyApp(ui, server)\n\nYou‚Äôll need to hit the ‚ÄòRun app‚Äô button in the top right corner of your script.\nNote There‚Äôs an additional reactive tool, reactive expressions. Reactive expressions can be used to eliminate redundant code in your app, thereby improving efficiency. Find more information here."
  },
  {
    "objectID": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#but-what-about-maps",
    "href": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#but-what-about-maps",
    "title": "Interactive mapping with RShiny",
    "section": "But what about maps?",
    "text": "But what about maps?\nMy go-to for mapping in R these days is {tmap}\nOne of the reasons is that I can make either interactive or static, publication-quality maps with ease. Just set the ‚Äòmode‚Äô.\n\nlibrary(tmap)\ndata('World')\ntmap_mode('view')\n\ntmap mode set to interactive viewing\n\nqtm(World, 'name') # qtm stands for 'quick thematic mapper'"
  },
  {
    "objectID": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#share-with-shiny",
    "href": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#share-with-shiny",
    "title": "Interactive mapping with RShiny",
    "section": "Share with shiny",
    "text": "Share with shiny\nIf we‚Äôre in a hurry, we can put our interactive {tmap} in a {shiny} app to share\n\nlibrary(shiny)\nlibrary(tmap)\ndata('World')\nui &lt;- fluidPage(\n  tmapOutput('map')\n)\nserver &lt;- function(input, output, session) {\n  output$map &lt;- renderTmap({\n    qtm(World, 'name')})\n}\nshinyApp(ui, server)\n\nDon‚Äôt forget to hit the ‚ÄòRun app‚Äô button in the top right corner of your script"
  },
  {
    "objectID": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#add-some-reactivity",
    "href": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#add-some-reactivity",
    "title": "Interactive mapping with RShiny",
    "section": "Add some reactivity",
    "text": "Add some reactivity\nLet‚Äôs add a widget to allow users to choose their country from a list\n\nlibrary(shiny)\nlibrary(tmap)\nlibrary(dplyr)\ndata('World')\nui &lt;- fluidPage(\n  tmapOutput('map'),\n  selectInput('var', 'Select country',\n              choices = c('Global', as.character(World$name)),\n              selected = 'Global')\n)\nserver &lt;- function(input, output, session) {\n  output$map &lt;- renderTmap({\n    if(input$var == 'Global'){\n      qtm(World, 'name')\n    }else{\n      qtm(filter(World, name == input$var))\n    }\n  })\n}\nshinyApp(ui, server)\n\nDon‚Äôt forget to hit the ‚ÄòRun app‚Äô button in the top right corner of your script"
  },
  {
    "objectID": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#customising-and-scaling-your-interactive-map",
    "href": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#customising-and-scaling-your-interactive-map",
    "title": "Interactive mapping with RShiny",
    "section": "Customising and scaling your interactive map",
    "text": "Customising and scaling your interactive map\nIf you want to customise your map with more features, the leaflet R package offers some nice functionality (there are others too)\n{tmap} and {leaflet} both use ‚Äòscalable vector graphics‚Äô (SVG) to visualise maps on the web\n\nThis is fine for simple apps, but if you need to render large datasets or do complex computations they will be really slow\n\nScaling-up your map app might require graphics to be rendered in ‚ÄòWebGL‚Äô instead of SVG\n\nThere are several R packages to try including mapdeck and rdeck\nThese usually require API tokens to access basemaps from mapbox\n\nShould I go SVG or WebGL?\n\nGenerally I‚Äôve found there is a feature vs.¬†speed trade-off\nRecommend start with SVG and if too slow then move to WebGL"
  },
  {
    "objectID": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#tips-and-tricks",
    "href": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#tips-and-tricks",
    "title": "Interactive mapping with RShiny",
    "section": "Tips and tricks",
    "text": "Tips and tricks\n\nLots of complex polygons? Use rmapshaper to simplify, this can really speed up a slow map app\nApp still slow? Use profvis to profile your script and find the bottleneck\nHave you reached the ‚Äòcliff of complexity‚Äô? Check out Engineering production grade shiny apps"
  },
  {
    "objectID": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#leaflet-example",
    "href": "posts/2022-07-20-intro-interactive-mapping-rshiny/index.html#leaflet-example",
    "title": "Interactive mapping with RShiny",
    "section": "Leaflet example",
    "text": "Leaflet example\nHead over to Christina‚Äôs public github repo and clone to your local computer. This will give you access to all of the scripts and data you need to:  1. Build an interactive {leaflet} map: follow the R script ‚Äòleaflet-map.R‚Äô  2. Turn the interactive {leaflet} map into a share-able and dynamic {shiny} web app: follow the R script ‚Äòleaflet-app.R‚Äô"
  },
  {
    "objectID": "posts/2022-08-16-newsletter/index.html",
    "href": "posts/2022-08-16-newsletter/index.html",
    "title": "Newsletter | Analysing climate data with R + Interactive Maps + AEO Forum + more.",
    "section": "",
    "text": "A huge thanks to Christina for the interactive mapping with RShiny workshop. All the material is available here. There was a lot of interest in the subject of interactive maps, and talk of a follow up session. Christina is very knowledgeable on all thing R-spatial, and would be happy to help if you are stuck trying to make a web map.\n\n\n\nWe have another great workshop is lined up. Ralph Trancoso is going to show us how to delve into climate data - both observations and projections - using R. If you want to include climate data in your research but don‚Äôt know where to start, you really won‚Äôt want to miss this one!\n\n\n\nNick Wiggins is taking over from Stephane in organising the UQ R user group sessions. The next one will be on August 24th. Each session is recorded on this collaborative document.\nAs well as the R users group, Nick will be hosting some beginner spatial analysis sessions. Introduction to QGIS will be held on Thursday 25 August 2022, 9.30am - 11.30am Introduction to Raster Analysis Will be held on Thursday 8 September 2022, 9.30am - 11.30am All hosted through the UQ library, book through here.\n\n\n\nThe Advancing Earth Observation Forum will be held next week, 22nd ‚Äì 26th August at the Brisbane Convention and Exhibition Centre. This will be a great opportunity to connect with and learn from others in the earth observation community. Registrations are still open.\n\n\n\nUnfortunately, the 2022 ResBaz conference has been postponed due to the Covid and flu surge. We will provide updates as they become available.\n\n\n\nmapscaping is a spatial podcast where they discuss a lot of interesting geospatial topics that come up in our sessions, so definitely worth checking out.\n\n\n\nHere are a few job vacancies from across the country that might be of interest. If you know of any vacancies, send them in and they will be included with the next newsletter.\nGraduate Spatial information officer, QLD government, Location flexible.\nGeospatial engineer, Nova Systems, Brisbane\nGIS technician, TerraLab, Victoria\nTeam Leader Conservation Planner / GIS analyst, Centre for Conservation Geography, Byron Bay."
  },
  {
    "objectID": "posts/2022-08-16-newsletter/index.html#blog-post-interactive-mapping-with-rshiny-christina-buelow",
    "href": "posts/2022-08-16-newsletter/index.html#blog-post-interactive-mapping-with-rshiny-christina-buelow",
    "title": "Newsletter | Analysing climate data with R + Interactive Maps + AEO Forum + more.",
    "section": "",
    "text": "A huge thanks to Christina for the interactive mapping with RShiny workshop. All the material is available here. There was a lot of interest in the subject of interactive maps, and talk of a follow up session. Christina is very knowledgeable on all thing R-spatial, and would be happy to help if you are stuck trying to make a web map."
  },
  {
    "objectID": "posts/2022-08-16-newsletter/index.html#next-workshop---analysing-climate-data-with-r-ralph-trancoso",
    "href": "posts/2022-08-16-newsletter/index.html#next-workshop---analysing-climate-data-with-r-ralph-trancoso",
    "title": "Newsletter | Analysing climate data with R + Interactive Maps + AEO Forum + more.",
    "section": "",
    "text": "We have another great workshop is lined up. Ralph Trancoso is going to show us how to delve into climate data - both observations and projections - using R. If you want to include climate data in your research but don‚Äôt know where to start, you really won‚Äôt want to miss this one!"
  },
  {
    "objectID": "posts/2022-08-16-newsletter/index.html#library-update-from-nick",
    "href": "posts/2022-08-16-newsletter/index.html#library-update-from-nick",
    "title": "Newsletter | Analysing climate data with R + Interactive Maps + AEO Forum + more.",
    "section": "",
    "text": "Nick Wiggins is taking over from Stephane in organising the UQ R user group sessions. The next one will be on August 24th. Each session is recorded on this collaborative document.\nAs well as the R users group, Nick will be hosting some beginner spatial analysis sessions. Introduction to QGIS will be held on Thursday 25 August 2022, 9.30am - 11.30am Introduction to Raster Analysis Will be held on Thursday 8 September 2022, 9.30am - 11.30am All hosted through the UQ library, book through here."
  },
  {
    "objectID": "posts/2022-08-16-newsletter/index.html#aeo-forum",
    "href": "posts/2022-08-16-newsletter/index.html#aeo-forum",
    "title": "Newsletter | Analysing climate data with R + Interactive Maps + AEO Forum + more.",
    "section": "",
    "text": "The Advancing Earth Observation Forum will be held next week, 22nd ‚Äì 26th August at the Brisbane Convention and Exhibition Centre. This will be a great opportunity to connect with and learn from others in the earth observation community. Registrations are still open."
  },
  {
    "objectID": "posts/2022-08-16-newsletter/index.html#resbaz-postponed",
    "href": "posts/2022-08-16-newsletter/index.html#resbaz-postponed",
    "title": "Newsletter | Analysing climate data with R + Interactive Maps + AEO Forum + more.",
    "section": "",
    "text": "Unfortunately, the 2022 ResBaz conference has been postponed due to the Covid and flu surge. We will provide updates as they become available."
  },
  {
    "objectID": "posts/2022-08-16-newsletter/index.html#spatial-podcast",
    "href": "posts/2022-08-16-newsletter/index.html#spatial-podcast",
    "title": "Newsletter | Analysing climate data with R + Interactive Maps + AEO Forum + more.",
    "section": "",
    "text": "mapscaping is a spatial podcast where they discuss a lot of interesting geospatial topics that come up in our sessions, so definitely worth checking out."
  },
  {
    "objectID": "posts/2022-08-16-newsletter/index.html#geospatial-jobs",
    "href": "posts/2022-08-16-newsletter/index.html#geospatial-jobs",
    "title": "Newsletter | Analysing climate data with R + Interactive Maps + AEO Forum + more.",
    "section": "",
    "text": "Here are a few job vacancies from across the country that might be of interest. If you know of any vacancies, send them in and they will be included with the next newsletter.\nGraduate Spatial information officer, QLD government, Location flexible.\nGeospatial engineer, Nova Systems, Brisbane\nGIS technician, TerraLab, Victoria\nTeam Leader Conservation Planner / GIS analyst, Centre for Conservation Geography, Byron Bay."
  },
  {
    "objectID": "posts/2022-09-01-analysing-climate-data-with-r/index.html",
    "href": "posts/2022-09-01-analysing-climate-data-with-r/index.html",
    "title": "Analysing Climate data with R",
    "section": "",
    "text": "This post contains the scripts provided by Ralph Trancoso in the Analysing Climate Data in R workshop. The recording is also available, just email mitchel.rudge@uq.edu.au for access.\n\n1 Installing and loading the data, and the raster, ncdf4, rgdal, and ggplot2 packages, setting directory, loading gridded data\nTo follow this tutorial, you will need to download some prepared climate data.\nSave this link to somewhere on you computer, in our example the c drive, the unzip the folder.\nIf you don‚Äôt have the ‚Äòraster‚Äô and ncdf4 packages installed, install them:\n\ninstall.packages(\"raster\") # Installing the packages required for the workshop\ninstall.packages(\"ncdf4\")\ninstall.packages(\"rgdal\")\ninstall.packages(\"ggplot2\")\n\nNow load the raster package, and set the directory to where you stored the Rclim folder.\nSet your home directory, for example, if you put the rclim folder on you C drive:\nwriteClipboard(gsub(‚Äú\\\\‚Äù, ‚Äú/‚Äù, readClipboard())\n\nhome &lt;- \"C:/Rclim\"\n\nNow load the raster packages, and set your directory to where the climate data is.\n\nlibrary(raster)\nsetwd(home) #workshop dataset\n\nsetwd(paste0(home, \"/worldclim\")) # Set work directory to worldclim data\n#getwd() # get work directory\n#dir() # list files in the work directory\n?stack # what does stack do?\naus_temp &lt;- stack(\"tmean_australia_wc.nc\")  # Loading gridded #data as RasterStack\n\n\n\n2 Querying the RasterStack data and quick plot using raster::plot and raster::spplot\nBelow are a whole bunch of checks that you can run on the raster data set.\n\nncol(aus_temp) #check the number of columns\n\n[1] 554\n\nnrow(aus_temp) #check the number of rows\n\n[1] 551\n\nncell(aus_temp) #check the number of cells\n\n[1] 305254\n\nnlayers(aus_temp) #check the number of layers\n\n[1] 12\n\ndim(aus_temp) #check the dimensions (rows, columns, layers)\n\n[1] 551 554  12\n\nprojection(aus_temp) #check the projection\n\n[1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\nres(aus_temp) #check the resolution\n\n[1] 0.08333333 0.08333333\n\ninMemory(aus_temp) #check if the data is stored in memory\n\n[1] FALSE\n\nfromDisk(aus_temp) #check if the data was read from disk\n\n[1] TRUE\n\nnames(aus_temp) #check the names of the layers\n\n [1] \"X1\"  \"X2\"  \"X3\"  \"X4\"  \"X5\"  \"X6\"  \"X7\"  \"X8\"  \"X9\"  \"X10\" \"X11\" \"X12\"\n\n\nNow plot the rasters using the plot function:\n\nplot(aus_temp/10)\n\n\n\n\nOr use spplot:\n\nspplot(aus_temp/10) # lattice plot, returns a trellice \n\n\n\n\nEach layer represents a month of the year, from 1-12. So lets rename the layers and plot again.\n\nmonths &lt;- c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n            \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n\nnames(aus_temp) &lt;- months #set the layer names to months\n\nplot(aus_temp/10)\n\n\n\n\nOr, using spplot to create a lattice plot\n\nspplot(aus_temp/10) # lattice plot, returns a trellice  \n\n\n\n\n\n\n3 Calculating anomalies as gridded time-series and global average\nFirst, load the CMIP6 data set.\n\nsetwd(paste0(home, \"/CMIP6\")) # Set work directory to CMIP6 data\nproj_temp &lt;- stack(\"tas_Asea_ACCESS-ESM1-5_ssp370_r1i1p1f1_gr1.5_1950-2100.nc\")\nnames(proj_temp)\n\n  [1] \"X1\"   \"X2\"   \"X3\"   \"X4\"   \"X5\"   \"X6\"   \"X7\"   \"X8\"   \"X9\"   \"X10\" \n [11] \"X11\"  \"X12\"  \"X13\"  \"X14\"  \"X15\"  \"X16\"  \"X17\"  \"X18\"  \"X19\"  \"X20\" \n [21] \"X21\"  \"X22\"  \"X23\"  \"X24\"  \"X25\"  \"X26\"  \"X27\"  \"X28\"  \"X29\"  \"X30\" \n [31] \"X31\"  \"X32\"  \"X33\"  \"X34\"  \"X35\"  \"X36\"  \"X37\"  \"X38\"  \"X39\"  \"X40\" \n [41] \"X41\"  \"X42\"  \"X43\"  \"X44\"  \"X45\"  \"X46\"  \"X47\"  \"X48\"  \"X49\"  \"X50\" \n [51] \"X51\"  \"X52\"  \"X53\"  \"X54\"  \"X55\"  \"X56\"  \"X57\"  \"X58\"  \"X59\"  \"X60\" \n [61] \"X61\"  \"X62\"  \"X63\"  \"X64\"  \"X65\"  \"X66\"  \"X67\"  \"X68\"  \"X69\"  \"X70\" \n [71] \"X71\"  \"X72\"  \"X73\"  \"X74\"  \"X75\"  \"X76\"  \"X77\"  \"X78\"  \"X79\"  \"X80\" \n [81] \"X81\"  \"X82\"  \"X83\"  \"X84\"  \"X85\"  \"X86\"  \"X87\"  \"X88\"  \"X89\"  \"X90\" \n [91] \"X91\"  \"X92\"  \"X93\"  \"X94\"  \"X95\"  \"X96\"  \"X97\"  \"X98\"  \"X99\"  \"X100\"\n[101] \"X101\" \"X102\" \"X103\" \"X104\" \"X105\" \"X106\" \"X107\" \"X108\" \"X109\" \"X110\"\n[111] \"X111\" \"X112\" \"X113\" \"X114\" \"X115\" \"X116\" \"X117\" \"X118\" \"X119\" \"X120\"\n[121] \"X121\" \"X122\" \"X123\" \"X124\" \"X125\" \"X126\" \"X127\" \"X128\" \"X129\" \"X130\"\n[131] \"X131\" \"X132\" \"X133\" \"X134\" \"X135\" \"X136\" \"X137\" \"X138\" \"X139\" \"X140\"\n[141] \"X141\" \"X142\" \"X143\" \"X144\" \"X145\" \"X146\" \"X147\" \"X148\" \"X149\" \"X150\"\n[151] \"X151\"\n\n\nWe can see that these names make no sense. So the names relate to years, we can re-name each layer:\n\nyears &lt;- seq(1950, 2100, by=1)\nnames(proj_temp ) &lt;- years\nnames(proj_temp)\n\n  [1] \"X1950\" \"X1951\" \"X1952\" \"X1953\" \"X1954\" \"X1955\" \"X1956\" \"X1957\" \"X1958\"\n [10] \"X1959\" \"X1960\" \"X1961\" \"X1962\" \"X1963\" \"X1964\" \"X1965\" \"X1966\" \"X1967\"\n [19] \"X1968\" \"X1969\" \"X1970\" \"X1971\" \"X1972\" \"X1973\" \"X1974\" \"X1975\" \"X1976\"\n [28] \"X1977\" \"X1978\" \"X1979\" \"X1980\" \"X1981\" \"X1982\" \"X1983\" \"X1984\" \"X1985\"\n [37] \"X1986\" \"X1987\" \"X1988\" \"X1989\" \"X1990\" \"X1991\" \"X1992\" \"X1993\" \"X1994\"\n [46] \"X1995\" \"X1996\" \"X1997\" \"X1998\" \"X1999\" \"X2000\" \"X2001\" \"X2002\" \"X2003\"\n [55] \"X2004\" \"X2005\" \"X2006\" \"X2007\" \"X2008\" \"X2009\" \"X2010\" \"X2011\" \"X2012\"\n [64] \"X2013\" \"X2014\" \"X2015\" \"X2016\" \"X2017\" \"X2018\" \"X2019\" \"X2020\" \"X2021\"\n [73] \"X2022\" \"X2023\" \"X2024\" \"X2025\" \"X2026\" \"X2027\" \"X2028\" \"X2029\" \"X2030\"\n [82] \"X2031\" \"X2032\" \"X2033\" \"X2034\" \"X2035\" \"X2036\" \"X2037\" \"X2038\" \"X2039\"\n [91] \"X2040\" \"X2041\" \"X2042\" \"X2043\" \"X2044\" \"X2045\" \"X2046\" \"X2047\" \"X2048\"\n[100] \"X2049\" \"X2050\" \"X2051\" \"X2052\" \"X2053\" \"X2054\" \"X2055\" \"X2056\" \"X2057\"\n[109] \"X2058\" \"X2059\" \"X2060\" \"X2061\" \"X2062\" \"X2063\" \"X2064\" \"X2065\" \"X2066\"\n[118] \"X2067\" \"X2068\" \"X2069\" \"X2070\" \"X2071\" \"X2072\" \"X2073\" \"X2074\" \"X2075\"\n[127] \"X2076\" \"X2077\" \"X2078\" \"X2079\" \"X2080\" \"X2081\" \"X2082\" \"X2083\" \"X2084\"\n[136] \"X2085\" \"X2086\" \"X2087\" \"X2088\" \"X2089\" \"X2090\" \"X2091\" \"X2092\" \"X2093\"\n[145] \"X2094\" \"X2095\" \"X2096\" \"X2097\" \"X2098\" \"X2099\" \"X2100\"\n\n\nThe X is at the start of each of each year to ensure they are of type character.\nNow we can create a simple function to calculate the temperature anomaly.\n\nanomaly &lt;- function(x) {\n    anom &lt;-  x - mean(x[[32:61]]) # for reference period 1981-2100\n    names(anom) &lt;- seq(1950, 2100, by=1)    \n    return(anom)\n}\n\n\nT_anom &lt;- anomaly(proj_temp)\n\nNow plot the temperature anomaly, from 1:16 (1950 - 1965)\n\nspplot(T_anom[[1:16]])\n\n\n\n\nAnd from 2084 - 2100\n\nspplot(T_anom[[135:151]])\n\n\n\n\nThen we can calculate the average temperature anomaly\n\nT_anom_mean &lt;- as.data.frame(cbind(years, cellStats(T_anom, mean)))\nnames(T_anom_mean) &lt;- c(\"year\", \"T_anom_mean\")\n\nFinally, we can take a look at the average temperature anomaloy for the entire dataset.\n\nsetwd(home) # Set work directory to main folder\ndir.create(\"output\")\nsetwd(paste0(home, \"/output\")) #set directory to output\njpeg(file=\"anomaly_ts.jpeg\", height = 600,  width = 1000, res=150)\nplot(T_anom_mean$year, T_anom_mean$T_anom_mean, type = \"p\", pch = 19, \n     col = \"red\", xlab=\"year\", ylab=\"Projected temperature anomaly\", \n     main=\"ACCESS-ESM1-5 SSP370 - ref period:1981-2010\")\ndev.off()\n\npng \n  2 \n\n\nHave a look in the output folder, you should see something like this\n\n\n\n4 Handling regions as shapefiles\nHere, we load and plot a shapefile of the worlds country boundaries.\nHere, we load and plot a shapefile of the worlds country boundaries.\n\nlibrary(rgdal)\n\nsetwd(paste0(home, \"/shp\"))\ncountries = readOGR(dsn=\".\", layer=\"TM_WORLD_BORDERS_SIMPL-0.3\")\n\nOGR data source with driver: ESRI Shapefile \nSource: \"C:\\Users\\uqnwiggi\\OneDrive - The University of Queensland\\UQGAC\\Rclim\\shp\", layer: \"TM_WORLD_BORDERS_SIMPL-0.3\"\nwith 246 features\nIt has 11 fields\nInteger64 fields read as strings:  POP2005 \n\nplot(countries)\n\n\n\n\nOr use spplot to color by population:\n\nspplot(countries, \"POP2005\")\n\n\n\n\n\n\n5 Regionalizations of climate data - plotting time-series as line plot and bar chart\n\nlibrary(ggplot2)\n\nsetwd(paste0(home, \"/CMIP6\")) # Set work directory to CMIP6 data\nproj_temp &lt;- stack(\"tas_Asea_ACCESS-ESM1-5_ssp370_r1i1p1f1_gr1.5_1950-2100.nc\")\nproj_temp &lt;- proj_temp -273.15\nnames(proj_temp) &lt;- c(seq(1950,2100, by=1))\n\n## From your countries vector we read in, select a country customise your study area for the workshop analysis:\nmy_country &lt;- subset(countries, NAME == \"Australia\")\n\n\ndf&lt;- as.data.frame(countries@data)\n#fix(df) # to have a look at the dataframe of the countries\n\nmydf &lt;- structure(list(\nlongitude = c(153.0251, 145.7781, 149.1821, 146.8169, 139.4927, 144.2555), \nlatitude = c(-27.4698, -16.9186, -21.1425, -19.2590, -20.7256, -23.4422)), \n.Names = c(\"longitude\", \"latitude\"), class = \"data.frame\", row.names = c(NA, -6L))\nxy &lt;- mydf[,c(1,2)]\nspdf &lt;- SpatialPointsDataFrame(coords = xy, data = mydf, proj4string = CRS(\"+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0\"))\n\nFinally, plot the points we chose on the map of Australia.\n\nplot(my_country)\nplot(spdf, add=T, col=\"red\")\n\n\n\n\n\nproj_temp_cities &lt;- extract(proj_temp, spdf)\nproj_temp_cities &lt;- as.data.frame(proj_temp_cities)\nproj_temp_cities &lt;-as.data.frame(t(proj_temp_cities))\nproj_temp_cities$year &lt;- 1:nrow(proj_temp_cities)+1980\nnames(proj_temp_cities) &lt;- c(\"Brisbane\", \"Cairns\", \"Mackay\", \"Townsville\", \"Mount_Isa\", \"Longreach\", \"year\")\n\nWe can plot the projected temperature change of the cities we have selected.\nWe can plot the projected temperature change of the cities we have selected.\n\n#install.packages(\"reshape2\")\nlibrary(reshape2)\nproj_temp_cities_melt &lt;- melt(proj_temp_cities, id=\"year\")\n\nts1 &lt;- ggplot(proj_temp_cities_melt) +\n    geom_line(aes(x=year, y=value, colour=variable)) +\n    ggtitle(\"Projected average temperature ACCESS-ESM1.5 SSP3-7.0\") +\n    ylab(\"Temperature (¬∞C)\")  +\n    scale_color_brewer(name= \"cities\", palette=\"Set1\") +\n    theme_bw() \nts1\n\n\n\n\n\n\n6 Convertig gridcells to data frame within a study area mask and plotting boxplots in ggplot2\n\nsetwd(paste0(home, \"/IPCC\")) # Set work directory to CMIP6 data\ndir() \n\n [1] \"CMIP6 - Consecutive Dry Days (CDD) Change days - Warming 1.5¬∞C SSP5-8.5 (rel. to 1850-1900) - Annual (32 models).nc\"\n [2] \"CMIP6 - Consecutive Dry Days (CDD) Change days - Warming 2¬∞C SSP5-8.5 (rel. to 1850-1900) - Annual (32 models).nc\"  \n [3] \"CMIP6 - Consecutive Dry Days (CDD) Change days - Warming 3¬∞C SSP5-8.5 (rel. to 1850-1900) - Annual (32 models).nc\"  \n [4] \"CMIP6 - Consecutive Dry Days (CDD) Change days - Warming 4¬∞C SSP5-8.5 (rel. to 1850-1900) - Annual (19 models).nc\"  \n [5] \"CMIP6 - Maximum temperature (TX) Change deg C - Warming 1.5¬∞C SSP5-8.5 (rel. to 1850-1900) - Annual (27 models).nc\" \n [6] \"CMIP6 - Maximum temperature (TX) Change deg C - Warming 2¬∞C SSP5-8.5 (rel. to 1850-1900) - Annual (27 models).nc\"   \n [7] \"CMIP6 - Maximum temperature (TX) Change deg C - Warming 3¬∞C SSP5-8.5 (rel. to 1850-1900) - Annual (27 models).nc\"   \n [8] \"CMIP6 - Maximum temperature (TX) Change deg C - Warming 4¬∞C SSP5-8.5 (rel. to 1850-1900) - Annual (16 models).nc\"   \n [9] \"CMIP6 - Mean temperature (T) Change deg C - Warming 1.5¬∞C SSP5-8.5 (rel. to 1850-1900) - Annual (34 models).nc\"     \n[10] \"CMIP6 - Mean temperature (T) Change deg C - Warming 2¬∞C SSP5-8.5 (rel. to 1850-1900) - Annual (34 models).nc\"       \n[11] \"CMIP6 - Mean temperature (T) Change deg C - Warming 3¬∞C SSP5-8.5 (rel. to 1850-1900) - Annual (34 models).nc\"       \n[12] \"CMIP6 - Mean temperature (T) Change deg C - Warming 4¬∞C SSP5-8.5 (rel. to 1850-1900) - Annual (20 models).nc\"       \n[13] \"CMIP6 - Total precipitation (PR) Change % - Warming 1.5¬∞C SSP5-8.5 (rel. to 1850-1900) - Annual (33 models).nc\"     \n[14] \"CMIP6 - Total precipitation (PR) Change % - Warming 2¬∞C SSP5-8.5 (rel. to 1850-1900) - Annual (33 models).nc\"       \n[15] \"CMIP6 - Total precipitation (PR) Change % - Warming 3¬∞C SSP5-8.5 (rel. to 1850-1900) - Annual (33 models).nc\"       \n[16] \"CMIP6 - Total precipitation (PR) Change % - Warming 4¬∞C SSP5-8.5 (rel. to 1850-1900) - Annual (19 models).nc\"       \n\n# Mean temperature\ntemp_list &lt;- list.files(path= paste0(home, \"/IPCC\"), pattern = \"Mean temperature\", full.names = TRUE)\ntemp_GW &lt;- stack(temp_list)\nnames(temp_GW) &lt;- c(\"GW1.5\", \"GW2.0\", \"GW3.0\", \"GW4.0\") \nplot(temp_GW)\n\n\n\n\n\n#masking data outside country and converting grid to dataframe\n\ntemp_GW_country &lt;- as.data.frame(mask(temp_GW, my_country))\ndim(temp_GW_country)\n\n[1] 64800     4\n\nhead(temp_GW_country)   \n\n  GW1.5 GW2.0 GW3.0 GW4.0\n1    NA    NA    NA    NA\n2    NA    NA    NA    NA\n3    NA    NA    NA    NA\n4    NA    NA    NA    NA\n5    NA    NA    NA    NA\n6    NA    NA    NA    NA\n\ntemp_GW_country &lt;- na.omit(temp_GW_country)\ndim(temp_GW_country)\n\n[1] 699   4\n\n#install.packages(\"reshape2\")\nlibrary(reshape2)\ntemp_GW_country_m &lt;- melt(temp_GW_country)\n\nNo id variables; using all as measure variables\n\n#creating a boxplot of avg temp per global warming level on ggplot2\n\nbp1 &lt;- ggplot(temp_GW_country_m, aes(x=variable, y=value, fill=variable)) +\ngeom_boxplot()+\nxlab(\"Global warming level (¬∞C)\")+\nylab(\"Change in average surface temperature (¬∞C)\")+\nggtitle(paste(\"Change in average surface temperature per global warming level in \", my_country@data$NAME[1], sep=\"\")) +\ntheme_bw()\n\nbp1 &lt;- bp1 + scale_color_brewer(name= \"GW level\", palette=\"YlOrRd\") # why it does not work? - change from color to fill\nbp1\n\n\n\n\n\n\n7 Repeat the extraction and plot for precipitation and/or other metrics\n\n# Total precipitation\nprec_list &lt;- list.files(path= paste0(home, \"/IPCC\"), pattern = \"Total precipitation\", full.names = TRUE)\nprec_GW &lt;- stack(prec_list)\nnames(prec_GW) &lt;- c(\"GW1.5\", \"GW2.0\", \"GW3.0\", \"GW4.0\") \nplot(prec_GW)\n\n\n\n\n\n#masking data outside country and converting grid to dataframe\n\nprec_GW_country &lt;- as.data.frame(mask(prec_GW, my_country))\nprec_GW_GW_country &lt;- na.omit(prec_GW_country)\nprec_GW_country_m &lt;- melt(prec_GW_country)\n\nNo id variables; using all as measure variables\n\n#creating a boxplot of precipitation per global warming level on ggplot2\n\nbp2 &lt;- ggplot(prec_GW_country_m, aes(x=variable, y=value, fill=variable)) +\ngeom_boxplot()+\nxlab(\"Global warming level (¬∞C)\")+\nylab(\"Change in total precipitation (%)\")+\nggtitle(paste(\"Change in total precipitation per global warming level in \", my_country@data$NAME[1], sep=\"\")) +\ntheme_bw() +\nscale_fill_brewer(name= \"GW level\", palette=\"YlOrRd\")\nbp2\n\nWarning: Removed 256404 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n\nWhere to find climate data\nIPCC interactive atlas The authority for climate data is the IPCC. And the interactive atlas has the latest data https://interactive-atlas.ipcc.ch/regional-information\nWorldclim Global climate and weather data. WorldClim is a database of high spatial resolution global weather and climate data. https://www.worldclim.org/\nSILO gridded data for Australia until yesterday: https://longpaddock.qld.gov.au/silo/gridded-data/\nLongPaddock Provided by the Queensland Government, gridded data available for a range of variables in NetCDF and GeoTiff formats. The NetCDF datasets are arranged in annual blocks where each file contains all of the grids for the selected year and variable. https://longpaddock.qld.gov.au\nCMIP5 downscaled climate projections over Queensland High-resolution climate change projections for Queensland using dynamical downscaling of CMIP5 global climate models are available for download in gridded format with spatial resolution of 10 km at Terrestrial Ecosystem Research Network (TERN) https://longpaddock.qld.gov.au/qld-future-climate/data-info/tern/\n\n\nAbout the author\n\n \nRalph is a research scientist with expertise in climate change, ecohydrology, and deforestation impacts. Ralph is particularly interested in how climate and landscape changes affect the environment and impact society. He is a Principal Climate Change Scientist at the Department of Environment and Science (Queensland Government) and a Research Fellow at the School of Biological Sciences (University of Queensland)."
  },
  {
    "objectID": "posts/2022-09-09-newsletter/index.html",
    "href": "posts/2022-09-09-newsletter/index.html",
    "title": "Newsletter | Climate data blog post + problem solving session + conference in Fiji + more",
    "section": "",
    "text": "Last Thursday Ralph ran us through the basics of climate analysis in R. This was a really informative workshop that demystified climate data. All the scripts are now up on the blog, as well as a link to the materials. If you missed the workshop, the recording is up on cloudstor (let me know if you don‚Äôt have access).\n\n\n\nOur next workshop will be a free-range problem-solving session. So, if you have any questions or issues you think someone in the community could help with, please bring them along.\n\n\n\nYou heard right! The Pacific Geospatial Conference will be held in Suva, Fiji from the 28th of November till 2nd December. Here is the Conference Program. By all reports this is an excellent conference that would be interesting to members of this community.\n\n\n\nThe UQ library will be hosting a QGIS: Custom Maps on your Phone workshop on Thursday 22 September 2022, 9.30am - 11.30am.\nThis session is split into two main sections. First it will cover useful spatial data portals (freely accessing aerial photography, government spatial data, quality digital elevation models, and more), and then it will cover how to package this data for use in the field on a mobile phone.\n\n\n\nResearch Fellow ‚Äì Remote sensing riparian vegetation, CDU, Darwin\nTeam Leader Conservation Planner / GIS analyst, Centre for Conservation Geography, Byron Bay\nGIS Analyst / Senior GIS Analyst, Ecology Australia, Melbourne\nGraduate GIS Analyst, Biosis, Melbourne"
  },
  {
    "objectID": "posts/2022-09-09-newsletter/index.html#blog-post-analysing-climate-data-with-r-ralph-trancoso",
    "href": "posts/2022-09-09-newsletter/index.html#blog-post-analysing-climate-data-with-r-ralph-trancoso",
    "title": "Newsletter | Climate data blog post + problem solving session + conference in Fiji + more",
    "section": "",
    "text": "Last Thursday Ralph ran us through the basics of climate analysis in R. This was a really informative workshop that demystified climate data. All the scripts are now up on the blog, as well as a link to the materials. If you missed the workshop, the recording is up on cloudstor (let me know if you don‚Äôt have access)."
  },
  {
    "objectID": "posts/2022-09-09-newsletter/index.html#problem-solving-session",
    "href": "posts/2022-09-09-newsletter/index.html#problem-solving-session",
    "title": "Newsletter | Climate data blog post + problem solving session + conference in Fiji + more",
    "section": "",
    "text": "Our next workshop will be a free-range problem-solving session. So, if you have any questions or issues you think someone in the community could help with, please bring them along."
  },
  {
    "objectID": "posts/2022-09-09-newsletter/index.html#conference-in-fiji",
    "href": "posts/2022-09-09-newsletter/index.html#conference-in-fiji",
    "title": "Newsletter | Climate data blog post + problem solving session + conference in Fiji + more",
    "section": "",
    "text": "You heard right! The Pacific Geospatial Conference will be held in Suva, Fiji from the 28th of November till 2nd December. Here is the Conference Program. By all reports this is an excellent conference that would be interesting to members of this community."
  },
  {
    "objectID": "posts/2022-09-09-newsletter/index.html#uq-library-update-qgis-custom-maps-on-your-phone",
    "href": "posts/2022-09-09-newsletter/index.html#uq-library-update-qgis-custom-maps-on-your-phone",
    "title": "Newsletter | Climate data blog post + problem solving session + conference in Fiji + more",
    "section": "",
    "text": "The UQ library will be hosting a QGIS: Custom Maps on your Phone workshop on Thursday 22 September 2022, 9.30am - 11.30am.\nThis session is split into two main sections. First it will cover useful spatial data portals (freely accessing aerial photography, government spatial data, quality digital elevation models, and more), and then it will cover how to package this data for use in the field on a mobile phone."
  },
  {
    "objectID": "posts/2022-09-09-newsletter/index.html#jobs-vacancies",
    "href": "posts/2022-09-09-newsletter/index.html#jobs-vacancies",
    "title": "Newsletter | Climate data blog post + problem solving session + conference in Fiji + more",
    "section": "",
    "text": "Research Fellow ‚Äì Remote sensing riparian vegetation, CDU, Darwin\nTeam Leader Conservation Planner / GIS analyst, Centre for Conservation Geography, Byron Bay\nGIS Analyst / Senior GIS Analyst, Ecology Australia, Melbourne\nGraduate GIS Analyst, Biosis, Melbourne"
  },
  {
    "objectID": "posts/2022-11-03-build-blog-w-quarto/index.html",
    "href": "posts/2022-11-03-build-blog-w-quarto/index.html",
    "title": "Build a blog with Quarto, Git, and RStudio",
    "section": "",
    "text": "We will be covering some basics of multiple programming skills (Git/GitHub, R, Markdown, etc‚Ä¶) with the aim of empowering people to contribute to quarto websites such as this one - the Brisbane Spatial Share Community of Practice. The idea is to encourage community members to contribute material directly instead of funneling everything through a website administrator.\nBackground of the Community of Practice with founder, Mitchel Rudge. See the About page for more info.\nSo the group already has a website‚Ä¶ why another blog? Essentially, we found blogdown to be buggier than we wanted. Also, the folder structure was not very intutitive and are unique to each hugo theme. Finally, for the needs of our Spatial Share group the multi-lingual aspects of quarto were an important draw."
  },
  {
    "objectID": "posts/2022-11-03-build-blog-w-quarto/index.html#research-bazaar-queensland-2022",
    "href": "posts/2022-11-03-build-blog-w-quarto/index.html#research-bazaar-queensland-2022",
    "title": "Build a blog with Quarto, Git, and RStudio",
    "section": "Research Bazaar Queensland 2022",
    "text": "Research Bazaar Queensland 2022\nThis session was run as a workshop for ResBaz Queensland 2022. ResBaz is a global festival promoting digital literacy at the centre of modern research.\nWhat we will cover:\n\nQuarto basics\nGit and GitHub basics\nHow to create/edit a post on a quarto website\n\nI have pieced this together using many other resources on the above which are mentioned throughout. This is also coming from a learning-as-we-go approach and by no means expert opinion. Thank you to Mitch and Christina for their help with this tutorial and workshop!\nWhat you will need:\n\nInstallations - Git, R Windows mac, RStudio (Quarto should be installed included with recent &gt;2022.07 versions of RSTudio - otherwise download Quarto separately)\nGitHub account (free) with your login and personal access token (PAT) details handy\nNetlify account (free)"
  },
  {
    "objectID": "posts/2022-11-03-build-blog-w-quarto/index.html#creating-a-blog-with-quarto",
    "href": "posts/2022-11-03-build-blog-w-quarto/index.html#creating-a-blog-with-quarto",
    "title": "Build a blog with Quarto, Git, and RStudio",
    "section": "Creating a blog with quarto",
    "text": "Creating a blog with quarto\n\nWhat is quarto?\nQuarto is a ‚Äúmulti-language, next-generation version of R markdown from RStudio.‚Äù It is designed to be programming language (compatible with R, python, Julia, and more‚Ä¶?) and tool agnostic (RStudio, VSCode, jupyter, Observable). In this tutorial, we are focusing on Quarto and RStudio.\nThe basic model of Quarto publishing is taking a source document and rendering it to a variety of outputs like html, pdfs, and Word. The backend process is illustrated below. The key difference from R Markdown is that it uses pandoc. For those interested in the details, I would recommend the Welcome to Quarto! 2 hr workshop on Youtube led by Tom Mock at RStudio.\n\nFAQ for R Markdown users.\n\n\nMake a quarto blog in RStudio\nRStudio has quarto built-in with recent versions after 2022.07. Go to File &gt; New Project or the R in a blue cube under ‚ÄòEdit‚Äô and you will see Quarto options right there!\n\nLet‚Äôs click on the Quarto Blog option. In the next window, name you project (e.g., myblog), select where to save the project with the Browse button, and ensure ‚ÄòCreate a git repository‚Äô is checked. More on git later.\n\nThe default project is populated with some example files and folders. The open index.qmd file is the ‚Äòhome page‚Äô of the blog that will list all the posts. The .qmd is the file extension for a Quarto file just like .Rmd for R Markdown. Go ahead and change the first title field in the YAML. For instance, change ‚Äòmy blog‚Äô to ‚ÄòMy Blog‚Äô.\n\n\n\n\n\n\nNote\n\n\n\nYAML stands for ‚ÄòYet Another Markup Language‚Äô and is delineated by a triple dash (‚Äî) at the beginning and end of the YAML section. This is where you define settings for you quarto document/post.\n\n\nNow, let‚Äôs look at one of the template posts. In the Files pane click on posts &gt; welcome &gt; index.qmd. Here we can see a template for a ‚ÄòWelcome‚Äô post.\nA recent addition to RStudio is that you can view the ‚ÄòSource‚Äô (top left pane) as either the Source code or Visual editor. These views can be swapped by toggling the buttons at the top right of the pane. The Source code (blue box below) displays all the source code for your quarto file such are your R code (in chunks - none in this example) and Markdown narrative text.\n\n\n\n\n\n\nNote\n\n\n\nA code chunck is delineated by three backticks (button to the left of 1) and {} with the language for the code chunk (R, python etc.). Click the green C+ button to the left of run to add a new R chunck. Or keyboard shortcut Ctrl/Cmd + Alt + I.\n\n\n\nThe Visual editor displays a rendered version of your quarto file - more like what it will look like when the site is published. This is more similar to writing in a text editor like Word. You can also see there some extra formatting buttons in the Visual Editor like bold/italics and super/subscripts. The ‚ÄòTable‚Äô function is also a welcome edition as formatting tables in Markdown is very finicky and tedious.\nTry inserting a table or super/subscript (Format &gt; Text &gt; Strikethrough/Superscript/Subscript/Smallcaps) in the Visual editor and then toggle to the Source code. Now you can see the associated Markdown code for whatever you just did! Super handy.\nNow, open the _quarto.yml file. Here we can see the project type (a website), some website formatting (the navigation bar), and some other customization fields. Update the title field to match the title we edited earlier. In my case, it was ‚ÄòMy Blog‚Äô. Feel free to edit other fields such as your GitHub, Twitter, LinkedIn profile links etc. You can also change the theme to one of many built-in themes.\n\n\nDon‚Äôt forget to save your files when you make changes. If the file name in the tab is red - that means you have unsaved changes.\n\n\n\n\n\n\n\nNote\n\n\n\nDon‚Äôt forget to save your files when you make changes. If the file name in the tab is red - that means you have unsaved changes.\n\n\nThe Visual editor is pretty cool, but it‚Äôs not exactly the same as previewing your website before publishing. Click on the Render button at the top and see what happens.\nA preview of your blog should have popped-up in a web browser. You can navigate like you would a website to see all the features. Go to the about page - we can see it is the default page with the Quarto blog project. If you‚Äôd like, open the about.qmd page in the project directory and make a change. Add some text, delete a link (like LinkedIn) and then save your changes.\nCongrats - you‚Äôve made a blog!\n\n\n\nCelebrate! Credit: http://www.reactiongifs.com/cheering-minions/\n\n\nIn the top right view pane of RStudio, you can see Render and Background Jobs tabs. If you‚Äôd like to get an idea of what is happening in the background, check out these tabs. In the Backgroun Jobs tab, there is a red stop sign in the top right corner to stop previewing your site.\nBut how do I share it with the world?? First, we will need to version control our project with git and store it in a remote repository."
  },
  {
    "objectID": "posts/2022-11-03-build-blog-w-quarto/index.html#git---what-is-it",
    "href": "posts/2022-11-03-build-blog-w-quarto/index.html#git---what-is-it",
    "title": "Build a blog with Quarto, Git, and RStudio",
    "section": "Git - what is it?",
    "text": "Git - what is it?\nA version control software (think track changes) useful for collaborating on and sharing code.\nGit is the software itself that records changes to a set of files locally. There are several hosting platforms that are like online repositories (think Dropbox, Google Drive, etc.) that work with Git: Bitbucket, GitLab, and GitHub to name a few.\nThese platforms not only allow for version control but also to collaborate, organize, and back up projects.\n\nIn this case, we will be using GitHub to access the website files, make some changes (i.e., add a post), and then incorporate those changes back to the website repository on GitHub which will automatically update the website itself. üôå\nThere are lots of fabulous and free resources online that go more into depth on Git:\n\nIf you need to be convinced to use Git for version control see this article and Happy Git and GitHub for the useR to git started both by Git/R guru Jenny Bryan.\nSee Caitie Kuempel‚Äôs R Ladies Brisbane presentation on getting started with GitHub in RStudio.\n\n\nGit Terminology\nRepository/repo - where a project is stored in GitHub. Think of it like a folder holding all the relevant documents that you can version control, view history, and add collaborators. The repository or repo holds all the relevant files for the website - most of which we will not touch.\nCommit - is one or more changes to a file or set of files that you are asking GitHub to keep track of.\nPush - sending your committed changes to a remote repository on GitHub. Local changes updated on the GitHub website where other people can access.\nPull - incorporating and merging changes. An edit on the remote repository on GitHub can be pulled to a local repository.\nDiff - difference, or changes made that are visible as insertions/deletions for a commit.\nMain - the default branch you are on. Master has recently updated to main, but they are the same thing. You are more likely to come across master on older resources. Jenny Bryan strongly urges you to create a new branch to work off of which requires using command line. For the purpose of contributing to a quarto website, I will forgo covering this as it is unlikely more than one person will be contributing at the same time.\nOrigin - the remote repo online from which you have cloned your local copy from.\n\n\n\nHow committing goes‚Ä¶ Credit: xkcd comics\n\n\n\n\nUsethis on our blog project\n\n\n\n\n\n\nHappy Git and GitHub for the useR is the online bible of using git with R by legendary Jenny Bryan. This book covers all the basics in details and provides workflows and troubleshooting Jenny Bryan has a whole chapters on what is covered below. Ch 7 Configure Git, Ch 9 PATs, and Ch 17.3 Create and connect a GitHub repo. It‚Äôs an excellent resource that I would highly encourage you to check out.\n\n\n\nUsethis is a workflow package designed to automate repetitive tasks for package development and project setup. Here, we‚Äôll be using it for the latter.\n\n\n\n\n\n\nNote\n\n\n\nYou might need to install usethis. Check by search in the Packages window in the bottom right pane in RStudio.\ninstall.packages(\"usethis\")\nlibrary(usethis)\nOr you can install the development version: install.packages(\"devtools\") if you don‚Äôt have devtools. devtools::install_github(‚Äúr-lib/usethis‚Äù)\n\n\n\nConfigure Git\nIf you have never used Git before you will need to configure your account. We can do this in R with usethis using your GitHub username and email. These must be the details associated with your GitHub account. You will only need to do this once.\nusethis::use_git_config(user.name = \"Jane Doe\", user.email = \"jane@example.org\")\nIf you are not sure if you have configured Git, you can check with running usethis::edit_git_config() in the console. This will open your .gitconfig file and you can check the [user] details. If these are not right you can go ahead and change theme in the file and save.\n\n\n\nCreate a personal access token\nWe must also configure a personal access token or PAT with usethis. Type usethis::create_github_token() in the console. You will either need to log into GitHub or confirm your password. usethis will automatically select some scopes - which ones are selected? We can use these defaults - click the big, green Generate token button at the bottom.\nLeave this page open or copy this and keep it a text file/password manager. We will use it again later. You can even use gitcreds::gitcreds_set() to store your PAT now.\n\n\n\n\n\n\nWe can also save our PAT in RStudio with: gitcreds::gitcreds_set()\nThis should result in the following message. Enter password or token: ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nAdding new credentials‚Ä¶\nRemoving credentials from cache‚Ä¶\nDone.\n\n\n\n\n\nSetup the remote GitHub repository\nNow let‚Äôs setup a remote repo on GitHub using usethis::use_github. This section is straight out of 17.3 Happy git with R - create and connect a github repo with usethis.\nHow did that go? Did you get an error?\nDon‚Äôt worry. We‚Äôve forgotten to do an initial commit a common mistake. Click on the Commit button in the Git pane, select all (Ctrl/Cmd + Shift + A) and click Stage. We are going to stage all the files to push to our remote GitHub repo. This might be a bit slow since it is multiple files so be patient. Then, make a commit message in the top right of the window. Click Commit. This creates a record (or anchor in line with the climbing analogy below) in our project that we can view and go back to if we ever need to.\n\nOnce that‚Äôs complete, let‚Äôs usethis::use_github() in the Console again. How did it go this time?\nHopefully, you see something like this: \nNotice that part of what use_github() does is push the master or main branch to GitHub. (Master is legacy, but you may still see it around.)\nNow go to the link there use_github() set the remote too (in the third line with a ‚úîÔ∏è). You will see everything that you‚Äôve commited is now in the online, remote repo! Have a look around. You can navigate, add a README.md file, and even edit files in GitHub.\nLet‚Äôs go ahead and edit a file. Go to the ‚ÄòWelcome‚Äô post, click on the pencil ‚úèÔ∏è to edit, and make a change. At the bottom you can add a commit message then commit your change.\nCool, but how do we get these changes we made on our remote repo into our local repo on our computer? This is where pull comes in. Back in the RStudio Git pane, you should see a blue down arrow labelled Pull. Click this button. This will update your local repo on your computer with the edit we made in GitHub. Look at your files and see if you can find the change we just made online. Pulling is especially important if you are working on a collaborative project. In general, you want to pull first before pushing to ensure you are working with the most up-to-date version of the project. Even if you are not in a collaborative project, it is a good habit to get into.\nThose are the Git basics! In general, you want to use the following workflow when using git.\n\nMake a change in your files and save. The changes will not show up in the Git pane unless you save the changes.\nStage the files you would like to commit. Can stage multiple files at a time.\nWrite a concise, informative commit message and press the Commit button. If you don‚Äôt get any glaringly obvious Error messages, it‚Äôs probably all good.\nPull pressing the blue down arrow before pushing with the green up arrow.\nRepeat!"
  },
  {
    "objectID": "posts/2022-11-03-build-blog-w-quarto/index.html#publishing",
    "href": "posts/2022-11-03-build-blog-w-quarto/index.html#publishing",
    "title": "Build a blog with Quarto, Git, and RStudio",
    "section": "Publishing",
    "text": "Publishing\nThere are several options for publishing your quarto blog outlined here. Such as Quarto Pub and Netlify.\n\n\n\n\n\n\nNote\n\n\n\nQuarto Pub is a free publishing service for quarto content. It requires having a login and an access token. This is a relatively straight forward way to get your blog online. There are some limitations to the size and everything published on Quarto Pub is publicly visible. The link above outlines the steps to publish with Quart Pub. This method does not involve using git.\n\n\nFor this workshop, we will focus on Netlify. In your Netlify account, click on the teal Add new site button.\nImport an existing project from a GitHub repository. You will probably need to configure your Netlify on GitHub. Can either configure all repositories or pick a specific repository.\n\nAuthorize Quarto to access Netlify.\n\nNow that we‚Äôve connected our GitHub to Netlify, go to the Site settings. At the bottom ‚ÄòPublish directory‚Äô section put the _site/ folder where your website is rendered.\n\nIt will take a few minutes to deploy your website. Netlify automatically generates a random url like ‚Äúlighthearted-travesseiro-492jfg3‚Äù which can be changed in the ‚ÄòDomain settings‚Äô. Or course, you can also use/pay for a custom url to remove the ‚Äònetlify.app‚Äô at the end of the url.\nNow, make a change in the welcome post, render the site, commit the changes, and push the changes in the .qmd file. Look at your GitHub remote repo to check that the changes are there. Now check your website - did it update as well?\nAs we defined above, unless you also commited the updated contents of the _site/ foldr the website will not have updated. This folder is where all the rendered outputs are that are used to build the site on Netlify. Commit the updated _site/ folder and push. Now check your website again.\n\n\n\nCheers"
  },
  {
    "objectID": "posts/2022-11-03-build-blog-w-quarto/index.html#add-a-map",
    "href": "posts/2022-11-03-build-blog-w-quarto/index.html#add-a-map",
    "title": "Build a blog with Quarto, Git, and RStudio",
    "section": "Add a map",
    "text": "Add a map\nThis is a spatial community of practice - let‚Äôs add a map of ResBazQld 2022 to a post using leaflet.\n\n# install.packages(\"leaflet\")\nlibrary(leaflet)\nlibrary(magrittr)\nleaflet() %&gt;% \n  addTiles() %&gt;% # default background map\n  addMarkers(lat = -27.552, lng = 153.0535,\n             popup = \"Location of ResBazQld 2022\")"
  },
  {
    "objectID": "posts/2022-11-03-build-blog-w-quarto/index.html#troubleshooting-tbc",
    "href": "posts/2022-11-03-build-blog-w-quarto/index.html#troubleshooting-tbc",
    "title": "Build a blog with Quarto, Git, and RStudio",
    "section": "Troubleshooting TBC",
    "text": "Troubleshooting TBC"
  },
  {
    "objectID": "posts/2022-11-03-build-blog-w-quarto/index.html#resources",
    "href": "posts/2022-11-03-build-blog-w-quarto/index.html#resources",
    "title": "Build a blog with Quarto, Git, and RStudio",
    "section": "Resources",
    "text": "Resources\nQuarto\n\nBuilding a blog with quarto, Youtube video, website - Isabella Valasquez, Rstudio\nWelcome to Quarto!, Tom Mock, RStudio"
  },
  {
    "objectID": "posts/2023-01-22-newsletter-happy-new-year-february-meet-up-r-training/index.html",
    "href": "posts/2023-01-22-newsletter-happy-new-year-february-meet-up-r-training/index.html",
    "title": "Newsletter | Happy New Year! | February meet-up | R Training",
    "section": "",
    "text": "2023 is going to be a big year for the UQ Geospatial CoP.\nIn addition to monthly coding get-togethers, we‚Äôd like to kick-start the year with a survey to get input on the following:\n\nWho is the geospatial community? Tell us what your career stage is so we can better cater to the demographic of the community.\nWhat should our new name be? We‚Äôd like to re-brand the community, mainly to remove ‚ÄòUQ‚Äô from the title as over the years the group has grown to include individuals from other universities and organisations.\nEstablishing a Code of conduct, mission statement, and organising committee for the group. More details to come, but if you are interested in volunteering your time to help organise activities throughout the year, please let us know in the survey and/or reach out by email. Roles may include: Speaker facilitator, Website and communications manager, Listserv manager, Treasurer/funding manager, etc.\nSpeaker and/or topic recommendations for 2023. Let us know who the heroes are in your field that you‚Äôd like us to invite to speak to the Geospatial community. Alternatively, let us know what topics you are interested in learning about at one of the sessions.\n\n\n\n\nOn February 9th (2-4 pm) we‚Äôll have our first hybrid coding meet-up of the year, how to calculate areas accurately in R using both vector and raster data.\n\n\n\nUpcoming R courses on statistics and spatial analysis (Feb 14th-17th, register here):"
  },
  {
    "objectID": "posts/2023-01-22-newsletter-happy-new-year-february-meet-up-r-training/index.html#geospatial-cop-newsletter-january-2023",
    "href": "posts/2023-01-22-newsletter-happy-new-year-february-meet-up-r-training/index.html#geospatial-cop-newsletter-january-2023",
    "title": "Newsletter | Happy New Year! | February meet-up | R Training",
    "section": "",
    "text": "2023 is going to be a big year for the UQ Geospatial CoP.\nIn addition to monthly coding get-togethers, we‚Äôd like to kick-start the year with a survey to get input on the following:\n\nWho is the geospatial community? Tell us what your career stage is so we can better cater to the demographic of the community.\nWhat should our new name be? We‚Äôd like to re-brand the community, mainly to remove ‚ÄòUQ‚Äô from the title as over the years the group has grown to include individuals from other universities and organisations.\nEstablishing a Code of conduct, mission statement, and organising committee for the group. More details to come, but if you are interested in volunteering your time to help organise activities throughout the year, please let us know in the survey and/or reach out by email. Roles may include: Speaker facilitator, Website and communications manager, Listserv manager, Treasurer/funding manager, etc.\nSpeaker and/or topic recommendations for 2023. Let us know who the heroes are in your field that you‚Äôd like us to invite to speak to the Geospatial community. Alternatively, let us know what topics you are interested in learning about at one of the sessions.\n\n\n\n\nOn February 9th (2-4 pm) we‚Äôll have our first hybrid coding meet-up of the year, how to calculate areas accurately in R using both vector and raster data.\n\n\n\nUpcoming R courses on statistics and spatial analysis (Feb 14th-17th, register here):"
  },
  {
    "objectID": "posts/2023-02-05-calculating-areas-with-vector-and-raster-data-in-r/index.html",
    "href": "posts/2023-02-05-calculating-areas-with-vector-and-raster-data-in-r/index.html",
    "title": "Calculating areas with vector and raster data in R",
    "section": "",
    "text": "This short tutorial will focus on area calculation in R using both vector and raster data. Use the following code to install all necessary packages\ninstall.packages('terra', 'tmap', 'sf', 'tidyverse', 'exactextractr', 'rnaturalearthdata', 'microbenchmark')\nDisclaimer: the author of these notes is by no means an expert on this topic. This is merely a compilation of information I‚Äôve gathered while going down a googling ‚Äòrabbit-hole‚Äô. So please get in touch if anything in the notes should be corrected.\n\nArea in 3-dimensions vs.¬†2-dimensions\nCan I calculate area accurately using a latitude-longitude geographic coordinate reference system (GCS), or should I use a projected coordinate reference system (PCS)?\nGCS‚Äôs represent the Earth‚Äôs surface in 3-dimensions and have angular, lat-long units (Figure 1). They are defined by a datum (reference points associated with an ellipsoid (a model of the size and shape of the Earth)), a prime meridian, and an angular unit of measure. See here for more detail. Calculating area in a 3-D GCS requires geodesic (i.e., ellipsoidal) computations, which can be complex and computationally expensive.\nPCS‚Äôs flatten a GCS into 2-dimensions representing linear units (e.g., metres), helping to make area computations less complex and therefore faster, but also causing some distortion of the Earth‚Äôs surface. There are many PCS‚Äôs available to use (see here), some of which preserve area (i.e., equal-area projections).\n.\n\nWhich coordinate reference system to use?\nHere we‚Äôll focus on how coordinate reference systems influence area calculations. There is a common misconception that you cannot calculate area accurately when in a lat-long GCS, but this is not true. For example, read the help documentation from the expanse function in {terra}:\n\n?terra::expanse\n\nstarting httpd help server ... done\n\n\n‚ÄòFor vector data, the best way to compute area is to use the longitude/latitude CRS. This is contrary to (erroneous) popular belief that suggest that you should use a planar coordinate reference system.‚Äô\nMore generally however, choosing the best coordinate reference system will depend on the scale and location of your spatial data, and the goal of your analysis. Sometimes you will need to use multiple CRS‚Äôs to complete various tasks associated with a single analysis. We won‚Äôt dive into the details here, but find a good overview things to consider when choosing a CRS here.\nNote also that it‚Äôs always preferable to leave your raster data in it‚Äôs native projection if your analysis requires use of values stored in the raster pixels. This is because projecting rasters involves calculating new pixel values via resampling, which results in slight inaccuracies and potential loss of information. There are several methods for resampling depending on they type of data stored in the raster pixels (e.g., categorical vs.¬†continuous), see descriptions here.\n\n\n\nVector area calculations and the new-ish s2 geometry library\nSince 2021, {sf} (versions &gt;1) uses Google‚Äôs s2 spherical geometry engine by default when representing vector polygons that are in a lat-long GCS. Previously {sf} would use a planar geometry engine called GEOS, meaning that if you were in a 3-D lat-long GCS, {sf} was assuming flat, planar coordinates. This would result in warning messages like:\nalthough coordinates are longitude/latitude, st_intersects assumes that they are planar\nNow, when the s2 spherical geometry is on and your data is in a 3-D lat-long GCS, {sf} performs processes and computations on a sphere, rather than in 2-D. The advantages of using the s2 geometry library instead of GEOS are outlined in detail here: r-spatial.github.io/sf/articles/sf7.html. If your data is in a 2-D PCS, {sf} reverts to the planar GEOS geometry engine, assuming flat coordinates (so make sure you‚Äôre using a projection that preserves area).\n\nShould I have s2 turned on when calculating area in a geographic CRS?\nWhile the s2 geometry library offers many advantages, when it comes to area there is a speed vs.¬†accuracy trade-off. When s2 is on, {sf} calculates area assuming a spherical approximation of the Earth. But the Earth isn‚Äôt a perfect sphere, it‚Äôs a spheroid. It bulges at the equator.\nWhen s2 is off, {sf} performs geodesic (i.e., ellipsoidal) area calculations using the high precision GeographicLib that uses a spheroid to represent the Earth‚Äôs size and shape. Geodesic calculations will be more accurate than those performed with s2 on, as s2 assumes a spherical representation of the Earth. But geodesic computations more complex, and time costly.\nLet‚Äôs examine the speed vs.¬†accuracy trade-off. We‚Äôll start by loading some vector polygons representing countries of the world and check if the geometries are valid.\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.0     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.0\n‚úî ggplot2   3.4.1     ‚úî tibble    3.1.8\n‚úî lubridate 1.9.2     ‚úî tidyr     1.3.0\n‚úî purrr     1.0.1     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(sf)\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\nlibrary(tmap)\noptions(scipen=999)\ntmap_mode('view')\n\ntmap mode set to interactive viewing\n\nsf_use_s2() # check if s2 is on (it should be as default when sf is loaded)\n\n[1] TRUE\n\ndat &lt;- rnaturalearthdata::countries50 %&gt;% # load country polygons for the world\n  st_as_sf() %&gt;% # turn into a simple features dataframe\n  filter(admin == 'Sudan') # filter for a single country\nst_is_valid(dat) # check if geometry is valid\n\n[1] FALSE\n\n\nThe geometry is invalid. Compared to GEOS, s2 has strict polygon conformance that if violated will render invalid geometries. For vector data created in older versions of sf, this can be a bit of a headache. First thing is to try fixing the geometry.\n\ndat_v &lt;- dat %&gt;% st_make_valid()\nst_is_valid(dat_v) # check if geometry is valid\n\n[1] TRUE\n\nqtm(dat_v)\n\n\n\n\n\n\nIt works in this case - the geometry is valid. Now, let‚Äôs compare area estimates with s2 on and off.\n\narea_s2 &lt;- st_area(dat_v)/1e6 # area in km2, s2 on by default\nsf_use_s2(FALSE) # turn off s2\n\nSpherical geometry (s2) switched off\n\narea_GeoLib &lt;- st_area(dat_v)/1e6 # area in km2, s2 off, sf calculates geodesic area using GeographicLib\narea_s2 - area_GeoLib # subtract to get the difference in area estimates \n\n6381.861 [m^2]\n\n\nThere is an ~6000 km2 difference in area (note the m2 indicated in the output is incorrect, because we divided by 1e6 to convert m2 to km2). The geodesic calculation using the GeographicLib (i.e, s2 turned off) will be the most accurate estimate because it doesn‚Äôt assume the Earth is a perfect sphere. But which is faster? Let‚Äôs use {microbenchmark} to find out.\n\nlibrary(microbenchmark)\n\nmicrobenchmark(\n  {sf_use_s2(TRUE); area_s2 &lt;- st_area(dat_v)},\n  {sf_use_s2(FALSE); area_GeoLib &lt;- st_area(dat_v)})\n\nUnit: milliseconds\n                                                       expr     min       lq\n      {     sf_use_s2(TRUE)     area_s2 &lt;- st_area(dat_v) } 12.6714 14.65875\n {     sf_use_s2(FALSE)     area_GeoLib &lt;- st_area(dat_v) } 17.3233 19.41860\n     mean  median      uq     max neval cld\n 15.29274 15.2873 15.9106 20.1017   100  a \n 20.68676 20.3460 21.2224 32.6545   100   b\n\n\nSo computing geodesic area takes more time. In this case it‚Äôs only a few milliseconds, so we would turn s2 off and opt for the most accurate, albeit slower, area calculation.\nHow do geodesic area computations using a lat-long GCS compare to an equal-area PCS, like mollweide?\n\ndat_p &lt;- dat_v %&gt;% st_transform('ESRI:54009')\narea_mollweide &lt;- st_area(dat_p)/1e6 # sf uses planar geometry library GEOS\narea_mollweide - (area_GeoLib/1e6)\n\n10556.15 [m^2]\n\n\nThere is an ~ 10000 km2 difference in area estimates, much larger than the difference between area on a sphere (s2 on) and on a spheroid (s2 off, geodesic estimates using GeographicLib).\n\n\nIn summary‚Ä¶\nWhen calculating area of vector polygons in a lat-long GCS using {sf}, it‚Äôs best to turn-off s2 and let {sf} calculate area on an ellipsoid (instead of a spherical approximation of the Earth). However, if speed is an issue and you‚Äôre not as worried about accuracy, leave s2 on and calculate area using on a sphere.\nIf you‚Äôre calculating area in an equal-area PCS using {sf}, it doesn‚Äôt matter if s2 is on or off, area will be calculated using GEOS planar geometry engine.\n\n\n\nRaster area calculations\nLike {sf}, if your raster data is in a lat-long GCS, {terra} will compute area in meters squared using geodesic computations based on your spatial data‚Äôs ellipsoid with GeographicLib.\nNote that {terra} also has a class for vector data, called ‚ÄòSpatVectors‚Äô (raster objects are called ‚ÄòSpatRasters‚Äô). So we can also calculate area of vector data with {terra}. {sf} should provide the same estimate of area if s2 is turned off, meaning it is using GeographicLib for lat-long GCS computations. Let‚Äôs see if this is the case.\nNote: The workflows below are borrowed from here.\nFirst, we‚Äôll create a polygon and a raster.\n\nlibrary(terra)\n\nterra 1.7.18\n\n\n\nAttaching package: 'terra'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n#| warning: false\nsf_use_s2(TRUE) # make sure s2 is on (we already have sf loaded)\nr &lt;- rast(nrows=10, ncols=10, xmin=0, ymin=0, xmax=10, ymax=10) # make a raster\nr &lt;- init(r, 8) # initialise a spatraster with values\np &lt;- vect('POLYGON ((2 2, 7 6, 4 9, 2 2))', crs = crs(r)) # make a vector polygon\nr_mask &lt;- mask(r, p) # mask the raster to the polygon\nqtm(r) + qtm(r_mask) + qtm(st_as_sf(p))\n\n\n\n\n\nTry toggling the different layers on and off in the interactive map to see what the mask does.\nNote that the raster pixels cover a larger area than the polygon. By default ‚Äòmask‚Äô includes all raster pixels that the polygon touches. We could use the disagg function to reduce the resolution and better match the area of the raster to the area of the polygon (but we won‚Äôt do that here).\nNow let‚Äôs compare area estimates with {terra} and {sf}.\n\narea_t &lt;- expanse(p, unit = 'km') # here 'terra' is computing geodesic area (i.e., based on a spheroid, not planar)\nsf_use_s2(FALSE) # turn off s2 to uses the GeographicLig\n\nSpherical geometry (s2) switched off\n\narea_sf &lt;- st_area(st_as_sf(p))/1e6 # here 'sf' using GEOS to calculate area\nround(area_t - as.numeric(area_sf))\n\n[1] 0\n\n\nGreat, so as we expect, no difference in area estimates between {terra} and {sf} when s2 geometry library is turned off.\n\nCompute area of raster data using {terra}\nLet‚Äôs use {terra} to compute the area of a raster that intersects with a vector polygon. We‚Äôll use the functions cellSize and mask from {terra}.\n\nr_area &lt;- cellSize(r_mask, unit=\"km\") # create a raster where pixel values are area in km2\nqtm(r_area) + qtm(st_as_sf(p)) # have a quick look\n\n\n\n\ne &lt;- extract(r_area, p, exact=TRUE) # extract pixel values that intersect with vector polygon, and get fraction of each cell that is covered\narea_t - sum(e$area * e$fraction) # calculate difference between area of the vector polygon, and the estimate of area of the 'raster'\n\n[1] 165.0552\n\n\nThere is a 165 km2 difference in area calculated using a raster vs.¬†vector polygon.\n\n\nCompute area of raster data using {exactextractr}\n{exactextractr} can be used to perform raster computations within vector polygon areas very quickly, and it can do similar things as {terra}‚Äôs extract function that we used above. However, it uses a spherical approximation of the Earth, rather than a spheroid. Let‚Äôs compare estimates of area between {terra} and {exactextractr}.\n\nlibrary(exactextractr)\n\ne2 &lt;- exact_extract(r_area, st_as_sf(p))[[1]]\nsum(e$area * e$fraction) - sum(e2$value * e2$coverage_fraction)\n\n[1] 18.47752\n\n\nThe difference in area is very minor, ~18 km2. Which is faster?\n\nmicrobenchmark(extract(r_area, p, exact=TRUE), \n               exact_extract(r_area, st_as_sf(p)))\n\nUnit: milliseconds\n                               expr       min        lq     mean    median\n   extract(r_area, p, exact = TRUE)  3.849801  4.482651  5.85123  4.683601\n exact_extract(r_area, st_as_sf(p)) 22.067002 25.155950 28.42134 25.769050\n       uq      max neval cld\n  5.01555  96.5561   100  a \n 28.03330 220.6204   100   b\n\n\n{terra} wins! So in this case there is no speed vs.¬†accuracy trade-off. {terra} is faster and more accurate, because it is not assuming that the Earth is a sphere to make area calculations.\n\n\nComparing PCS area computations between {terra} and {sf}\nDo estimates of vector polygon area differ between {terra} and {sf} when in an equal-area PCS, like mollweide?\n\narea_t_mollweide &lt;- expanse(vect(dat_p), 'km', transform = F)\nround(as.numeric(area_mollweide) - area_t_mollweide)\n\n[1] 0\n\n\nNo difference.\n\n\nIn summary‚Ä¶\nIf using a lat-long GCS to calculate area in R, {terra} and {sf} will provide the same estimates for vector polygons, as long as s2 is turned off. For raster areas in polygons, {terra} is faster and more accurate than {exactextractr}."
  },
  {
    "objectID": "posts/2023-03-10-newletter/index.html",
    "href": "posts/2023-03-10-newletter/index.html",
    "title": "Newsletter | R spatial basics | How to measure area | Survey results",
    "section": "",
    "text": "We will have a combined session for our March workshop. In the first half, Christina will run through some R spatial basics.\nCheck out the material here. And if you want to follow along on your own computer during the workshop, download the code and data here. In the second half, we will host a problem-solving session. So bring along your spatial questions (ideally send them through before the workshop).\n\n\n\nChristina showed us how to measure area in R - PROPERLY. And it‚Äôs not as simple as you might think. Very much recommend watching the recording and reading the material if you ever need to measure area using R. (Please get in touch if you need the link.)\n\n\n\nThere are quite a few geospatial jobs going at the moment:\n\nEnvironmental GIS analyst, Eco-logical, Brisbane\nSenior Spatial Scientist and Spatial Scientist, NT government, Darwin\nSpatial Systems and Data Officer, Central Land Council, Alice Springs\nNatural capital data science internship (paid), Bush Heritage, remote\nData integration and stratification intern (paid), Bush Heritage, remote\n\n\n\n\nWe have been transferring all of the content from the old website made with blogdown, onto the new site made with QUARTO. Please check out the new site, and let Nick (n.wiggins at library dot uq dot edu dot au), who has been leading the switch, know if you find any issues. We also expect to move to a new domain shortly that reflects our new name.\nWhich brings me to‚Ä¶.\n\n\n\nThanks to everyone who took to time to fill out our survey. Here are a few of the main results.\n\n\n\n\n\n\nWe put it to the group to vote on a new name, mainly because a lot of our members are not affiliated with UQ ‚Äì so UQ geospatial didn‚Äôt seem appropriate anymore. The most popular result was Geospatial Community, so we will stick with that.\n\n\n\nFrom the survey results and some discussion among the organising committee, we also drafted a mission statement:\n‚ÄúOur mission is to build a supportive and inclusive community where students and professionals can grow their geospatial skills together.‚Äù\nI hope this captures what the group is all about; please get in touch to suggested changes.\n\n\n\nThere were some really interesting and topical suggestions for future workshops:\n\nHow to handle big spatial data on the HPC ‚Äì learn the basics of transferring data and scripts to the HPC, writing a bash shell script to submit a batch job to the HPC, and parallel processing to speed things up\nPros and Cons of different GIS software, e.g., ArcGIS, QGIS, R, python, etc\nWhat is GDAL and why is it important for spatial data scientists to know about?\nHow to model spatial data and deal with spatial autocorrelation\nRemote-sensing image analysis using machine learning We will do our best to find presenters to deliver workshops around these themes.\n\n\n\n\n\nHuge thanks to\n\nChristina Buelow\nCatherine Kim\nNick Wiggins\nMitchel Rudge\nTanya Dodgen\nAnnie Nguyen\nJason Dail\nKaiching Cheong\n\nfor volunteering to help out on the organising committee. Excited for the year ahead üòä"
  },
  {
    "objectID": "posts/2023-03-10-newletter/index.html#geospatial-community-march-newsletter",
    "href": "posts/2023-03-10-newletter/index.html#geospatial-community-march-newsletter",
    "title": "Newsletter | R spatial basics | How to measure area | Survey results",
    "section": "",
    "text": "We will have a combined session for our March workshop. In the first half, Christina will run through some R spatial basics.\nCheck out the material here. And if you want to follow along on your own computer during the workshop, download the code and data here. In the second half, we will host a problem-solving session. So bring along your spatial questions (ideally send them through before the workshop).\n\n\n\nChristina showed us how to measure area in R - PROPERLY. And it‚Äôs not as simple as you might think. Very much recommend watching the recording and reading the material if you ever need to measure area using R. (Please get in touch if you need the link.)\n\n\n\nThere are quite a few geospatial jobs going at the moment:\n\nEnvironmental GIS analyst, Eco-logical, Brisbane\nSenior Spatial Scientist and Spatial Scientist, NT government, Darwin\nSpatial Systems and Data Officer, Central Land Council, Alice Springs\nNatural capital data science internship (paid), Bush Heritage, remote\nData integration and stratification intern (paid), Bush Heritage, remote\n\n\n\n\nWe have been transferring all of the content from the old website made with blogdown, onto the new site made with QUARTO. Please check out the new site, and let Nick (n.wiggins at library dot uq dot edu dot au), who has been leading the switch, know if you find any issues. We also expect to move to a new domain shortly that reflects our new name.\nWhich brings me to‚Ä¶.\n\n\n\nThanks to everyone who took to time to fill out our survey. Here are a few of the main results.\n\n\n\n\n\n\nWe put it to the group to vote on a new name, mainly because a lot of our members are not affiliated with UQ ‚Äì so UQ geospatial didn‚Äôt seem appropriate anymore. The most popular result was Geospatial Community, so we will stick with that.\n\n\n\nFrom the survey results and some discussion among the organising committee, we also drafted a mission statement:\n‚ÄúOur mission is to build a supportive and inclusive community where students and professionals can grow their geospatial skills together.‚Äù\nI hope this captures what the group is all about; please get in touch to suggested changes.\n\n\n\nThere were some really interesting and topical suggestions for future workshops:\n\nHow to handle big spatial data on the HPC ‚Äì learn the basics of transferring data and scripts to the HPC, writing a bash shell script to submit a batch job to the HPC, and parallel processing to speed things up\nPros and Cons of different GIS software, e.g., ArcGIS, QGIS, R, python, etc\nWhat is GDAL and why is it important for spatial data scientists to know about?\nHow to model spatial data and deal with spatial autocorrelation\nRemote-sensing image analysis using machine learning We will do our best to find presenters to deliver workshops around these themes.\n\n\n\n\n\nHuge thanks to\n\nChristina Buelow\nCatherine Kim\nNick Wiggins\nMitchel Rudge\nTanya Dodgen\nAnnie Nguyen\nJason Dail\nKaiching Cheong\n\nfor volunteering to help out on the organising committee. Excited for the year ahead üòä"
  },
  {
    "objectID": "posts/2023-03-22-testimonial-chris-mancini/index.html",
    "href": "posts/2023-03-22-testimonial-chris-mancini/index.html",
    "title": "Community groups for Continuing Professional Development",
    "section": "",
    "text": "This post is provided by Chris Mancini (Civil Engineer, RPEQ, MIEAust, CPEng, NER), and to help inform the Geospatial Community including University Staff, Students, Researchers, and Industry Professionals about considering professional credentials and continuing professional development (CPD)."
  },
  {
    "objectID": "posts/2023-03-22-testimonial-chris-mancini/index.html#what-is-continuing-professional-development",
    "href": "posts/2023-03-22-testimonial-chris-mancini/index.html#what-is-continuing-professional-development",
    "title": "Community groups for Continuing Professional Development",
    "section": "What is Continuing Professional Development?",
    "text": "What is Continuing Professional Development?\nContinuing professional development can generally be described as increasing and enhancing knowledge and skills. Individual members of professional institutions that are awarded the status and credential of being Chartered and/or Registered are required to complete and maintain continuing professional development each year."
  },
  {
    "objectID": "posts/2023-03-22-testimonial-chris-mancini/index.html#how-do-you-use-cpd",
    "href": "posts/2023-03-22-testimonial-chris-mancini/index.html#how-do-you-use-cpd",
    "title": "Community groups for Continuing Professional Development",
    "section": "How do you use CPD?",
    "text": "How do you use CPD?\nIf you would like your university activities to be considered with CPD, keep a log including the date, the CPD activity/topic/provider, how the activity has extended your knowledge, and the number of hours. Academic research, postgraduate courses, university training, and university communities that advance and enhance your knowledge and skills, including the Geospatial Community, and the UQ R Users Group (UQRUG), and UQ Library Training, and other UQ communities including the UQ Research Computing Centre and Hacky Hour UQ (for example AI and machine learning) can be considered with CPD.\nWhile professional institutions and statutory bodies require approved university degrees (for example accredited with the Sydney Accord and/or the Washington Accord) and work experience (5 to 15+ years) to grant approval for membership status, CPD is also required for Chartered and Registered credentials. This is typically at least 150 hours across 1 to 3 years."
  },
  {
    "objectID": "posts/2023-03-22-testimonial-chris-mancini/index.html#what-types-of-professional-bodies-use-cpd",
    "href": "posts/2023-03-22-testimonial-chris-mancini/index.html#what-types-of-professional-bodies-use-cpd",
    "title": "Community groups for Continuing Professional Development",
    "section": "What types of professional bodies use CPD?",
    "text": "What types of professional bodies use CPD?\nIn Australia and New Zealand examples of professional credentials and professional bodies requiring CPD include (and are not limited to) Chartered Accountants with Chartered Accountants Australia and New Zealand (CAANZ); and Chartered Professional Engineers (CPEng) with Engineers Australia (EA) and Engineering New Zealand (ENZ). In Britain, the Royal Geographical Society includes Chartered Geographers and Geospatial Professionals, CGeog (GIS). I respect that there is a lot involved with and required for a professional and statutory body and institution to establish and maintain credentialing.\nEngineers Australia (EA) covers all of Australia. Each State and Territory of Australia has independent statutory and professional bodies that also require Professional Registration. The Board of Professional Engineers of Queensland (BPEQ) requires approved Registered Professional Engineers of Queensland (RPEQ) to maintain and enhance knowledge and skills with CPD (Continuing Professional Development - Board of Professional Engineers Queensland).\nTypes of CPD can be the following with Engineers Australia (CPD requirements | Engineers Australia).\n\nany tertiary course taken as an individual course or for a formal post-graduate award\nshort courses, workshops, seminars, conferences, technical inspections and meetings\nworkplace learning activities that extend competence in your area of practice or area of engineering\nprivate study that extends your knowledge and skill\nservice to the engineering profession\npreparation and presentation of materials for courses and conferences\ntertiary teaching or academic research\nstructured activities that meet the objectives of the CPD policy\n\nGroups such as the Geospatial Community who run regular workshops are a great resource for professionals looking to complete requirements for CPD."
  },
  {
    "objectID": "posts/2023-03-23-r-spatial-basics/index.html",
    "href": "posts/2023-03-23-r-spatial-basics/index.html",
    "title": "R Spatial Basics",
    "section": "",
    "text": "R Spatial basics with Dr.¬†Christina Buelow\nYou can find a direct link to the below content here, and download the code and data here."
  },
  {
    "objectID": "posts/2023-04-07-April-Newsletter/index.html",
    "href": "posts/2023-04-07-April-Newsletter/index.html",
    "title": "Newsletter | April Workshop Rasters and Random Forests",
    "section": "",
    "text": "Dr Jacinta Holloway-Brown will be providing a workshop on using the Stochastic spatial random forest SS-RF method for filling missing data in satellite images due to clouds, and also going over using raster2data for working with spatial image files (such as rasters), and converting these to dataframes in R in order to perform statistical analyses.\n\n\n\n\nIn March Dr Christina Buelow gave an awesome overview of the basics of analysing spatial data in R. You can find the full details of the workshop here, and keep an eye out for the recording on our YouTube channel (it will be linked on our website when it‚Äôs up and running).\n\n\n\n\n\nQGIS: Custom Maps on your Phone - Tuesday 11th April 2023, 9:30-11:30am\nQGIS: Vector Analysis - Tuesday 18th April 2023, 9:30-11:30am\nUQ R Usergroup (UQRUG) - Wednesday 26th April 2023, 12-2pm\n\n\n\nFind more details and registration here.\nDate: July 2nd 2023\nTime: 9:00-16:00\nLocation: Griffith University Gold Coast Campus, Building/Room G14_1.13B\nCost: $30.\nNote This isn‚Äôt a spatial workshop, but if you‚Äôre interested in statistical analysis and R programming, this will be a good one. Spots are limited so please only register if you are sure you can make it.\n\n\n\n\n\n\nMay 10-12, Adelaide South Australia\nAnnual conference put on by the Geospatial Council of Australia bringing together national and international delegates within and outside the geospatial Industry."
  },
  {
    "objectID": "posts/2023-04-07-April-Newsletter/index.html#next-workshop-friday-april-21st-1-3pm",
    "href": "posts/2023-04-07-April-Newsletter/index.html#next-workshop-friday-april-21st-1-3pm",
    "title": "Newsletter | April Workshop Rasters and Random Forests",
    "section": "",
    "text": "Dr Jacinta Holloway-Brown will be providing a workshop on using the Stochastic spatial random forest SS-RF method for filling missing data in satellite images due to clouds, and also going over using raster2data for working with spatial image files (such as rasters), and converting these to dataframes in R in order to perform statistical analyses."
  },
  {
    "objectID": "posts/2023-04-07-April-Newsletter/index.html#previous-workshop",
    "href": "posts/2023-04-07-April-Newsletter/index.html#previous-workshop",
    "title": "Newsletter | April Workshop Rasters and Random Forests",
    "section": "",
    "text": "In March Dr Christina Buelow gave an awesome overview of the basics of analysing spatial data in R. You can find the full details of the workshop here, and keep an eye out for the recording on our YouTube channel (it will be linked on our website when it‚Äôs up and running)."
  },
  {
    "objectID": "posts/2023-04-07-April-Newsletter/index.html#training",
    "href": "posts/2023-04-07-April-Newsletter/index.html#training",
    "title": "Newsletter | April Workshop Rasters and Random Forests",
    "section": "",
    "text": "QGIS: Custom Maps on your Phone - Tuesday 11th April 2023, 9:30-11:30am\nQGIS: Vector Analysis - Tuesday 18th April 2023, 9:30-11:30am\nUQ R Usergroup (UQRUG) - Wednesday 26th April 2023, 12-2pm\n\n\n\nFind more details and registration here.\nDate: July 2nd 2023\nTime: 9:00-16:00\nLocation: Griffith University Gold Coast Campus, Building/Room G14_1.13B\nCost: $30.\nNote This isn‚Äôt a spatial workshop, but if you‚Äôre interested in statistical analysis and R programming, this will be a good one. Spots are limited so please only register if you are sure you can make it."
  },
  {
    "objectID": "posts/2023-04-07-April-Newsletter/index.html#conference",
    "href": "posts/2023-04-07-April-Newsletter/index.html#conference",
    "title": "Newsletter | April Workshop Rasters and Random Forests",
    "section": "",
    "text": "May 10-12, Adelaide South Australia\nAnnual conference put on by the Geospatial Council of Australia bringing together national and international delegates within and outside the geospatial Industry."
  },
  {
    "objectID": "posts/2023-04-21-satellite-image-analysis-pipeline/index.html",
    "href": "posts/2023-04-21-satellite-image-analysis-pipeline/index.html",
    "title": "Satellite Image Analysis Pipeline in R",
    "section": "",
    "text": "Landsat image from Dr.¬†Jacinta Holloway-Brown‚Äôs materials\n\n\nDr Jacinta Holloway-Brown will be providing a workshop discussing the satellite image analysis pipeline from start to finish with examples. Steps covered will be:\n\nHow to search for and download free satellite images from US Geological Survey Earth Explorer\nHow to convert a satellite image to a data frame. Includes how to calculate a vegetation index and derive land cover classes in R. Tutorial: raster2data\nHow to fit a spatial random forest to satellite image data. This will include a brief overview of decision trees, random forest and stochastic spatial random forest methods. Code to fit spatial random forest will be provided and shared with the group.\nHow to fit stochastic spatial random forest SS-RF to get predictions of land cover for missing observations with associated probabilities.\n\nFill out the form (see ‚¨ÜÔ∏è) to get the invitation for the workshop."
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "December Newsletter | End of Year re-cap\n\n\n\n\n\n\nnewsletter\n\n\n\n\n\n\n\n\n\nDec 23, 2023\n\n\nMitch\n\n\n\n\n\n\n\n\n\n\n\n\nOctober Newsletter | Drop-in help virtual help session | Workshop: Large raster manipulation\n\n\n\n\n\n\nnewsletter\n\n\n\n\n\n\n\n\n\nOct 1, 2023\n\n\nChristina\n\n\n\n\n\n\n\n\n\n\n\n\nCreating inset maps using ‚Äòtmap‚Äô\n\n\n\n\n\n\nblog\n\n\ntutorial\n\n\nR\n\n\n\n\n\n\n\n\n\nSep 30, 2023\n\n\nDr Jackson Stockbridge\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember Newsletter | Spatial Autocorrelation | Drop-in help session | Upcoming training\n\n\n\n\n\n\nnewsletter\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nChristina\n\n\n\n\n\n\n\n\n\n\n\n\nAugust Newsletter | Upcoming workshop: Spatial High Performance Computing | In-person catch-up | Geospatial conferences\n\n\n\n\n\n\nnewsletter\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\nChristina\n\n\n\n\n\n\n\n\n\n\n\n\nJuly Newsletter | Learn Google Earth Engine this month\n\n\n\n\n\n\nnewsletter\n\n\n\n\n\n\n\n\n\nJul 4, 2023\n\n\nChristina\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting spatial data from web map servers in R\n\n\n\n\n\n\nblog\n\n\ntutorial\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 3, 2023\n\n\nDr Jackson Stockbridge\n\n\n\n\n\n\n\n\n\n\n\n\nJune Newsletter | Drop-in sessions | More Python this month\n\n\n\n\n\n\nnewsletter\n\n\n\n\n\n\n\n\n\nJun 1, 2023\n\n\nChristina\n\n\n\n\n\n\n\n\n\n\n\n\nNewsletter | Python + Folium | Help Sessions are Back!\n\n\n\n\n\n\nnewsletter\n\n\n\n\n\n\n\n\n\nMay 16, 2023\n\n\nAnnie\n\n\n\n\n\n\n\n\n\n\n\n\nNewsletter | April Workshop Rasters and Random Forests\n\n\n\n\n\n\nnewsletter\n\n\n\n\n\n\n\n\n\nApr 12, 2023\n\n\nNicholas Wiggins\n\n\n\n\n\n\n\n\n\n\n\n\nR Spatial Basics\n\n\n\n\n\n\nWorkshop\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\nChristina Buelow\n\n\n\n\n\n\n\n\n\n\n\n\nCommunity groups for Continuing Professional Development\n\n\n\n\n\n\ntestimonial\n\n\n\n\n\n\n\n\n\nMar 22, 2023\n\n\nChris Mancini\n\n\n\n\n\n\n\n\n\n\n\n\nNewsletter | R spatial basics | How to measure area | Survey results\n\n\n\n\n\n\nnewsletter\n\n\n\n\n\n\n\n\n\nMar 10, 2023\n\n\nMitchel Rudge\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating areas with vector and raster data in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nChristina Buelow\n\n\n\n\n\n\n\n\n\n\n\n\nNewsletter | Happy New Year! | February meet-up | R Training\n\n\n\n\n\n\nnewsletter\n\n\n\n\n\n\n\n\n\nJan 22, 2023\n\n\nChristina Buelow\n\n\n\n\n\n\n\n\n\n\n\n\nBuild a blog with Quarto, Git, and RStudio\n\n\nA workshop given at the Research Bazaar Queensland 2022.\n\n\n\nwebsite\n\n\n\n\n\n\n\n\n\nNov 3, 2022\n\n\nCatherine Kim\n\n\n\n\n\n\n\n\n\n\n\n\nNewsletter | Climate data blog post + problem solving session + conference in Fiji + more\n\n\n\n\n\n\nnewsletter\n\n\n\n\n\n\n\n\n\nSep 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysing Climate data with R\n\n\n\n\n\n\nR\n\n\nclimate\n\n\nraster\n\n\n\n\n\n\n\n\n\nSep 1, 2022\n\n\nRalph Trancoso\n\n\n\n\n\n\n\n\n\n\n\n\nNewsletter | Analysing climate data with R + Interactive Maps + AEO Forum + more.\n\n\n\n\n\n\nnewsletter\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive mapping with RShiny\n\n\n\n\n\n\ntutorial\n\n\nspatial\n\n\nvisualisation\n\n\n\n\n\n\n\n\n\nJul 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nNewsletter | Mapping interactively + R for ecologists + Data wrangling + more.\n\n\n\n\n\n\nnewsletter\n\n\n\n\n\n\n\n\n\nJul 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nProblem Solving Session II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nProblem Solving Session I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nCloud computing with Open Data Cube and Python\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2022\n\n\nTim Devereux\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to tidy spatial networks\n\n\n\n\n\n\nspatial\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nMar 31, 2022\n\n\nSt√©phane Guillou (UQ Library)\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Raster Analysis with R\n\n\n\n\n\n\nspatial\n\n\nR\n\n\nterra\n\n\n\n\n\n\n\n\n\nFeb 24, 2022\n\n\nMitch Rudge\n\n\n\n\n\n\n\n\n\n\n\n\nHow to contribute a post (using blogdown on our old site)\n\n\n\n\n\n\nwebsite\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 23, 2021\n\n\nCatherine Kim\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a geospatial blog with blogdown (on our old site)\n\n\n\n\n\n\ntutorial\n\n\nspatial\n\n\nvisualisation\n\n\n\n\n\n\n\n\n\nOct 28, 2021\n\n\nGeospatial Community\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#upcoming-workshop",
    "href": "index.html#upcoming-workshop",
    "title": "Welcome to the Geospatial Community",
    "section": "Upcoming Workshop",
    "text": "Upcoming Workshop\n\nHappy Holidays! See you in 2024!"
  },
  {
    "objectID": "index.html#last-workshop",
    "href": "index.html#last-workshop",
    "title": "Welcome to the Geospatial Community",
    "section": "Last Workshop",
    "text": "Last Workshop\n\n\n\n\n  \n\n\n\n\nNewsletter | Python + Folium | Help Sessions are Back!\n\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2023\n\n\nNicholas Wiggins\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-05-04-May-Newsletter/index.html",
    "href": "posts/2023-05-04-May-Newsletter/index.html",
    "title": "Newsletter | Python + Folium | Help Sessions are Back!",
    "section": "",
    "text": "Petition to increase Australian PhD Stipends\n\n\n\n\n\nPlease consider signing this e-petition to increase PhD stipends to minimum wage. Petition reason from the online petition is copied below.\nPetition Reason\nPhD students contribute enormously to Australia‚Äôs research every year, with their time accounting for 56% the total research output. They are the future professors and industry professionals of this country, responsible for driving innovation and ensuring strong international collaborations and competition. Their contribution is often rewarded with a stipend intended to cover the cost of living, with a minimum rate set by the Australian government. Currently, this scholarship rate is $29,863 per year for PhD students to live on while usually working more than 40 hrs per week on their research. Not only is this below minimum wage, but it is even forcing students to live below the poverty line. With this treatment, students are incentivised to accept positions abroad, reject higher degrees by research, or quit midway through their degree due to financial distress. Australia is actively discouraging higher education and slowing its economic growth and productivity by underpaying PhD students. We believe the government should support higher education and ensure a competitive standing for Australia. The way to do this is by setting a fair, liveable rate for PhD scholarships, which is at least equivalent to the minimum wage.\n\n\n\n\n\n\n\n\nDr Tony Howes¬†(UQ, School of Chemical Engineering) will be sharing how Python and Folium (a wrapper for Python of the JavaScript Leaflet library) work together to create an interactive map. The data will be from the Brisbane Dawn Nav, May 5th MapRun. Workshop materials will be in a Jupyter notebook and downloading¬†Anaconda¬†is recommended if you wish to follow along.\n\n\n\n\nIn April, Dr Jacinta Holloway-Brown presented a very informative and interesting workshop that outlined her use of Stochastic spatial random forest SS-RF methods for filling missing data in satellite images due to clouds, and also went over using raster2data for working with spatial image files (such as rasters), and converting these to dataframes in R in order to perform statistical analyses. Expect the blog post and recording associated with this workshop to be available shortly."
  },
  {
    "objectID": "posts/2023-05-04-May-Newsletter/index.html#geospatial-community-updates",
    "href": "posts/2023-05-04-May-Newsletter/index.html#geospatial-community-updates",
    "title": "Newsletter | Python + Folium | Help Sessions are Back!",
    "section": "Geospatial Community Updates",
    "text": "Geospatial Community Updates\n\nFirst Thursday help sessions\nIf you have a geospatial problem (or are good at answering them), we will be trialling help sessions on the first Thursday of each month. We will do our best to have people from a variety of backgrounds answer questions and will circulate a Teams link over email.\n\n\nWebsite changes\nYou will have noticed some major changes to the website, which now includes an easy way to join the group, and a calendar so everyone can quickly see when the next workshop will be. A massive thanks to Nick Wiggins for all the work in getting this together."
  },
  {
    "objectID": "posts/2023-05-04-May-Newsletter/index.html#geospatial-jobs",
    "href": "posts/2023-05-04-May-Newsletter/index.html#geospatial-jobs",
    "title": "Newsletter | Python + Folium | Help Sessions are Back!",
    "section": "Geospatial Jobs",
    "text": "Geospatial Jobs\nGIS Consultant, WSP Australia, Brisbane GIS Consultant, Arcadis, Brisbane Spatial Officers, PowerLink Queensland, Brisbane Spatial Data Officer, Unitywater, Moreton Bay GIS | Data and Analytics, EY, Brisbane city / flexible Biodiversity Modeller / Spatial Ecologist, CSIRO, Canberra"
  },
  {
    "objectID": "posts/2023-05-04-May-Newsletter/index.html#training",
    "href": "posts/2023-05-04-May-Newsletter/index.html#training",
    "title": "Newsletter | Python + Folium | Help Sessions are Back!",
    "section": "Training",
    "text": "Training\n\nUQ Library Training\nUQ R User Group (UQRUG) - Wednesday 31st May 2023, 12-2pm QGIS: Introduction to Mapping - Friday 19th May 2023, 9:30 am - 11:30 am QGIS: Custom Maps on your phone - Friday 26th May 2023, 9:30 am - 11:30 am QGIS: Vector Analysis - Friday 2nd June 2023, 9:30 am - 11:30 am\n\n\nExperimental Design and Data Analysis for Testing Casual Hypotheses in R\nFind more details and registration here. Date: July 2nd 2023 Time: 9:00-16:00 Location: Griffith University Gold Coast Campus, Building/Room G14_1.13B Cost: $30. Note: This isn‚Äôt a spatial workshop, but if you‚Äôre interested in statistical analysis and R programming, this will be a good one. Spots are limited so please only register if you are sure you can make it."
  },
  {
    "objectID": "index.html#last-post",
    "href": "index.html#last-post",
    "title": "Welcome to the Geospatial Community",
    "section": "Last Post",
    "text": "Last Post\n\n\n\n\n\n\n\n\n\n\nDecember Newsletter | End of Year re-cap\n\n\n\n\n\n\n\n\n\n\n\nDec 23, 2023\n\n\nMitch\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-03-10-newletter/index.html#petition-to-increase-australian-phd-stipends",
    "href": "posts/2023-03-10-newletter/index.html#petition-to-increase-australian-phd-stipends",
    "title": "Newsletter | R spatial basics | How to measure area | Survey results",
    "section": "Petition to increase Australian PhD Stipends",
    "text": "Petition to increase Australian PhD Stipends\nPlease consider signing this e-petition to increase PhD stipends to minimum wage. Petition reason from the online petition is copied below.\nPetition Reason\nPhD students contribute enormously to Australia‚Äôs research every year, with their time accounting for 56% the total research output. They are the future professors and industry professionals of this country, responsible for driving innovation and ensuring strong international collaborations and competition. Their contribution is often rewarded with a stipend intended to cover the cost of living, with a minimum rate set by the Australian government. Currently, this scholarship rate is $29,863 per year for PhD students to live on while usually working more than 40 hrs per week on their research. Not only is this below minimum wage, but it is even forcing students to live below the poverty line. With this treatment, students are incentivised to accept positions abroad, reject higher degrees by research, or quit midway through their degree due to financial distress. Australia is actively discouraging higher education and slowing its economic growth and productivity by underpaying PhD students. We believe the government should support higher education and ensure a competitive standing for Australia. The way to do this is by setting a fair, liveable rate for PhD scholarships, which is at least equivalent to the minimum wage.\n:::\n\nGeospatial Jobs\nThere are quite a few geospatial jobs going at the moment:\n\nEnvironmental GIS analyst, Eco-logical, Brisbane\nSenior Spatial Scientist and Spatial Scientist, NT government, Darwin\nSpatial Systems and Data Officer, Central Land Council, Alice Springs\nNatural capital data science internship (paid), Bush Heritage, remote\nData integration and stratification intern (paid), Bush Heritage, remote\n\n\n\nNew Website!\nWe have been transferring all of the content from the old website made with blogdown, onto the new site made with QUARTO. Please check out the new site, and let Nick (n.wiggins at library dot uq dot edu dot au), who has been leading the switch, know if you find any issues. We also expect to move to a new domain shortly that reflects our new name.\nWhich brings me to‚Ä¶.\n\n\nSurvey results\nThanks to everyone who took to time to fill out our survey. Here are a few of the main results.\n\nWho are we?\n\n\n\nWhat should we be called?\nWe put it to the group to vote on a new name, mainly because a lot of our members are not affiliated with UQ ‚Äì so UQ geospatial didn‚Äôt seem appropriate anymore. The most popular result was Geospatial Community, so we will stick with that.\n\n\nMission statement\nFrom the survey results and some discussion among the organising committee, we also drafted a mission statement:\n‚ÄúOur mission is to build a supportive and inclusive community where students and professionals can grow their geospatial skills together.‚Äù\nI hope this captures what the group is all about; please get in touch to suggested changes.\n\n\nWhat people want to learn about?\nThere were some really interesting and topical suggestions for future workshops:\n\nHow to handle big spatial data on the HPC ‚Äì learn the basics of transferring data and scripts to the HPC, writing a bash shell script to submit a batch job to the HPC, and parallel processing to speed things up\nPros and Cons of different GIS software, e.g., ArcGIS, QGIS, R, python, etc\nWhat is GDAL and why is it important for spatial data scientists to know about?\nHow to model spatial data and deal with spatial autocorrelation\nRemote-sensing image analysis using machine learning We will do our best to find presenters to deliver workshops around these themes.\n\n\n\n\nOrganising committe\nHuge thanks to\n\nChristina Buelow\nCatherine Kim\nNick Wiggins\nMitchel Rudge\nTanya Dodgen\nAnnie Nguyen\nJason Dail\nKaiching Cheong\n\nfor volunteering to help out on the organising committee. Excited for the year ahead üòä"
  },
  {
    "objectID": "posts/2023-06-01-june-newsletter/index.html#geospatial-community-june-newsletter",
    "href": "posts/2023-06-01-june-newsletter/index.html#geospatial-community-june-newsletter",
    "title": "June Newsletter | Drop-in sessions | More Python this month",
    "section": "",
    "text": "Drop-in sessions are back this Thursday, so bring your thorniest spatial questions and we‚Äôll help problem solve. In the first half of the session Dr Jackson Stockbridge will give a brief overview of how to access web mapping services (WMS) and web feature services (WFS) in R.\n\n\n\nJoin us for an exciting session where Dr C√©sar Herrara Acosta (Griffith University) will take you on a new dive into the dynamic world of the Python programming language ecosystem! Building upon previous sessions, C√©sar will showcase Python libraries for (1) analysis and visualization of spatial data (e.g.¬†geopandas, datashader, holoviews) and (2) basic computer vision algorithms (OpenCV). C√©sar will also make use of the incredibly versatile Jupyter Notebooks and Jupyter Lab frameworks, which empowers users with seamless workflows for code development, data exploration, and interactive presentations.\n\n\n\nLast month, Dr Tony Howes (UQ, School of Chemical Engineering) ran an excellent workshop on how to make an interactive map using Python and Folium."
  },
  {
    "objectID": "posts/2023-06-01-june-newsletter/index.html#geospatial-community-updates",
    "href": "posts/2023-06-01-june-newsletter/index.html#geospatial-community-updates",
    "title": "June Newsletter | Drop-in sessions | More Python this month",
    "section": "Geospatial Community Updates",
    "text": "Geospatial Community Updates\n\n\nGroup changes\nwebsite updates\nsurveys\nideas for upcoming workshops\ntestimonials\norg committee details\nAnything about us that we might want to share"
  },
  {
    "objectID": "posts/2023-06-01-june-newsletter/index.html#geospatial-news",
    "href": "posts/2023-06-01-june-newsletter/index.html#geospatial-news",
    "title": "June Newsletter | Drop-in sessions | More Python this month",
    "section": "Geospatial News",
    "text": "Geospatial News\n\n\nConferences/Forums/Resbaz\nNew software/packages\nPodcasts/Media\nNew useful resources"
  },
  {
    "objectID": "posts/2023-06-01-june-newsletter/index.html#geospatial-jobs",
    "href": "posts/2023-06-01-june-newsletter/index.html#geospatial-jobs",
    "title": "June Newsletter | Drop-in sessions | More Python this month",
    "section": "Geospatial Jobs",
    "text": "Geospatial Jobs\n\n\nGovernment Jobs\nIndustry Jobs\nNot for profit/NGO Jobs\nUniversity Jobs"
  },
  {
    "objectID": "posts/2023-06-01-june-newsletter/index.html#training",
    "href": "posts/2023-06-01-june-newsletter/index.html#training",
    "title": "June Newsletter | Drop-in sessions | More Python this month",
    "section": "Training",
    "text": "Training\n\n\nUpcoming UQ Library Training\nUniversity/School specific training\nOther training opportunities"
  },
  {
    "objectID": "posts/2023-06-01-june-newsletter/index.html",
    "href": "posts/2023-06-01-june-newsletter/index.html",
    "title": "June Newsletter | Drop-in sessions | More Python this month",
    "section": "",
    "text": "Drop-in sessions are back this Thursday, so bring your thorniest spatial questions and we‚Äôll help problem solve. In the first half of the session Dr Jackson Stockbridge will give a brief overview of how to access web mapping services (WMS) and web feature services (WFS) in R.\n\n\n\nJoin us for an exciting session where Dr C√©sar Herrara Acosta (Griffith University) will take you on a new dive into the dynamic world of the Python programming language ecosystem! Building upon previous sessions, C√©sar will showcase Python libraries for (1) analysis and visualization of spatial data (e.g.¬†geopandas, datashader, holoviews) and (2) basic computer vision algorithms (OpenCV). C√©sar will also make use of the incredibly versatile Jupyter Notebooks and Jupyter Lab frameworks, which empowers users with seamless workflows for code development, data exploration, and interactive presentations.\n\n\n\nLast month, Dr Tony Howes (UQ, School of Chemical Engineering) ran an excellent workshop on how to make an interactive map using Python and Folium."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Whether its making a simple site map or running deep learning analyses on satellite data, students and researchers are often called upon to conduct geospatial analysis. üó∫Ô∏è\nFortunately, there a wealth of freely available tools can help to complete these tasks: think R, Python and QGIS. üî®\nUnfortunately, a steep learning curve is often required to apply these tools, particularly when we don‚Äôt all have a strong background in coding, GIS or remote sensing.\nThe Geospatial Community tries to make this process a bit less painful, by sharing our experiences, skills and tricks in a friendly and inclusive environment.\nüë©‚Äçüíª We are based in Brisbane, Australia run monthly, online, skill-sharing tutorial sessions where a member of the community teaches us something they have learned along the way.\nThis usually occurs on the last Thursday of each month.\nAnd check out our blog for previous sessions.\nIf you want to get involved, just email us at uqgeo.community @ gmail dot com\n\n\nOrganisers\n\n\nMitch Rudge\n\n\n\nMitch created the GeoSpatial Community, completed his PhD at UQ, and now works as a Natural Capital Data Scientist at Bush Heritage Australia.\n\n\n\n\nChristina Buelow\n\n\n\nChristina is a Research Fellow at the Australian Rivers Institute at Griffith University\n\n\n\n\nCatherine Kim\n\n\n\nCat is a marine scientist working on the Reef Restoration and Adaptation Program at QUT.\n\n\n\n\nNicholas Wiggins\n\n\n\nNick is a Technology Trainer at UQ Library and runs the UQ R User Group\n\n\n\n\nAnnie (Ngoc) Nguyen\n\n\n\nAnnie is a PhD student in the School of Natural Science at UTAS.\n\n\n\n\nKai Ching Cheong\n\n\n\nKai Ching is a researcher at Griffith University researching the impact of mangrove habitat losses and its ecosystem services degradation in aspect of biodiversity conservation in Central Indo-Pacific.\n\n\n\n\nJason Dail\n\n\n\nJason is a PhD Candidate at UQ focusing on natural resource management, ecosystem services, ecosystem based adaptation, geomorphology, GIS, Remote Sensing and Physical Geography.\n\n\n\n\nXiang Zhao\n\n\n\nZhao is a research student working on optimal biodiversity surveying and monitoring design problem in terrestrial Antarctica.\n\n\n\n\nTanya Dodgen\n\n\n\nTanya is a PhD student at QUT investigating coral reef rubble with a focus on its geographical distribution and how it plays into reef recovery."
  },
  {
    "objectID": "posts/2023-05-04-May-Newsletter/index.html#geospatial-community-may-newsletter",
    "href": "posts/2023-05-04-May-Newsletter/index.html#geospatial-community-may-newsletter",
    "title": "Newsletter | Python + Folium | Help Sessions are Back!",
    "section": "",
    "text": "Petition to increase Australian PhD Stipends\n\n\n\n\n\nPlease consider signing this e-petition to increase PhD stipends to minimum wage. Petition reason from the online petition is copied below.\nPetition Reason\nPhD students contribute enormously to Australia‚Äôs research every year, with their time accounting for 56% the total research output. They are the future professors and industry professionals of this country, responsible for driving innovation and ensuring strong international collaborations and competition. Their contribution is often rewarded with a stipend intended to cover the cost of living, with a minimum rate set by the Australian government. Currently, this scholarship rate is $29,863 per year for PhD students to live on while usually working more than 40 hrs per week on their research. Not only is this below minimum wage, but it is even forcing students to live below the poverty line. With this treatment, students are incentivised to accept positions abroad, reject higher degrees by research, or quit midway through their degree due to financial distress. Australia is actively discouraging higher education and slowing its economic growth and productivity by underpaying PhD students. We believe the government should support higher education and ensure a competitive standing for Australia. The way to do this is by setting a fair, liveable rate for PhD scholarships, which is at least equivalent to the minimum wage.\n\n\n\n\n\n\n\n\nDr Tony Howes¬†(UQ, School of Chemical Engineering) will be sharing how Python and Folium (a wrapper for Python of the JavaScript Leaflet library) work together to create an interactive map. The data will be from the Brisbane Dawn Nav, May 5th MapRun. Workshop materials will be in a Jupyter notebook and downloading¬†Anaconda¬†is recommended if you wish to follow along.\n\n\n\n\nIn April, Dr Jacinta Holloway-Brown presented a very informative and interesting workshop that outlined her use of Stochastic spatial random forest SS-RF methods for filling missing data in satellite images due to clouds, and also went over using raster2data for working with spatial image files (such as rasters), and converting these to dataframes in R in order to perform statistical analyses. Expect the blog post and recording associated with this workshop to be available shortly."
  },
  {
    "objectID": "posts/2023-06-04-wms-data-R/index.html",
    "href": "posts/2023-06-04-wms-data-R/index.html",
    "title": "Extracting spatial data from web map servers in R",
    "section": "",
    "text": "Extracting spatial data from web map servers in R\nby Dr Jackson Stockbridge\n\nWhat you‚Äôll learn in these notes\n\nWhat does spatial data on a web map server (WMS) look like?\nHow to use R to access these data\nConvert WMS data in a simple features (sf) object\nA broad overview of the capabilities of importing WMS data into R\n\n\n\nThe problem\nI have recently embarked on a mission to collate a spatial database on environmental features and pressures that will eventually feed into a project aiming to develop marine spatial planning frameworks. I love working with data, so the idea of going out and finding as much as I could to work with in R was exciting (I appreciate that there are few people that would share this excitement).\nSoon into my mission, I was provided a report that was seemingly a gold mine of different, and relevant datasets. However, I quickly stumbled upon a problem I did not foresee. I was directed to a link leading to an interactive map server, and the data I was after were displayed beautifully online (Fig. 1). I figured it would be a simple case of clicking another link to download the data behind this map, but for the life of me, I could not find this link.\n\n\n\nFig. 1. A screenshot of the data I was trying to access displayed on a web map server. Looks great, how do I access it myself!?\n\n\nAs with most of the issues I come across with data (or just with life in general) I figured that R would have a solution. And it did! Here, I will provide a brief overview of how I managed to get the data from the web map server (WMS) into the wonderfully friendly and versatile ‚Äòsf‚Äô format in R.\nBefore getting into the code, I want to acknowledge that the solution and code provided here originally accessed from a very helpful blog post by Thierry Onkelinx. I will post the link to the tutorial below, along with a link to their github page, which is full of useful blogs, codes, and functions.\nTutorial\nGithub page\nI should also state that the tutorial by Thierry provides more details on the capabilities of this code and the objects it creates, as well as what the functions are doing ‚Äòbehind the scenes‚Äô. The code provided here will just scratch the surface, and provide a way to read the WMS data into R as an ‚Äòsf‚Äô object.\nIf you have not come across a data link that leads to a WMS, an example is provided below. This link will display a summary of Australian shipping activity during 1999.\n\n\nCode\nDisclaimer: These notes assume a basic understanding of how to use R and working with spatial data as ‚Äòsf‚Äô objects.\nInstall R and RStudio\nSo, you have followed a link (like the example above) and found yourself looking at the data you want on a WMS. Lets head over to R to start the process of extracting the data.\nInstall and load the following packages:\n\n# Load packages\n#install.packages('tidyverse', 'sf', 'httr', 'ows4R')\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(httr)\nlibrary(ows4R)\n\nThere is an important part of the link provided that R will use to request the data. This is bold and underlined in our example link below:\nhttps://www.cmar.csiro.au/geoserver/nerp/wms?service=WMS&version=1.1.0&request=GetMap&layers=nerp:ausrep_shipping_1999&styles=&bbox=105.0,-49.0,165.0,-5.0&width=768&height=563&srs=EPSG:4326&format=application/openlayers\nLets copy this section of the link into our R script and save it into our global environment. IMPORTANTLY, we need to change ‚Äòwms‚Äô to ‚Äòwfs‚Äô in the link.\n\n# Save URL\nwfs_url <- 'https://www.cmar.csiro.au/geoserver/nerp/wfs?'\n\nI will humbly admit that the nuts and bolts of WMS and web features servers (WFS) are beyond my understanding. I like to think of it as WMS refers to the finished map product, and WFS to the meta data behind the map. WFS seems to be more versatile and useful than WMS when it comes to using these in R.\nNext, we use the ‚Äòparse‚Äô function to convert the URL into a list, allowing us (and R) to do more useful things with it.\n\n# Make URL R-friendly\nurl <- parse_url(wfs_url)\n\nWe now need to generate a connection that will allow R to talk to the server.\n\n# Generate connection to server\nurl_client <- WFSClient$new(wfs_url, serviceVersion = \"2.0.0\")\n\nThis will add ‚Äòurl_client‚Äô into the global environment as an R6 object (or ‚Äòenvironment‚Äô). Don‚Äôt worry if this seems new or strange, this is another example of something that I do not fully understand. However, it does allow us to get a closer look at the capabilities of bringing WMS data into R. By viewing this object (using ‚ÄòView‚Äô), we can dive into its many layers and see what is at our fingertips!\n\n#View(url_client)\n\nWe can also use the following function to explore the operations that provide us with more information on our data.\n\n# Explore operations\nurl_client$\n  getCapabilities()$\n  getOperationsMetadata()$\n  getOperations() %>%\n  map_chr(function(x){x$getName()})\n\n [1] \"GetCapabilities\"       \"DescribeFeatureType\"   \"GetFeature\"           \n [4] \"GetPropertyValue\"      \"ListStoredQueries\"     \"DescribeStoredQueries\"\n [7] \"CreateStoredQuery\"     \"DropStoredQuery\"       \"LockFeature\"          \n[10] \"GetFeatureWithLock\"    \"Transaction\"          \n\n\nSomeone with advanced knowledge and experience with R will be able to use these to truly get the most out of the WMS data. However, this is beyond the scope of this blog, and we will just focus on getting the spatial data into an ‚Äòsf‚Äô object.\nIn saying that, we will get a taste of the capabilities using the following function, which will allow us to look at the data layers that we are able to extract.\n\n# See layer names\nurl_client$getFeatureTypes() %>%\n  map_chr(function(x){x$getName()})\n\n  [1] \"nerp:1996-2000-Data_All_SeismicSurveys__2D_2013_09\" \n  [2] \"nerp:2001-2005-Data_All_SeismicSurveys__2D_2013_09\" \n  [3] \"nerp:2006-2010-Data_All_SeismicSurveys__2D_2013_09\" \n  [4] \"nerp:seismic2d_sum_1961to1965\"                      \n  [5] \"nerp:seismic2d_sum_1966to1970\"                      \n  [6] \"nerp:seismic2d_sum_1971to1975\"                      \n  [7] \"nerp:seismic2d_sum_1976to1980\"                      \n  [8] \"nerp:seismic2d_sum_1981to1985\"                      \n  [9] \"nerp:seismic2d_sum_1986to1990\"                      \n [10] \"nerp:seismic2d_sum_1991to1995\"                      \n [11] \"nerp:seismic2d_sum_1996to2000\"                      \n [12] \"nerp:seismic2d_sum_2001to2005\"                      \n [13] \"nerp:seismic2d_sum_2006to2010\"                      \n [14] \"nerp:37005002_Broadnosesevengill-clip\"              \n [15] \"nerp:37013003_spottedwobb\"                          \n [16] \"nerp:37013020_gulfwobbe\"                            \n [17] \"nerp:37015001_draughtboard-clip\"                    \n [18] \"nerp:37015009_sawtailcatshark\"                      \n [19] \"nerp:37015013_whitefin_swellshark\"                  \n [20] \"nerp:37017003_whiskery-clip\"                        \n [21] \"nerp:37017008_schoolshark\"                          \n [22] \"nerp:37018001_bronze-clip\"                          \n [23] \"nerp:37018003_dusky\"                                \n [24] \"nerp:37020003_Brier_Shark\"                          \n [25] \"nerp:37020005_blackbellylantern\"                    \n [26] \"nerp:37020006_pikedspurdog-clip\"                    \n [27] \"nerp:37020048_greeneyespurdog\"                      \n [28] \"nerp:37023002_commonsawshark-clip\"                  \n [29] \"nerp:37024001_australianangelshark\"                 \n [30] \"nerp:37024002_ornateangelshark\"                     \n [31] \"nerp:37027011_southernFidlerRay\"                    \n [32] \"nerp:37031007_thornbackskate-clip\"                  \n [33] \"nerp:37031009_peacockskate\"                         \n [34] \"nerp:37031010_bightskate\"                           \n [35] \"nerp:37031028_greyskate\"                            \n [36] \"nerp:37035001_shorttailsting-clip\"                  \n [37] \"nerp:37035002_blacksting\"                           \n [38] \"nerp:37039001_southerneagleRay-clip\"                \n [39] \"nerp:37042001_ogilbysghostshark\"                    \n [40] \"nerp:37042003_blackfinGhostshark\"                   \n [41] \"nerp:37042005_SouthernChimaera\"                     \n [42] \"nerp:37043001_ElephantFish-clip\"                    \n [43] \"nerp:seismic3d_sum_1976to1980\"                      \n [44] \"nerp:seismic3d_sum_1981to1985\"                      \n [45] \"nerp:seismic3d_sum_1986to1990\"                      \n [46] \"nerp:seismic3d_sum_1991to1995\"                      \n [47] \"nerp:seismic3d_sum_1996to2000\"                      \n [48] \"nerp:seismic3d_sum_2001to2005\"                      \n [49] \"nerp:seismic3d_sum_2006to2010\"                      \n [50] \"nerp:ais_shipping_2013\"                             \n [51] \"nerp:ais_shipping_2014\"                             \n [52] \"nerp:ais_shipping_2015\"                             \n [53] \"nerp:AMSA-spills-Chemical-2009-2013\"                \n [54] \"nerp:AMSA-spills-Garbage-2009-2013\"                 \n [55] \"nerp:AMSA-spills-Oil-2009-2013\"                     \n [56] \"nerp:AMSA-spills-OtherSubstances\"                   \n [57] \"nerp:AMSA-spills_HarmfulSubstances-2009-2013\"       \n [58] \"nerp:ausrep_shipping_1999\"                          \n [59] \"nerp:ausrep_shipping_2000\"                          \n [60] \"nerp:ausrep_shipping_2001\"                          \n [61] \"nerp:ausrep_shipping_2002\"                          \n [62] \"nerp:ausrep_shipping_2003\"                          \n [63] \"nerp:ausrep_shipping_2004\"                          \n [64] \"nerp:ausrep_shipping_2005\"                          \n [65] \"nerp:ausrep_shipping_2006\"                          \n [66] \"nerp:ausrep_shipping_2007\"                          \n [67] \"nerp:ausrep_shipping_2008\"                          \n [68] \"nerp:ausrep_shipping_2009\"                          \n [69] \"nerp:ausrep_shipping_2010\"                          \n [70] \"nerp:ausrep_shipping_2011\"                          \n [71] \"nerp:csq_al_20112014\"                               \n [72] \"nerp:al_2013070120180631\"                           \n [73] \"nerp:bsczsf_2013070120180631\"                       \n [74] \"nerp:csq_ds_20112014\"                               \n [75] \"nerp:ds_2013070120180631\"                           \n [76] \"nerp:csq_bl_20112014\"                               \n [77] \"nerp:bl_2013070120180631\"                           \n [78] \"nerp:tw_2013070120180631\"                           \n [79] \"nerp:dugong_broadscale\"                             \n [80] \"nerp:dugong_finescale\"                              \n [81] \"nerp:humpbacks_gbr\"                                 \n [82] \"nerp:Fish-csq_gn_20112014\"                          \n [83] \"nerp:gn_2013070120180631\"                           \n [84] \"nerp:greenturtles\"                                  \n [85] \"nerp:csq_hl_20112014\"                               \n [86] \"nerp:hl_2013070120180631\"                           \n [87] \"nerp:wahumpbackdistbroadscale\"                      \n [88] \"nerp:j_2013070120180631\"                            \n [89] \"nerp:llp_2013070120180631\"                          \n [90] \"nerp:mid_2013070120180631\"                          \n [91] \"nerp:oil_spills_sum\"                                \n [92] \"nerp:oil_lease_areas\"                               \n [93] \"nerp:cables_active_hydro_proj\"                      \n [94] \"nerp:cables_deco_hydro_proj\"                        \n [95] \"nerp:updatedpetroleumpipelines\"                     \n [96] \"nerp:wells_oil_gas\"                                 \n [97] \"nerp:csq_llp_20112014\"                              \n [98] \"nerp:csq_pl_20112014\"                               \n [99] \"nerp:pb_2013070120180631\"                           \n[100] \"nerp:1991_enumeration_proj\"                         \n[101] \"nerp:1996_enumeration_proj\"                         \n[102] \"nerp:2001_enumeration_proj\"                         \n[103] \"nerp:2006_enumeration_proj\"                         \n[104] \"nerp:2011_enumeration_proj\"                         \n[105] \"nerp:Pre-1996-Data_All_SeismicSurveys__2D_2013_09\"  \n[106] \"nerp:csq_ps_20112014\"                               \n[107] \"nerp:ps_2013070120180631\"                           \n[108] \"nerp:sa_handline_msf_2006_2010\"                     \n[109] \"nerp:sa_line_msf_2006_2010\"                         \n[110] \"nerp:sa_line_msf_2011_2015\"                         \n[111] \"nerp:sa_nets_msf_2006_2010\"                         \n[112] \"nerp:sa_nets_msf_2011_2015\"                         \n[113] \"nerp:sa_prawn_2006_2010\"                            \n[114] \"nerp:sa_prawn_2011_2015\"                            \n[115] \"nerp:sa_rl_2006_2010\"                               \n[116] \"nerp:sa_rl_2011_2015\"                               \n[117] \"nerp:sa_sardine_2006_2010\"                          \n[118] \"nerp:sa_sardine_2011_2015\"                          \n[119] \"nerp:sa_traps_msf_2006_2010\"                        \n[120] \"nerp:sa_traps_msf_2011_2015\"                        \n[121] \"nerp:sa_abalone_2011_2015\"                          \n[122] \"nerp:sa_handline_msf_2011_2015\"                     \n[123] \"nerp:sa_othergear_msf_2006_2010\"                    \n[124] \"nerp:sa_othergear_msf_2011_2015\"                    \n[125] \"nerp:sa_abalone_2006_2010\"                          \n[126] \"nerp:seagrass_aggregations\"                         \n[127] \"nerp:srw_agregations\"                               \n[128] \"nerp:srw_aggregations_finescale\"                    \n[129] \"nerp:srw_range_broadscale\"                          \n[130] \"nerp:spermwhale_aggregations_finescale\"             \n[131] \"nerp:spermwhale_aggregations_totals\"                \n[132] \"nerp:spermwhale_core_broadscale\"                    \n[133] \"nerp:TASEffort_DS\"                                  \n[134] \"nerp:TASEffort_GNMN\"                                \n[135] \"nerp:TASEffort_HG\"                                  \n[136] \"nerp:TASEffort_DN\"                                  \n[137] \"nerp:TASEffort_HL\"                                  \n[138] \"nerp:TASEffort_JIG\"                                 \n[139] \"nerp:TASEffort_LLDL\"                                \n[140] \"nerp:TASEffort_Seine\"                               \n[141] \"nerp:TASEffort_Trolling\"                            \n[142] \"nerp:csq_tw_20112014\"                               \n[143] \"nerp:csq_tr_20112014\"                               \n[144] \"nerp:tr_2013070120180631\"                           \n[145] \"nerp:tl_2013070120180631\"                           \n[146] \"nerp:csq_tl_20112014\"                               \n[147] \"nerp:vesseldensity2013_broadscale\"                  \n[148] \"nerp:vesseldensity2013_finescale\"                   \n[149] \"nerp:vesseldensity2014_broadscale\"                  \n[150] \"nerp:vesseldensity2014_finescale\"                   \n[151] \"nerp:vesseldensity2015_broadscale\"                  \n[152] \"nerp:vesseldensity2015_finescale\"                   \n[153] \"nerp:ez_broadscale\"                                 \n[154] \"nerp:ez_finescale\"                                  \n[155] \"nerp:humpback_westcoast\"                            \n[156] \"nerp:national_aquaculture\"                          \n[157] \"nerp:nsw_state_fisheries_20112015_gn_mn\"            \n[158] \"nerp:nsw_state_fisheries_20112015_hg\"               \n[159] \"nerp:nsw_state_fisheries_20112015_hl\"               \n[160] \"nerp:nsw_state_fisheries_20112015_ll_dl\"            \n[161] \"nerp:nsw_state_fisheries_20112015_seine_setnet\"     \n[162] \"nerp:nsw_state_fisheries_20112015_seine_shots\"      \n[163] \"nerp:nsw_state_fisheries_20112015_trap_pot\"         \n[164] \"nerp:nsw_state_fisheries_20112015_tw_ds\"            \n[165] \"nerp:nt_state_fisheries_aquarium_display_a12\"       \n[166] \"nerp:nt_state_fisheries_bait_net_a3\"                \n[167] \"nerp:nt_state_fisheries_barramundi_a7\"              \n[168] \"nerp:nt_state_fisheries_coastal_line_a1\"            \n[169] \"nerp:nt_state_fisheries_coastal_net_a2\"             \n[170] \"nerp:nt_state_fisheries_demersal_a6\"                \n[171] \"nerp:nt_state_fisheries_finfish_trawl_a16\"          \n[172] \"nerp:nt_state_fisheries_jigging_fishery_licence_a17\"\n[173] \"nerp:nt_state_fisheries_mud_crad_a8\"                \n[174] \"nerp:nt_state_fisheries_offshore_net_and_line_a5\"   \n[175] \"nerp:nt_state_fisheries_restricted_bait_a15\"        \n[176] \"nerp:nt_state_fisheries_spanish_mackerel_a4\"        \n[177] \"nerp:nt_state_fisheries_timor_reef_a18\"             \n[178] \"nerp:nt_state_fisheries_trepang_a13\"                \n[179] \"nerp:nt_state_fisheriesmollusc_a9\"                  \n[180] \"nerp:qldeffort_harvest2011\"                         \n[181] \"nerp:qldeffort_line2011\"                            \n[182] \"nerp:qldeffort_net2011\"                             \n[183] \"nerp:qldeffort_pot2011\"                             \n[184] \"nerp:qldeffort_trawl2011\"                           \n[185] \"nerp:recboat_v1\"                                    \n[186] \"nerp:recboat_v2\"                                    \n[187] \"nerp:recboat_v3\"                                    \n[188] \"nerp:recboat_v4\"                                    \n[189] \"nerp:recboat_v5\"                                    \n[190] \"nerp:viceffort2011_bs_hl\"                           \n[191] \"nerp:viceffort2011_bs_lldl\"                         \n[192] \"nerp:viceffort2011_bs_mn\"                           \n[193] \"nerp:viceffort2011_bs_twds\"                         \n[194] \"nerp:viceffort2011_ppwp_lldl\"                       \n[195] \"nerp:viceffort2011_ppwp_mn\"                         \n[196] \"nerp:viceffort2011_ppwp_purseseine\"                 \n[197] \"nerp:viceffort2011_ppwp_seinenet\"                   \n[198] \"nerp:viceffort2011_rl_trappot\"                      \n[199] \"nerp:wa_state_fisheries_20112015_gillnet\"           \n[200] \"nerp:wa_state_fisheries_20112015_line\"              \n[201] \"nerp:wa_state_fisheries_20112015_ll_dl\"             \n[202] \"nerp:wa_state_fisheries_20112015_seine_and_haul_net\"\n[203] \"nerp:wa_state_fisheries_20112015_trap_and_pot\"      \n[204] \"nerp:wa_state_fisheries_20112015_trawl_\"            \n\n\nAs you can see, there are actually far more data available than just the shipping summary we were originally looking for! There are over 200 different spatial layers available here, making this a very fruitful endeavour (something we can not always admit to when spending time solving an R problem‚Ä¶).\nNow we‚Äôll get to the important part, getting this layer into an oh-so-familiar-and-comforting-and-versatile ‚Äòsf‚Äô object.\n\n# Extract as sf\nurl$query <- list(service = 'wfs',\n                  # version = '2.0.0', # optional\n                  request = 'GetFeature',\n                  # Change typename to layer name\n                  typename = 'nerp:ausrep_shipping_1999',\n                  srsName = 'EPSG:4326')\nlayer_url <- build_url(url)\n\nSome notes on this code:\n\n‚Äòversion‚Äô is an optional argument and allows us to specify the server version we want R to talk to.\n‚Äòtypename‚Äô refers to the data layer you are wanting to extract. We can change this to any of those listed from the output of the code we ran to get the names of all data layers. Doing so will extract that layer from the server.\n‚ÄòsrsName‚Äô is where we state the coordinate reference system we want our data projected to. If this does not make sense, there are some other great blogs on the Geospatial website to help with this!\n‚Äòbuild_url‚Äô is doing just that; building a URL for our request.\n\nLets turn this into our ‚Äòsf‚Äô object!\n\nlayer_sf <- read_sf(layer_url)\n\nAnd that‚Äôs it! We now have our data layer into a format that allows us to tidy, wrangle, and analyse to our hearts content.\nWe have the all important data now, but that is really only scratching the surface with what we can do with this ‚Äòurl_client‚Äô object. Going into more detail is beyond the scope of this blog but I encourage people who want to learn more to go onto the tutorial and Github page of that tutorials author (links are repeated below).\nTutorial\nGithub page\nJust to give you a taste of the other operations available, here are some example functions. First, we can access the meta data (if available on the server).\n\n# Get meta data\nurl_client$\n  getCapabilities()$\n  getOperationsMetadata()$\n  getOperations() %>%\n  map(function(x){x$getParameters()})\n\n[[1]]\n[[1]]$AcceptVersions\n[1] \"1.0.0\" \"1.1.0\" \"2.0.0\"\n\n[[1]]$AcceptFormats\n[1] \"text/xml\"\n\n[[1]]$Sections\n[1] \"ServiceIdentification\" \"ServiceProvider\"       \"OperationsMetadata\"   \n[4] \"FeatureTypeList\"       \"Filter_Capabilities\"  \n\n\n[[2]]\n[[2]]$outputFormat\n[1] \"application/gml+xml; version=3.2\"\n\n\n[[3]]\n[[3]]$resultType\n[1] \"results\" \"hits\"   \n\n[[3]]$outputFormat\n [1] \"application/gml+xml; version=3.2\"    \n [2] \"GML2\"                                \n [3] \"KML\"                                 \n [4] \"SHAPE-ZIP\"                           \n [5] \"application/json\"                    \n [6] \"application/vnd.google-earth.kml xml\"\n [7] \"application/vnd.google-earth.kml+xml\"\n [8] \"csv\"                                 \n [9] \"gml3\"                                \n[10] \"gml32\"                               \n[11] \"json\"                                \n[12] \"text/csv\"                            \n[13] \"text/xml; subtype=gml/2.1.2\"         \n[14] \"text/xml; subtype=gml/3.1.1\"         \n[15] \"text/xml; subtype=gml/3.2\"           \n\n[[3]]$resolve\n[1] \"none\"  \"local\"\n\n\n[[4]]\n[[4]]$resolve\n[1] \"none\"  \"local\"\n\n[[4]]$outputFormat\n[1] \"application/gml+xml; version=3.2\"\n\n\n[[5]]\nnamed list()\n\n[[6]]\nnamed list()\n\n[[7]]\n[[7]]$language\n[1] \"urn:ogc:def:queryLanguage:OGC-WFS::WFSQueryExpression\"\n\n\n[[8]]\nnamed list()\n\n[[9]]\n[[9]]$releaseAction\n[1] \"ALL\"  \"SOME\"\n\n\n[[10]]\n[[10]]$resultType\n[1] \"results\" \"hits\"   \n\n[[10]]$outputFormat\n [1] \"application/gml+xml; version=3.2\"    \n [2] \"GML2\"                                \n [3] \"KML\"                                 \n [4] \"SHAPE-ZIP\"                           \n [5] \"application/json\"                    \n [6] \"application/vnd.google-earth.kml xml\"\n [7] \"application/vnd.google-earth.kml+xml\"\n [8] \"csv\"                                 \n [9] \"gml3\"                                \n[10] \"gml32\"                               \n[11] \"json\"                                \n[12] \"text/csv\"                            \n[13] \"text/xml; subtype=gml/2.1.2\"         \n[14] \"text/xml; subtype=gml/3.1.1\"         \n[15] \"text/xml; subtype=gml/3.2\"           \n\n[[10]]$resolve\n[1] \"none\"  \"local\"\n\n\n[[11]]\n[[11]]$inputFormat\n[1] \"application/gml+xml; version=3.2\"\n\n[[11]]$releaseAction\n[1] \"ALL\"  \"SOME\"\n\n\nWe can also use the ‚Äòpluck‚Äô function from the ‚Äòpurrr‚Äô package to extract an element hidden deep within a nested object. Here, we are extracting the available output formats for the data layers.\n\n# See output formats\nurl_client$\n  getCapabilities()$\n  getOperationsMetadata()$\n  getOperations() %>%\n  map(function(x){x$getParameters()}) %>%\n  # '3' is an index into the object\n  pluck(3, 'outputFormat')\n\n [1] \"application/gml+xml; version=3.2\"    \n [2] \"GML2\"                                \n [3] \"KML\"                                 \n [4] \"SHAPE-ZIP\"                           \n [5] \"application/json\"                    \n [6] \"application/vnd.google-earth.kml xml\"\n [7] \"application/vnd.google-earth.kml+xml\"\n [8] \"csv\"                                 \n [9] \"gml3\"                                \n[10] \"gml32\"                               \n[11] \"json\"                                \n[12] \"text/csv\"                            \n[13] \"text/xml; subtype=gml/2.1.2\"         \n[14] \"text/xml; subtype=gml/3.1.1\"         \n[15] \"text/xml; subtype=gml/3.2\"           \n\n\nAs you can see, you can export the data in a different format if you are wanting it as something other than an ‚Äòsf‚Äô object or shapefile. We can also turn it into an Excel friendly .csv file.\nSo that brings us to the end of our brief look into WMS and WFS data and how we can access the data (and much more) using R. Hopefully, you have learned something useful.\nHappy data hunting!"
  },
  {
    "objectID": "posts/2023-06-04-wms-data-R/index.html#the-problem",
    "href": "posts/2023-06-04-wms-data-R/index.html#the-problem",
    "title": "Extracting spatial data from web map servers in R",
    "section": "The problem",
    "text": "The problem\nI have recently embarked on a mission to collate a spatial database on environmental features and pressures that will eventually feed into a project aiming to develop marine spatial planning frameworks. I love working with data, so the idea of going out and finding as much as I could to work with in R was exciting (I appreciate that there are few people that would share this excitement).\nSoon into my mission, I was provided a report that was seemingly a gold mine of different, and relevant datasets. However, I quickly stumbled upon a problem I did not foresee. I was directed to a link leading to an interactive map server, and the data I was after were displayed beautifully online (Fig. 1). I figured it would be a simple case of clicking another link to download the data behind this map, but for the life of me, I could not find this link.\n\n\n\nFig. 1. A screenshot of the data I was trying to access displayed on a web map server. Looks great, how do I access it myself!?\n\n\nAs with most of the issues I come across with data (or just with life in general) I figured that R would have a solution. And it did! Here, I will provide a brief overview of how I managed to get the data from the web map server (WMS) into the wonderfully friendly and versatile ‚Äòsf‚Äô format in R.\nBefore getting into the code, I want to acknowledge that the solution and code provided here originally accessed from a very helpful blog post by Thierry Onkelinx. I will post the link to the tutorial below, along with a link to their github page, which is full of useful blogs, codes, and functions.\nTutorial\nGithub page\nI should also state that the tutorial by Thierry provides more details on the capabilities of this code and the objects it creates, as well as what the functions are doing ‚Äòbehind the scenes‚Äô. The code provided here will just scratch the surface, and provide a way to read the WMS data into R as an ‚Äòsf‚Äô object.\nIf you have not come across a data link that leads to a WMS, an example is provided below. This link will display a summary of Australian shipping activity during 1999."
  },
  {
    "objectID": "posts/2023-06-04-wms-data-R/index.html#code",
    "href": "posts/2023-06-04-wms-data-R/index.html#code",
    "title": "Extracting spatial data from web map servers in R",
    "section": "Code",
    "text": "Code\nDisclaimer: These notes assume a basic understanding of how to use R and working with spatial data as ‚Äòsf‚Äô objects.\nInstall R and RStudio\nSo, you have followed a link (like the example above) and found yourself looking at the data you want on a WMS. Lets head over to R to start the process of extracting the data.\nInstall and load the following packages:\n\n# Load packages\n#install.packages('tidyverse', 'sf', 'httr', 'ows4R')\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(httr)\nlibrary(ows4R)\n\nThere is an important part of the link provided that R will use to request the data. This is bold and underlined in our example link below:\nhttps://www.cmar.csiro.au/geoserver/nerp/wms?service=WMS&version=1.1.0&request=GetMap&layers=nerp:ausrep_shipping_1999&styles=&bbox=105.0,-49.0,165.0,-5.0&width=768&height=563&srs=EPSG:4326&format=application/openlayers\nLets copy this section of the link into our R script and save it into our global environment. IMPORTANTLY, we need to change ‚Äòwms‚Äô to ‚Äòwfs‚Äô in the link.\n\n# Save URL\nwfs_url <- 'https://www.cmar.csiro.au/geoserver/nerp/wfs?'\n\nI will humbly admit that the nuts and bolts of WMS and web features servers (WFS) are beyond my understanding. I like to think of it as WMS refers to the finished map product, and WFS to the meta data behind the map. WFS seems to be more versatile and useful than WMS when it comes to using these in R.\nNext, we use the ‚Äòparse‚Äô function to convert the URL into a list, allowing us (and R) to do more useful things with it.\n\n# Make URL R-friendly\nurl <- parse_url(wfs_url)\n\nWe now need to generate a connection that will allow R to talk to the server.\n\n# Generate connection to server\nurl_client <- WFSClient$new(wfs_url, serviceVersion = \"2.0.0\")\n\nThis will add ‚Äòurl_client‚Äô into the global environment as an R6 object (or ‚Äòenvironment‚Äô). Don‚Äôt worry if this seems new or strange, this is another example of something that I do not fully understand. However, it does allow us to get a closer look at the capabilities of bringing WMS data into R. By viewing this object (using ‚ÄòView‚Äô), we can dive into its many layers and see what is at our fingertips!\n\n#View(url_client)\n\nWe can also use the following function to explore the operations that provide us with more information on our data.\n\n# Explore operations\nurl_client$\n  getCapabilities()$\n  getOperationsMetadata()$\n  getOperations() %>%\n  map_chr(function(x){x$getName()})\n\n [1] \"GetCapabilities\"       \"DescribeFeatureType\"   \"GetFeature\"           \n [4] \"GetPropertyValue\"      \"ListStoredQueries\"     \"DescribeStoredQueries\"\n [7] \"CreateStoredQuery\"     \"DropStoredQuery\"       \"LockFeature\"          \n[10] \"GetFeatureWithLock\"    \"Transaction\"          \n\n\nSomeone with advanced knowledge and experience with R will be able to use these to truly get the most out of the WMS data. However, this is beyond the scope of this blog, and we will just focus on getting the spatial data into an ‚Äòsf‚Äô object.\nIn saying that, we will get a taste of the capabilities using the following function, which will allow us to look at the data layers that we are able to extract.\n\n# See layer names\nurl_client$getFeatureTypes() %>%\n  map_chr(function(x){x$getName()})\n\n  [1] \"nerp:1996-2000-Data_All_SeismicSurveys__2D_2013_09\" \n  [2] \"nerp:2001-2005-Data_All_SeismicSurveys__2D_2013_09\" \n  [3] \"nerp:2006-2010-Data_All_SeismicSurveys__2D_2013_09\" \n  [4] \"nerp:seismic2d_sum_1961to1965\"                      \n  [5] \"nerp:seismic2d_sum_1966to1970\"                      \n  [6] \"nerp:seismic2d_sum_1971to1975\"                      \n  [7] \"nerp:seismic2d_sum_1976to1980\"                      \n  [8] \"nerp:seismic2d_sum_1981to1985\"                      \n  [9] \"nerp:seismic2d_sum_1986to1990\"                      \n [10] \"nerp:seismic2d_sum_1991to1995\"                      \n [11] \"nerp:seismic2d_sum_1996to2000\"                      \n [12] \"nerp:seismic2d_sum_2001to2005\"                      \n [13] \"nerp:seismic2d_sum_2006to2010\"                      \n [14] \"nerp:37005002_Broadnosesevengill-clip\"              \n [15] \"nerp:37013003_spottedwobb\"                          \n [16] \"nerp:37013020_gulfwobbe\"                            \n [17] \"nerp:37015001_draughtboard-clip\"                    \n [18] \"nerp:37015009_sawtailcatshark\"                      \n [19] \"nerp:37015013_whitefin_swellshark\"                  \n [20] \"nerp:37017003_whiskery-clip\"                        \n [21] \"nerp:37017008_schoolshark\"                          \n [22] \"nerp:37018001_bronze-clip\"                          \n [23] \"nerp:37018003_dusky\"                                \n [24] \"nerp:37020003_Brier_Shark\"                          \n [25] \"nerp:37020005_blackbellylantern\"                    \n [26] \"nerp:37020006_pikedspurdog-clip\"                    \n [27] \"nerp:37020048_greeneyespurdog\"                      \n [28] \"nerp:37023002_commonsawshark-clip\"                  \n [29] \"nerp:37024001_australianangelshark\"                 \n [30] \"nerp:37024002_ornateangelshark\"                     \n [31] \"nerp:37027011_southernFidlerRay\"                    \n [32] \"nerp:37031007_thornbackskate-clip\"                  \n [33] \"nerp:37031009_peacockskate\"                         \n [34] \"nerp:37031010_bightskate\"                           \n [35] \"nerp:37031028_greyskate\"                            \n [36] \"nerp:37035001_shorttailsting-clip\"                  \n [37] \"nerp:37035002_blacksting\"                           \n [38] \"nerp:37039001_southerneagleRay-clip\"                \n [39] \"nerp:37042001_ogilbysghostshark\"                    \n [40] \"nerp:37042003_blackfinGhostshark\"                   \n [41] \"nerp:37042005_SouthernChimaera\"                     \n [42] \"nerp:37043001_ElephantFish-clip\"                    \n [43] \"nerp:seismic3d_sum_1976to1980\"                      \n [44] \"nerp:seismic3d_sum_1981to1985\"                      \n [45] \"nerp:seismic3d_sum_1986to1990\"                      \n [46] \"nerp:seismic3d_sum_1991to1995\"                      \n [47] \"nerp:seismic3d_sum_1996to2000\"                      \n [48] \"nerp:seismic3d_sum_2001to2005\"                      \n [49] \"nerp:seismic3d_sum_2006to2010\"                      \n [50] \"nerp:ais_shipping_2013\"                             \n [51] \"nerp:ais_shipping_2014\"                             \n [52] \"nerp:ais_shipping_2015\"                             \n [53] \"nerp:AMSA-spills-Chemical-2009-2013\"                \n [54] \"nerp:AMSA-spills-Garbage-2009-2013\"                 \n [55] \"nerp:AMSA-spills-Oil-2009-2013\"                     \n [56] \"nerp:AMSA-spills-OtherSubstances\"                   \n [57] \"nerp:AMSA-spills_HarmfulSubstances-2009-2013\"       \n [58] \"nerp:ausrep_shipping_1999\"                          \n [59] \"nerp:ausrep_shipping_2000\"                          \n [60] \"nerp:ausrep_shipping_2001\"                          \n [61] \"nerp:ausrep_shipping_2002\"                          \n [62] \"nerp:ausrep_shipping_2003\"                          \n [63] \"nerp:ausrep_shipping_2004\"                          \n [64] \"nerp:ausrep_shipping_2005\"                          \n [65] \"nerp:ausrep_shipping_2006\"                          \n [66] \"nerp:ausrep_shipping_2007\"                          \n [67] \"nerp:ausrep_shipping_2008\"                          \n [68] \"nerp:ausrep_shipping_2009\"                          \n [69] \"nerp:ausrep_shipping_2010\"                          \n [70] \"nerp:ausrep_shipping_2011\"                          \n [71] \"nerp:csq_al_20112014\"                               \n [72] \"nerp:al_2013070120180631\"                           \n [73] \"nerp:bsczsf_2013070120180631\"                       \n [74] \"nerp:csq_ds_20112014\"                               \n [75] \"nerp:ds_2013070120180631\"                           \n [76] \"nerp:csq_bl_20112014\"                               \n [77] \"nerp:bl_2013070120180631\"                           \n [78] \"nerp:tw_2013070120180631\"                           \n [79] \"nerp:dugong_broadscale\"                             \n [80] \"nerp:dugong_finescale\"                              \n [81] \"nerp:humpbacks_gbr\"                                 \n [82] \"nerp:Fish-csq_gn_20112014\"                          \n [83] \"nerp:gn_2013070120180631\"                           \n [84] \"nerp:greenturtles\"                                  \n [85] \"nerp:csq_hl_20112014\"                               \n [86] \"nerp:hl_2013070120180631\"                           \n [87] \"nerp:wahumpbackdistbroadscale\"                      \n [88] \"nerp:j_2013070120180631\"                            \n [89] \"nerp:llp_2013070120180631\"                          \n [90] \"nerp:mid_2013070120180631\"                          \n [91] \"nerp:oil_spills_sum\"                                \n [92] \"nerp:oil_lease_areas\"                               \n [93] \"nerp:cables_active_hydro_proj\"                      \n [94] \"nerp:cables_deco_hydro_proj\"                        \n [95] \"nerp:updatedpetroleumpipelines\"                     \n [96] \"nerp:wells_oil_gas\"                                 \n [97] \"nerp:csq_llp_20112014\"                              \n [98] \"nerp:csq_pl_20112014\"                               \n [99] \"nerp:pb_2013070120180631\"                           \n[100] \"nerp:1991_enumeration_proj\"                         \n[101] \"nerp:1996_enumeration_proj\"                         \n[102] \"nerp:2001_enumeration_proj\"                         \n[103] \"nerp:2006_enumeration_proj\"                         \n[104] \"nerp:2011_enumeration_proj\"                         \n[105] \"nerp:Pre-1996-Data_All_SeismicSurveys__2D_2013_09\"  \n[106] \"nerp:csq_ps_20112014\"                               \n[107] \"nerp:ps_2013070120180631\"                           \n[108] \"nerp:sa_handline_msf_2006_2010\"                     \n[109] \"nerp:sa_line_msf_2006_2010\"                         \n[110] \"nerp:sa_line_msf_2011_2015\"                         \n[111] \"nerp:sa_nets_msf_2006_2010\"                         \n[112] \"nerp:sa_nets_msf_2011_2015\"                         \n[113] \"nerp:sa_prawn_2006_2010\"                            \n[114] \"nerp:sa_prawn_2011_2015\"                            \n[115] \"nerp:sa_rl_2006_2010\"                               \n[116] \"nerp:sa_rl_2011_2015\"                               \n[117] \"nerp:sa_sardine_2006_2010\"                          \n[118] \"nerp:sa_sardine_2011_2015\"                          \n[119] \"nerp:sa_traps_msf_2006_2010\"                        \n[120] \"nerp:sa_traps_msf_2011_2015\"                        \n[121] \"nerp:sa_abalone_2011_2015\"                          \n[122] \"nerp:sa_handline_msf_2011_2015\"                     \n[123] \"nerp:sa_othergear_msf_2006_2010\"                    \n[124] \"nerp:sa_othergear_msf_2011_2015\"                    \n[125] \"nerp:sa_abalone_2006_2010\"                          \n[126] \"nerp:seagrass_aggregations\"                         \n[127] \"nerp:srw_agregations\"                               \n[128] \"nerp:srw_aggregations_finescale\"                    \n[129] \"nerp:srw_range_broadscale\"                          \n[130] \"nerp:spermwhale_aggregations_finescale\"             \n[131] \"nerp:spermwhale_aggregations_totals\"                \n[132] \"nerp:spermwhale_core_broadscale\"                    \n[133] \"nerp:TASEffort_DS\"                                  \n[134] \"nerp:TASEffort_GNMN\"                                \n[135] \"nerp:TASEffort_HG\"                                  \n[136] \"nerp:TASEffort_DN\"                                  \n[137] \"nerp:TASEffort_HL\"                                  \n[138] \"nerp:TASEffort_JIG\"                                 \n[139] \"nerp:TASEffort_LLDL\"                                \n[140] \"nerp:TASEffort_Seine\"                               \n[141] \"nerp:TASEffort_Trolling\"                            \n[142] \"nerp:csq_tw_20112014\"                               \n[143] \"nerp:csq_tr_20112014\"                               \n[144] \"nerp:tr_2013070120180631\"                           \n[145] \"nerp:tl_2013070120180631\"                           \n[146] \"nerp:csq_tl_20112014\"                               \n[147] \"nerp:vesseldensity2013_broadscale\"                  \n[148] \"nerp:vesseldensity2013_finescale\"                   \n[149] \"nerp:vesseldensity2014_broadscale\"                  \n[150] \"nerp:vesseldensity2014_finescale\"                   \n[151] \"nerp:vesseldensity2015_broadscale\"                  \n[152] \"nerp:vesseldensity2015_finescale\"                   \n[153] \"nerp:ez_broadscale\"                                 \n[154] \"nerp:ez_finescale\"                                  \n[155] \"nerp:humpback_westcoast\"                            \n[156] \"nerp:national_aquaculture\"                          \n[157] \"nerp:nsw_state_fisheries_20112015_gn_mn\"            \n[158] \"nerp:nsw_state_fisheries_20112015_hg\"               \n[159] \"nerp:nsw_state_fisheries_20112015_hl\"               \n[160] \"nerp:nsw_state_fisheries_20112015_ll_dl\"            \n[161] \"nerp:nsw_state_fisheries_20112015_seine_setnet\"     \n[162] \"nerp:nsw_state_fisheries_20112015_seine_shots\"      \n[163] \"nerp:nsw_state_fisheries_20112015_trap_pot\"         \n[164] \"nerp:nsw_state_fisheries_20112015_tw_ds\"            \n[165] \"nerp:nt_state_fisheries_aquarium_display_a12\"       \n[166] \"nerp:nt_state_fisheries_bait_net_a3\"                \n[167] \"nerp:nt_state_fisheries_barramundi_a7\"              \n[168] \"nerp:nt_state_fisheries_coastal_line_a1\"            \n[169] \"nerp:nt_state_fisheries_coastal_net_a2\"             \n[170] \"nerp:nt_state_fisheries_demersal_a6\"                \n[171] \"nerp:nt_state_fisheries_finfish_trawl_a16\"          \n[172] \"nerp:nt_state_fisheries_jigging_fishery_licence_a17\"\n[173] \"nerp:nt_state_fisheries_mud_crad_a8\"                \n[174] \"nerp:nt_state_fisheries_offshore_net_and_line_a5\"   \n[175] \"nerp:nt_state_fisheries_restricted_bait_a15\"        \n[176] \"nerp:nt_state_fisheries_spanish_mackerel_a4\"        \n[177] \"nerp:nt_state_fisheries_timor_reef_a18\"             \n[178] \"nerp:nt_state_fisheries_trepang_a13\"                \n[179] \"nerp:nt_state_fisheriesmollusc_a9\"                  \n[180] \"nerp:qldeffort_harvest2011\"                         \n[181] \"nerp:qldeffort_line2011\"                            \n[182] \"nerp:qldeffort_net2011\"                             \n[183] \"nerp:qldeffort_pot2011\"                             \n[184] \"nerp:qldeffort_trawl2011\"                           \n[185] \"nerp:recboat_v1\"                                    \n[186] \"nerp:recboat_v2\"                                    \n[187] \"nerp:recboat_v3\"                                    \n[188] \"nerp:recboat_v4\"                                    \n[189] \"nerp:recboat_v5\"                                    \n[190] \"nerp:viceffort2011_bs_hl\"                           \n[191] \"nerp:viceffort2011_bs_lldl\"                         \n[192] \"nerp:viceffort2011_bs_mn\"                           \n[193] \"nerp:viceffort2011_bs_twds\"                         \n[194] \"nerp:viceffort2011_ppwp_lldl\"                       \n[195] \"nerp:viceffort2011_ppwp_mn\"                         \n[196] \"nerp:viceffort2011_ppwp_purseseine\"                 \n[197] \"nerp:viceffort2011_ppwp_seinenet\"                   \n[198] \"nerp:viceffort2011_rl_trappot\"                      \n[199] \"nerp:wa_state_fisheries_20112015_gillnet\"           \n[200] \"nerp:wa_state_fisheries_20112015_line\"              \n[201] \"nerp:wa_state_fisheries_20112015_ll_dl\"             \n[202] \"nerp:wa_state_fisheries_20112015_seine_and_haul_net\"\n[203] \"nerp:wa_state_fisheries_20112015_trap_and_pot\"      \n[204] \"nerp:wa_state_fisheries_20112015_trawl_\"            \n\n\nAs you can see, there are actually far more data available than just the shipping summary we were originally looking for! There are over 200 different spatial layers available here, making this a very fruitful endeavour (something we can not always admit to when spending time solving an R problem‚Ä¶).\nNow we‚Äôll get to the important part, getting this layer into an oh-so-familiar-and-comforting-and-versatile ‚Äòsf‚Äô object.\n\n# Extract as sf\nurl$query <- list(service = 'wfs',\n                  # version = '2.0.0', # optional\n                  request = 'GetFeature',\n                  # Change typename to layer name\n                  typename = 'nerp:ausrep_shipping_1999',\n                  srsName = 'EPSG:4326')\nlayer_url <- build_url(url)\n\nSome notes on this code:\n\n‚Äòversion‚Äô is an optional argument and allows us to specify the server version we want R to talk to.\n‚Äòtypename‚Äô refers to the data layer you are wanting to extract. We can change this to any of those listed from the output of the code we ran to get the names of all data layers. Doing so will extract that layer from the server.\n‚ÄòsrsName‚Äô is where we state the coordinate reference system we want our data projected to. If this does not make sense, there are some other great blogs on the Geospatial website to help with this!\n‚Äòbuild_url‚Äô is doing just that; building a URL for our request.\n\nLets turn this into our ‚Äòsf‚Äô object!\n\nlayer_sf <- read_sf(layer_url)\n\nAnd that‚Äôs it! We now have our data layer into a format that allows us to tidy, wrangle, and analyse to our hearts content.\nWe have the all important data now, but that is really only scratching the surface with what we can do with this ‚Äòurl_client‚Äô object. Going into more detail is beyond the scope of this blog but I encourage people who want to learn more to go onto the tutorial and Github page of that tutorials author (links are repeated below).\nTutorial\nGithub page\nJust to give you a taste of the other operations available, here are some example functions. First, we can access the meta data (if available on the server).\n\n# Get meta data\nurl_client$\n  getCapabilities()$\n  getOperationsMetadata()$\n  getOperations() %>%\n  map(function(x){x$getParameters()})\n\n[[1]]\n[[1]]$AcceptVersions\n[1] \"1.0.0\" \"1.1.0\" \"2.0.0\"\n\n[[1]]$AcceptFormats\n[1] \"text/xml\"\n\n[[1]]$Sections\n[1] \"ServiceIdentification\" \"ServiceProvider\"       \"OperationsMetadata\"   \n[4] \"FeatureTypeList\"       \"Filter_Capabilities\"  \n\n\n[[2]]\n[[2]]$outputFormat\n[1] \"application/gml+xml; version=3.2\"\n\n\n[[3]]\n[[3]]$resultType\n[1] \"results\" \"hits\"   \n\n[[3]]$outputFormat\n [1] \"application/gml+xml; version=3.2\"    \n [2] \"GML2\"                                \n [3] \"KML\"                                 \n [4] \"SHAPE-ZIP\"                           \n [5] \"application/json\"                    \n [6] \"application/vnd.google-earth.kml xml\"\n [7] \"application/vnd.google-earth.kml+xml\"\n [8] \"csv\"                                 \n [9] \"gml3\"                                \n[10] \"gml32\"                               \n[11] \"json\"                                \n[12] \"text/csv\"                            \n[13] \"text/xml; subtype=gml/2.1.2\"         \n[14] \"text/xml; subtype=gml/3.1.1\"         \n[15] \"text/xml; subtype=gml/3.2\"           \n\n[[3]]$resolve\n[1] \"none\"  \"local\"\n\n\n[[4]]\n[[4]]$resolve\n[1] \"none\"  \"local\"\n\n[[4]]$outputFormat\n[1] \"application/gml+xml; version=3.2\"\n\n\n[[5]]\nnamed list()\n\n[[6]]\nnamed list()\n\n[[7]]\n[[7]]$language\n[1] \"urn:ogc:def:queryLanguage:OGC-WFS::WFSQueryExpression\"\n\n\n[[8]]\nnamed list()\n\n[[9]]\n[[9]]$releaseAction\n[1] \"ALL\"  \"SOME\"\n\n\n[[10]]\n[[10]]$resultType\n[1] \"results\" \"hits\"   \n\n[[10]]$outputFormat\n [1] \"application/gml+xml; version=3.2\"    \n [2] \"GML2\"                                \n [3] \"KML\"                                 \n [4] \"SHAPE-ZIP\"                           \n [5] \"application/json\"                    \n [6] \"application/vnd.google-earth.kml xml\"\n [7] \"application/vnd.google-earth.kml+xml\"\n [8] \"csv\"                                 \n [9] \"gml3\"                                \n[10] \"gml32\"                               \n[11] \"json\"                                \n[12] \"text/csv\"                            \n[13] \"text/xml; subtype=gml/2.1.2\"         \n[14] \"text/xml; subtype=gml/3.1.1\"         \n[15] \"text/xml; subtype=gml/3.2\"           \n\n[[10]]$resolve\n[1] \"none\"  \"local\"\n\n\n[[11]]\n[[11]]$inputFormat\n[1] \"application/gml+xml; version=3.2\"\n\n[[11]]$releaseAction\n[1] \"ALL\"  \"SOME\"\n\n\nWe can also use the ‚Äòpluck‚Äô function from the ‚Äòpurrr‚Äô package to extract an element hidden deep within a nested object. Here, we are extracting the available output formats for the data layers.\n\n# See output formats\nurl_client$\n  getCapabilities()$\n  getOperationsMetadata()$\n  getOperations() %>%\n  map(function(x){x$getParameters()}) %>%\n  # '3' is an index into the object\n  pluck(3, 'outputFormat')\n\n [1] \"application/gml+xml; version=3.2\"    \n [2] \"GML2\"                                \n [3] \"KML\"                                 \n [4] \"SHAPE-ZIP\"                           \n [5] \"application/json\"                    \n [6] \"application/vnd.google-earth.kml xml\"\n [7] \"application/vnd.google-earth.kml+xml\"\n [8] \"csv\"                                 \n [9] \"gml3\"                                \n[10] \"gml32\"                               \n[11] \"json\"                                \n[12] \"text/csv\"                            \n[13] \"text/xml; subtype=gml/2.1.2\"         \n[14] \"text/xml; subtype=gml/3.1.1\"         \n[15] \"text/xml; subtype=gml/3.2\"           \n\n\nAs you can see, you can export the data in a different format if you are wanting it as something other than an ‚Äòsf‚Äô object or shapefile. We can also turn it into an Excel friendly .csv file.\nSo that brings us to the end of our brief look into WMS and WFS data and how we can access the data (and much more) using R. Hopefully, you have learned something useful.\nHappy data hunting!"
  },
  {
    "objectID": "posts/2023-07-04-july-newsletter/index.html",
    "href": "posts/2023-07-04-july-newsletter/index.html",
    "title": "July Newsletter | Learn Google Earth Engine this month",
    "section": "",
    "text": "We have an exciting workshop coming up this month; Martin Peikert from Queensland University of Technology will provide an introduction to Google Earth Engine, how it works, how to set it up and get started, where to get data, and finally he‚Äôll take us through a case study in machine learning land-cover classification.\n\n\n\nLast month Dr C√©sar Herrara Acosta (Griffith University) provided an excellent tour of geospatial analysis and computer vision in Python. Check-out his Jupyter Notebooks to try it for yourself, a top-of-the-line learning resource for those starting out in Python: link here.\n\n\n\n\n\nDon‚Äôt miss out, registration is still open for the TERN Science Symposium in Brisbane, July 26th-27th.\n\n\n\nWe‚Äôre planning an in-person get together for the Geospatial Community in Southbank, Brisbane this August - dates etc. still to be decided."
  },
  {
    "objectID": "Code_of_Conduct.html",
    "href": "Code_of_Conduct.html",
    "title": "Geospatial Community",
    "section": "",
    "text": "This code of conduct outlines our expectations for participants within the GeoCommunity, as well as steps to reporting unacceptable behavior. We are committed to providing a welcoming and inspiring community for all and expect our code of conduct to be honored. Anyone who violates this code of conduct may be banned from the community.\nOur Open Source community strives to:\n\nBe friendly and patient.\nBe welcoming: We strive to be a community that welcomes and supports people of all backgrounds and identities. This includes, but is not limited to members of any race, ethnicity, culture, national origin, colour, immigration status, social and economic class, educational level, sex, sexual orientation, gender identity and expression, age, size, family status, political belief, religion, and mental and physical ability.\nBe considerate: Your work will be used by other people, and you in turn will depend on the work of others. Any decision you take will affect users and colleagues, and you should take those consequences into account when making decisions. Although we are based in Australia, the UQ community is extremely diverse, so you might not be communicating in someone else‚Äôs primary language.\nBe respectful: Not all of us will agree all the time, but disagreement is no excuse for poor behavior and poor manners. We might all experience some frustration now and then, but we cannot allow that frustration to turn into a personal attack. It‚Äôs important to remember that a community where people feel uncomfortable or threatened is not a productive one.\nBe careful in the words that we choose: Be kind to others. Do not insult or put down other participants. Harassment and other exclusionary behavior aren‚Äôt acceptable. This includes, but is not limited to: Violent threats or language directed against another person; Discriminatory jokes and language; Posting sexually explicit or violent material; Posting (or threatening to post) other people‚Äôs personally identifying information (‚Äúdoxing‚Äù); Personal insults, especially those using racist or sexist terms; Unwelcome sexual attention; Advocating for, or encouraging, any of the above behavior; Repeated harassment of others. In general, if someone asks you to stop, then stop.\nTry to understand why we disagree: Disagreements, both social and technical, happen all the time. It is important that we resolve disagreements and differing views constructively. Remember that we‚Äôre different. Diversity contributes to the strength of our community, which is composed of people from a wide range of backgrounds. Different people have different perspectives on issues. Being unable to understand why someone holds a viewpoint doesn‚Äôt mean that they‚Äôre wrong. Don‚Äôt forget that it is human to err and blaming each other doesn‚Äôt get us anywhere. Instead, focus on helping to resolve issues and learning from mistakes.\n\n\n\nWe encourage everyone to participate. We are committed to building a community for all. At times, we might fail, but we actively attempt to treat everyone as fairly as possible. Whenever a participant or a host has made a mistake, we expect them to take responsibility for it. If someone has been harmed or offended, it is our responsibility to listen carefully and respectfully, and do our best to right the wrong.\nAlthough this list cannot be exhaustive, we explicitly honor diversity in age, gender, gender identity or expression, culture, ethnicity, language, national origin, political beliefs, profession, race, religion, sexual orientation, socioeconomic status, and technical ability. We will not tolerate discrimination based on any of the protected characteristics above, or on people‚Äôs abilities and physical appearance.\nWe acknowledge the problem with the recent historical default in the scientific computing community‚Äôs makeup, i.e.¬†usually dominated by white hetcis men. We consider that it is our responsibility to counter this self-sustaining dominance by actively encouraging diversity, and by making every effort to ensure participants that do not fit that recent historical default feel welcomed, supported and safe.\n\n\n\nAs a community, we recognise that we have a diverse group of students, faculty, staff, and guests, and we embrace and value the diversity of all our members. It is our policy to be inclusive and mindful of this diversity in our interactions with others. Our members come from all walks of life and so do we. We expect great people from a wide variety of backgrounds, not just because it is the right thing, but because it makes our community stronger.\n\n\n\n‚ÄúInclusion is about the actions we take each day. Think about specific conversations or situations, where you can be exposed to new ideas and perspectives, walk in someone else‚Äôs shoes, or encourage those who might feel like outsiders to join a conversation. I bet you will discover something new about yourself and others too‚Äù\n\n\n\n‚Äì Terri Cooper, Chief Inclusion Officer. >>>\nWe strive to make the collective sum of our individual differences, life experiences, knowledge, innovation, self-expression, and talent our culture.\n\n\n\nIf you experience or witness unacceptable behavior, or have any other concerns, please report it by contacting us directly (uqgeo.community@gmail.com). All reports will be handled with discretion. In your report please include:\n\nYour contact information.\nNames (real, nicknames, or pseudonyms) of any individuals involved. If there are additional witnesses, please include them as well. Your account of what occurred, and if you believe the incident is ongoing. If there is a publicly available record (e.g.¬†a mailing list archive or a public IRC logger), please include a link.\nAny additional information that may be helpful.\n\nAfter filing a report, a representative will contact you personally, review the incident, follow up with any additional questions, and make a decision as to how to respond. If the person who is harassing you is part of the response team, they will recuse themselves from handling your incident. If the complaint originates from a member of the response team, it will be handled by a different member of the response team. We will respect confidentiality requests for the purpose of protecting victims of abuse.\n\n\n\nThis Code of Conduct is based on the UQRUG Code of Conduct, which was based on Galaxy‚Äôs Code of Conduct, released under the Academic Free License version 3.0, which was in turn based on the Open Code of Conduct from the TODOGroup."
  },
  {
    "objectID": "posts/2023-07-04-july-newsletter/index.html#geospatial-community-july-newsletter",
    "href": "posts/2023-07-04-july-newsletter/index.html#geospatial-community-july-newsletter",
    "title": "July Newsletter | Learn Google Earth Engine this month",
    "section": "",
    "text": "We have an exciting workshop coming up this month; Martin Peikert from Queensland University of Technology will provide an introduction to Google Earth Engine, how it works, how to set it up and get started, where to get data, and finally he‚Äôll take us through a case study in machine learning land-cover classification.\n\n\n\nLast month Dr C√©sar Herrara Acosta (Griffith University) provided an excellent tour of geospatial analysis and computer vision in Python. Check-out his Jupyter Notebooks to try it for yourself, a top-of-the-line learning resource for those starting out in Python: link here.\n\n\n\n\n\nDon‚Äôt miss out, registration is still open for the TERN Science Symposium in Brisbane, July 26th-27th.\n\n\n\nWe‚Äôre planning an in-person get together for the Geospatial Community in Southbank, Brisbane this August - dates etc. still to be decided."
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Geospatial Community",
    "section": "",
    "text": "Today"
  },
  {
    "objectID": "calendar/calendar.html",
    "href": "calendar/calendar.html",
    "title": "Geospatial Community",
    "section": "",
    "text": "Today"
  },
  {
    "objectID": "posts/2023-08-01-august-newsletter/index.html",
    "href": "posts/2023-08-01-august-newsletter/index.html",
    "title": "August Newsletter | Upcoming workshop: Spatial High Performance Computing | In-person catch-up | Geospatial conferences",
    "section": "",
    "text": "This month Dr Nevenka (a.k.a. Nena) Bulovic will demonstrate how to run your code on a High Performance Computer (HPC) and provides tips and best practices. She is a research fellow at the Sustainable Minerals Institute, University of Queensland. She is currently exploring potential climate change impacts on mine closure efforts around Australia, and determining future strategies for incorporating climate change into mine closure plans. Prior to this role as a part of her PhD, she investigated the utility of remote sensing data for rain and snow estimation across mountainous terrain. Both projects incorporate her major research interests: application and analysis of large spatial datasets to improve our understanding of hydroclimate and management of water resources\n\n\n\nLet us know if you want to meet with the group this month and get some help - fill out the form here. If there‚Äôs interest in a drop-in session, we‚Äôll organise it for Thursday, August 10th, 2-4pm AEST.\n\n\n\nLet‚Äôs meet in person and celebrate this awesome community of GIS enthusiasts! Friday August 18th in Southbank at the River Quay Green from 4pm. BYO food and drinks. Hope to see you there! Apologies to those not based in Brisbane.\n\n\n\nLast month Martin Peikert from Queensland University Technology gave a great introduction to Google Earth Engine and machine learning landcover classification. Blog post and youtube recording to come."
  },
  {
    "objectID": "posts/2023-08-01-august-newsletter/index.html#geospatial-news",
    "href": "posts/2023-08-01-august-newsletter/index.html#geospatial-news",
    "title": "August Newsletter | Upcoming workshop: Spatial High Performance Computing | In-person catch-up | Geospatial conferences",
    "section": "Geospatial News",
    "text": "Geospatial News\nUpcoming 2023 Pacific Geospatial Conference in Suva, Fiji from November 27 - December 1, 2023, link here.\nFor more than 20 years, the Pacific Islands GIS and Remote Sensing User Conference has provided a platform for: (i) GIS and Remote Sensing (RS) users from nearly all Pacific Island Countries, (ii) satellite data, software, hardware and consulting enterprises and (iii) scientists from universities and research institutions. This year‚Äôs conference theme is ‚ÄúImproving Resilience in the Pacific Islands through GIS and Remote Sensing‚Äù."
  },
  {
    "objectID": "posts/2023-08-01-august-newsletter/index.html#geospatial-community-august-newsletter",
    "href": "posts/2023-08-01-august-newsletter/index.html#geospatial-community-august-newsletter",
    "title": "August Newsletter | Upcoming workshop: Spatial High Performance Computing | In-person catch-up | Geospatial conferences",
    "section": "",
    "text": "This month Dr Nevenka (a.k.a. Nena) Bulovic will demonstrate how to run your code on a High Performance Computer (HPC) and provides tips and best practices. She is a research fellow at the Sustainable Minerals Institute, University of Queensland. She is currently exploring potential climate change impacts on mine closure efforts around Australia, and determining future strategies for incorporating climate change into mine closure plans. Prior to this role as a part of her PhD, she investigated the utility of remote sensing data for rain and snow estimation across mountainous terrain. Both projects incorporate her major research interests: application and analysis of large spatial datasets to improve our understanding of hydroclimate and management of water resources\n\n\n\nLet us know if you want to meet with the group this month and get some help - fill out the form here. If there‚Äôs interest in a drop-in session, we‚Äôll organise it for Thursday, August 10th, 2-4pm AEST.\n\n\n\nLet‚Äôs meet in person and celebrate this awesome community of GIS enthusiasts! Friday August 18th in Southbank at the River Quay Green from 4pm. BYO food and drinks. Hope to see you there! Apologies to those not based in Brisbane.\n\n\n\nLast month Martin Peikert from Queensland University Technology gave a great introduction to Google Earth Engine and machine learning landcover classification. Blog post and youtube recording to come."
  },
  {
    "objectID": "about.html#mitch-rudge",
    "href": "about.html#mitch-rudge",
    "title": "About",
    "section": "Mitch Rudge",
    "text": "Mitch Rudge"
  },
  {
    "objectID": "about.html#christina-buelow",
    "href": "about.html#christina-buelow",
    "title": "About",
    "section": "Christina Buelow",
    "text": "Christina Buelow"
  },
  {
    "objectID": "about.html#catherine-kim",
    "href": "about.html#catherine-kim",
    "title": "About",
    "section": "Catherine Kim",
    "text": "Catherine Kim"
  },
  {
    "objectID": "about.html#nicholas-wiggins",
    "href": "about.html#nicholas-wiggins",
    "title": "About",
    "section": "Nicholas Wiggins",
    "text": "Nicholas Wiggins"
  },
  {
    "objectID": "about.html#annie-ngoc-nguyen",
    "href": "about.html#annie-ngoc-nguyen",
    "title": "About",
    "section": "Annie (Ngoc) Nguyen",
    "text": "Annie (Ngoc) Nguyen"
  },
  {
    "objectID": "about.html#kai-ching-cheong",
    "href": "about.html#kai-ching-cheong",
    "title": "About",
    "section": "Kai Ching Cheong",
    "text": "Kai Ching Cheong"
  },
  {
    "objectID": "about.html#jason-dail",
    "href": "about.html#jason-dail",
    "title": "About",
    "section": "Jason Dail",
    "text": "Jason Dail"
  },
  {
    "objectID": "about.html#xiang-zhao",
    "href": "about.html#xiang-zhao",
    "title": "About",
    "section": "Xiang Zhao",
    "text": "Xiang Zhao"
  },
  {
    "objectID": "about.html#tanya-dodgen",
    "href": "about.html#tanya-dodgen",
    "title": "About",
    "section": "Tanya Dodgen",
    "text": "Tanya Dodgen"
  },
  {
    "objectID": "posts/2023-09-01-september-newsletter/index.html",
    "href": "posts/2023-09-01-september-newsletter/index.html",
    "title": "September Newsletter | Spatial Autocorrelation | Drop-in help session | Upcoming training",
    "section": "",
    "text": "Join us for a drop-in help session - Dr Jackson Stockbridge will give us a quick tutorial on inset mapping with {tmap} in R, and then we‚Äôll discuss discuss creating time series from netcdf files.\nFill out the form if you want to discuss other topics.\n\n\n\nXiang Zhao from Queensland University of Technology will lead a discussion on spatial autocorrelation, a challenging but important topic.\n\n\n\nLast month Dr.¬†Nena Bulovic from The University of Queensland gave us tips and tricks for getting started processing spatial data on the High Performance Computer (HPC). Video recording and notes are available."
  },
  {
    "objectID": "posts/2023-09-01-september-newsletter/index.html#geospatial-community-september-newsletter",
    "href": "posts/2023-09-01-september-newsletter/index.html#geospatial-community-september-newsletter",
    "title": "September Newsletter | Spatial Autocorrelation | Drop-in help session | Upcoming training",
    "section": "",
    "text": "Join us for a drop-in help session - Dr Jackson Stockbridge will give us a quick tutorial on inset mapping with {tmap} in R, and then we‚Äôll discuss discuss creating time series from netcdf files.\nFill out the form if you want to discuss other topics.\n\n\n\nXiang Zhao from Queensland University of Technology will lead a discussion on spatial autocorrelation, a challenging but important topic.\n\n\n\nLast month Dr.¬†Nena Bulovic from The University of Queensland gave us tips and tricks for getting started processing spatial data on the High Performance Computer (HPC). Video recording and notes are available."
  },
  {
    "objectID": "posts/2023-09-01-september-newsletter/index.html#geospatial-news",
    "href": "posts/2023-09-01-september-newsletter/index.html#geospatial-news",
    "title": "September Newsletter | Spatial Autocorrelation | Drop-in help session | Upcoming training",
    "section": "Geospatial News",
    "text": "Geospatial News\n\nFOSS4G SoTM Oceania Conference is in October.\nAlthough the cut-off for submissions has happened, you can contact myself as the Program Chair (program@foss4g-oceania.org) if you wish to still submit a talk. This conference will be a great opportunity to meet the Open Source Geospatial Community in Oceania.\n\n\n\nNew platform to unlock the power of geospatial and AI"
  },
  {
    "objectID": "posts/2023-09-01-september-newsletter/index.html#training",
    "href": "posts/2023-09-01-september-newsletter/index.html#training",
    "title": "September Newsletter | Spatial Autocorrelation | Drop-in help session | Upcoming training",
    "section": "Training",
    "text": "Training\nR courses at the University of Queensland in February 2024, with a whole day on mapping and spatial analysis, register here"
  },
  {
    "objectID": "posts/2023-09-01-september-newsletter/index.html#jobs",
    "href": "posts/2023-09-01-september-newsletter/index.html#jobs",
    "title": "September Newsletter | Spatial Autocorrelation | Drop-in help session | Upcoming training",
    "section": "Jobs",
    "text": "Jobs\nAssociate Research Fellow, Marine Habitat Mapping\nSenior Technical Officer, Marine"
  },
  {
    "objectID": "posts/2023-09-01-october-newsletter/index.html",
    "href": "posts/2023-09-01-october-newsletter/index.html",
    "title": "October Newsletter | Drop-in help virtual help session | Workshop: Large raster manipulation",
    "section": "",
    "text": "Let us know ahead of time if you have any questions you‚Äôd like us to focus on in help-session by filling out this form.\n\n\n\nDr Caitie Kuempel from Griffith University will lead a workshop on large spatial data manipulation in R with {gdalUtils}. Read more from Caitie about the workshop below:\nDo you work with large spatial data sets? Have you spent hours waiting for data to reproject using packages like raster and terra? There may be a faster solution using the gdalUtils package. This package provides R wrappers for the Geospatial Data Abstraction (GDAL) Utilities, which is a translator library for raster and vector geospatial data formats. It uses command line syntax, making it a bit of a black box compared to other R packages, but can save you heaps of processing time. I don‚Äôt claim to be an expert ‚Äì but I‚Äôll introduce a function that I have found to be particularly useful: gdalwarp. I‚Äôll walk through the syntax and how to use the gdalwarp function for reprojecting and transforming large data more efficiently. We will also have a group discussion on potentially useful packages and workflows for large spatial data manipulation.\n\n\n\nZhao Xiang from Queensland University of Technology gave a stimulating presentation on building spatial models and dealing with spatial autocorrelation. Thank-you Zhao! The recording will be made available on our youtube channel.\n\n\n\nDr Jackson Stockbridge led an overview of inset mapping with {tmap} at our September drop-in help session, check-out the blog here"
  },
  {
    "objectID": "posts/2023-09-01-october-newsletter/index.html#geospatial-community-october-newsletter",
    "href": "posts/2023-09-01-october-newsletter/index.html#geospatial-community-october-newsletter",
    "title": "October Newsletter | Drop-in help virtual help session | Workshop: Large raster manipulation",
    "section": "",
    "text": "Let us know ahead of time if you have any questions you‚Äôd like us to focus on in help-session by filling out this form.\n\n\n\nDr Caitie Kuempel from Griffith University will lead a workshop on large spatial data manipulation in R with {gdalUtils}. Read more from Caitie about the workshop below:\nDo you work with large spatial data sets? Have you spent hours waiting for data to reproject using packages like raster and terra? There may be a faster solution using the gdalUtils package. This package provides R wrappers for the Geospatial Data Abstraction (GDAL) Utilities, which is a translator library for raster and vector geospatial data formats. It uses command line syntax, making it a bit of a black box compared to other R packages, but can save you heaps of processing time. I don‚Äôt claim to be an expert ‚Äì but I‚Äôll introduce a function that I have found to be particularly useful: gdalwarp. I‚Äôll walk through the syntax and how to use the gdalwarp function for reprojecting and transforming large data more efficiently. We will also have a group discussion on potentially useful packages and workflows for large spatial data manipulation.\n\n\n\nZhao Xiang from Queensland University of Technology gave a stimulating presentation on building spatial models and dealing with spatial autocorrelation. Thank-you Zhao! The recording will be made available on our youtube channel.\n\n\n\nDr Jackson Stockbridge led an overview of inset mapping with {tmap} at our September drop-in help session, check-out the blog here"
  },
  {
    "objectID": "posts/2023-09-01-october-newsletter/index.html#training",
    "href": "posts/2023-09-01-october-newsletter/index.html#training",
    "title": "October Newsletter | Drop-in help virtual help session | Workshop: Large raster manipulation",
    "section": "Training",
    "text": "Training\nThe annual Research Bazaar is coming up November 21-23 - find out more here\n\nR courses at the University of Queensland in February 2024, with a whole day on mapping and spatial analysis, register here"
  },
  {
    "objectID": "posts/2023-09-30-inset-mapping-tmap/Inset_maps_tmap.html",
    "href": "posts/2023-09-30-inset-mapping-tmap/Inset_maps_tmap.html",
    "title": "Creating inset maps using ‚Äòtmap‚Äô",
    "section": "",
    "text": "by Dr Jackson Stockbridge"
  },
  {
    "objectID": "posts/2023-09-30-inset-mapping-tmap/Inset_maps_tmap.html#creating-inset-maps-using-tmap",
    "href": "posts/2023-09-30-inset-mapping-tmap/Inset_maps_tmap.html#creating-inset-maps-using-tmap",
    "title": "Creating inset maps using ‚Äòtmap‚Äô",
    "section": "Creating inset maps using ‚Äòtmap‚Äô",
    "text": "Creating inset maps using ‚Äòtmap‚Äô\n\nWhat you‚Äôll learn in these notes\n\nHow to produce inset maps using the tmap package\nHow to overcome common issues with inset maps"
  },
  {
    "objectID": "posts/2023-09-30-inset-mapping-tmap/Inset_maps_tmap.html#why-use-inset-maps",
    "href": "posts/2023-09-30-inset-mapping-tmap/Inset_maps_tmap.html#why-use-inset-maps",
    "title": "Creating inset maps using ‚Äòtmap‚Äô",
    "section": "Why use inset maps?",
    "text": "Why use inset maps?\nYou‚Äôve produced an incredible figure that does a great job of visually comuunicating your message. This figure may show the location of your study, the distribution of your habitat of interest, or the results from your experiment. How could you possibly make this figure any more clear or informative?\nInset maps can compliment your existing figures by adding clarification on study location, or by providing essential context that is missing from the figure alone. For example, imagine you have just completed a months worth of fieldwork in challenging conditions. You have worked harder than ever to ensure you sampled as many locations as possible. You then plot the (many) sampling locations on a map, but there is one cluster of overlapping points that do not clearly show how many samples are at that location, or what they are representing. You could use an inset map to zoom in on that area and increase the precision of the points..\nA common use of inset maps is to make your study location more intuitive to a wider audience. If you‚Äôve conducted your study in an estuary in South Australia, this region may be very familiar to the locals, but how would an international audience know exactly where this is? An inset map that shows the Australian continent with your estuary highlighted points an international audience to exactly where your research has taken place.\nLets run some code and see for ourselves!\nInstall R and RStudio\nInstall and load the following packages:\n\n# Load packages\n#install.packages('tidyverse', 'sf', 'tmap', 'tmaptools', 'gridextra', 'cowplot')\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(tmaptools)\nlibrary(gridExtra)\nlibrary(cowplot)\nlibrary(here)\n\nAnd now to load the data:\n\n# Import data\nAus &lt;- st_read('STE_2016_AUST.gpkg')\nSa &lt;- st_read('sa.gpkg')\nsites &lt;- st_read('sites.gpkg')\n\nWe‚Äôll start with our study in South Australia. We want to show where our study has taken place, so we produce a map of Spencer Gulf, a large South Australian inverse estuary.\n\n# Map of South Australia\nSa_map &lt;- tm_shape(Sa) +\n  tm_polygons()\nSa_map\n\n\n\n\nLooks great! But someone who is not from Australia might not immediately know exactly where this is. If you‚Äôre presenting a talk, the last thing you want is a section of your audience trying to think about where your study is taking place, because if they are concentrating on that, they will be missing the vital information you are presenting.\nSo lets produce a map of Australia:\n\n# Map of Australia\nAus_map &lt;- tm_shape(Aus) +\n  tm_polygons()\nAus_map\n\n\n\n\nNext, we will create a polygon that will show the audience exactly where Spencer Gulf is within Australia. We can use the ‚Äòbb‚Äô function from ‚Äòtmaptools‚Äô to do this:\n\n# Bounding box for Spencer Gulf\nbbox_sg &lt;- bb(c(xmin = 135, ymin = -32.25, xmax = 138.4, ymax = -35.8))\n\n# Convert to sf object\nbbox_sg &lt;- st_as_sfc(bbox_sg)\n\nNow we can add our polygon onto the map of Australia:\n\n# Create inset map\ninsetmap &lt;- tm_shape(Aus) +\n  tm_polygons() +\n  tm_shape(bbox_sg) +\n  tm_borders('blue', lwd = 1.5)\ninsetmap\n\n\n\n\nSince we have defined the study area bounding box (the blue box), we can use the ‚Äòbbox‚Äô argument in ‚Äòtm_shape‚Äô to increase the precision of our main map:\n\n# Map of study area\nStudyArea_map &lt;- tm_shape(Sa, bbox = bbox_sg) +\n  tm_polygons()\nStudyArea_map\n\n\n\n\nWe will now start the process of combining our maps to make our final figure. The function we will use to create the figure requires our maps to be in a slightly different format. We need to convert them from tmap objects, into objects compatible with ggplot functions. Thankfully, this is very straight forward, though depending on the detail of the map, it may take a few minutes to run:\n\n# Convert maps to tmap_grob objects\nStudyArea_gg &lt;- tmap_grob(StudyArea_map)\ninsetmap_gg &lt;- tmap_grob(insetmap)\n\nNow it‚Äôs finally time to produce the final figure.\nSome important arguments to pay attention to are the ‚Äòx‚Äô, ‚Äòy‚Äô, and ‚Äòscale‚Äô arguments. These control the position and size of the map you are plotting within ‚Äòdraw_plot‚Äô (you can also use ‚Äòheight‚Äô and ‚Äòwidth‚Äô in place of ‚Äòscale‚Äô for greater control of the map size).\nI won‚Äôt lie, these numbers make little sense to me. My best guess is that R is basing the positioning on a square grid, where the centre point is 0. So if ‚Äòx‚Äô is greater than 0, it‚Äôll move the plot to the right, and less than 0 will move the plot to the left. Similarly, if ‚Äòy‚Äô is greater than 0, it‚Äôll move the plot up, and less than 0 will move the plot down.\nIt always takes a bit of tinkering around with these numbers to get everything looking right. The important thing to remember is that these numbers range between -1 and 1 for ‚Äòx‚Äô and ‚Äòy‚Äô, and 0 and 1 for scale. Only make small changes with the numbers, as the movement of the plot can be quite dramatic.\nIncluded in the code below are a couple of functions called ‚Äòdev.size‚Äô and ‚Äòdev.new‚Äô. These are important, but I will explain their purpose shortly.\n\n# Plot window size\ndev.size()\n\n[1] 7 5\n\n# 4.944444 3.055556\ndev.new(width = 4.944444, height = 3.055556, noRStudioGD = T)\n\n\n# Study area with inset map\nfinal_figure &lt;- \n  ggdraw() +\n  draw_plot(StudyArea_gg, x = 0.02, y = 0.05, \n            width = 1, height = 0.82) +\n  draw_plot(insetmap_gg, x = -0.1, y = 0.3, \n            scale = 0.25)\nfinal_figure\n\n\n\n\nAnd that‚Äôs all it takes‚Ä¶or at least I wish it was. There is an important issue that should be raised, and it is this issue that is overcome using the ‚Äòdev.size‚Äô and ‚Äòdev.new‚Äô functions. It seems that the ‚Äòx‚Äô and ‚Äòy‚Äô numbers are linked to the actual size of your plot window in RStudio. Consequently, if you change the window size, or share your code with a collaborator, the results will not be reproduced as intended.\nTo overcome this, we used ‚Äòdev.size‚Äô to to find the dimensions of our plot window at the time we are viewing it, and then ‚Äòdev.new‚Äô to open up a new plotting window that matches these dimensions. These numbers should be changed once you have positioned your plot to your liking.\nGiven all the trouble you may have to go through to get the size and position right, you may be tempted to combine maps in Powerpoint or Publisher. However, you soon see the value in making reproducible code in R when you need to make changes to maps and do not need to continuously export the maps to these programs and rearrange them manually‚Ä¶this gets tedious after the first five times!\nWe will finish by quickly adding points that represent sample sites onto our map of South Australia, and then using an inset map to zoom in on this area to better visualise these points. Our points will also be coloured depending on the habitat present at that site.\nHere is our map of South Australia with the sample sites overlaid.\n\n# Map of South Australia with sites\npts_map &lt;- tm_shape(Sa) +\n  tm_polygons() +\n  tm_shape(sites) +\n  tm_dots(col = 'Habitat')\npts_map\n\n\n\n\nThis map doesn‚Äôt really tell us much about our sampling, and we didn‚Äôt do all that fieldwork to not show it off!\nAgain, we need to set the bounding box for the area we want to zoom in on (this will be the inset map). Don‚Äôt forget to convert this to a polygon (see above) if you want to highlight the area in the main map.\n\n# Bounding box for inset map\nbbox_sites &lt;- bb(c(xmin = 136.5, ymin = -32.25, xmax = 138.4, ymax = -33.8))\n\n\n# Inset map with sites\ninsetmap &lt;- tm_shape(Sa, bbox = bbox_sites) +\n  tm_polygons() +\n  tm_shape(sites) +\n  tm_dots(col = 'Habitat',\n          size = 0.1,\n          palette = 'Dark2') +\n  tm_layout(legend.outside = T)\ninsetmap\n\n\n\n\nThen the process is exactly the same as above when combining the maps. If you‚Äôd like to practice, see if you can adjust the Spencer Gulf polygon on the Australia map (or adding a new one onto the South Australia map) so it more accurately aligns with area showing the site points. Then combine that as an inset map with the site point map above to create your final figure.\nHappy mapping!"
  },
  {
    "objectID": "posts/2023-09-30-inset-mapping-tmap/index.html",
    "href": "posts/2023-09-30-inset-mapping-tmap/index.html",
    "title": "Creating inset maps using ‚Äòtmap‚Äô",
    "section": "",
    "text": "by Dr Jackson Stockbridge"
  },
  {
    "objectID": "posts/2023-09-30-inset-mapping-tmap/index.html#creating-inset-maps-using-tmap",
    "href": "posts/2023-09-30-inset-mapping-tmap/index.html#creating-inset-maps-using-tmap",
    "title": "Creating inset maps using ‚Äòtmap‚Äô",
    "section": "Creating inset maps using ‚Äòtmap‚Äô",
    "text": "Creating inset maps using ‚Äòtmap‚Äô\n\nWhat you‚Äôll learn in these notes\n\nHow to produce inset maps using the tmap package\nHow to overcome common issues with inset maps"
  },
  {
    "objectID": "posts/2023-09-30-inset-mapping-tmap/index.html#why-use-inset-maps",
    "href": "posts/2023-09-30-inset-mapping-tmap/index.html#why-use-inset-maps",
    "title": "Creating inset maps using ‚Äòtmap‚Äô",
    "section": "Why use inset maps?",
    "text": "Why use inset maps?\nYou‚Äôve produced an incredible figure that does a great job of visually comuunicating your message. This figure may show the location of your study, the distribution of your habitat of interest, or the results from your experiment. How could you possibly make this figure any more clear or informative?\nInset maps can compliment your existing figures by adding clarification on study location, or by providing essential context that is missing from the figure alone. For example, imagine you have just completed a months worth of fieldwork in challenging conditions. You have worked harder than ever to ensure you sampled as many locations as possible. You then plot the (many) sampling locations on a map, but there is one cluster of overlapping points that do not clearly show how many samples are at that location, or what they are representing. You could use an inset map to zoom in on that area and increase the precision of the points..\nA common use of inset maps is to make your study location more intuitive to a wider audience. If you‚Äôve conducted your study in an estuary in South Australia, this region may be very familiar to the locals, but how would an international audience know exactly where this is? An inset map that shows the Australian continent with your estuary highlighted points an international audience to exactly where your research has taken place.\nLets run some code and see for ourselves!\nInstall R and RStudio\nInstall and load the following packages:\n\n# Load packages\n#install.packages('tidyverse', 'sf', 'tmap', 'tmaptools', 'gridextra', 'cowplot')\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(tmaptools)\nlibrary(gridExtra)\nlibrary(cowplot)\nlibrary(here)\n\nAnd now to load the data:\n\n# Import data\nAus &lt;- st_read('STE_2016_AUST.gpkg')\nSa &lt;- st_read('sa.gpkg')\nsites &lt;- st_read('sites.gpkg')\n\nWe‚Äôll start with our study in South Australia. We want to show where our study has taken place, so we produce a map of Spencer Gulf, a large South Australian inverse estuary.\n\n# Map of South Australia\nSa_map &lt;- tm_shape(Sa) +\n  tm_polygons()\nSa_map\n\n\n\n\nLooks great! But someone who is not from Australia might not immediately know exactly where this is. If you‚Äôre presenting a talk, the last thing you want is a section of your audience trying to think about where your study is taking place, because if they are concentrating on that, they will be missing the vital information you are presenting.\nSo lets produce a map of Australia:\n\n# Map of Australia\nAus_map &lt;- tm_shape(Aus) +\n  tm_polygons()\nAus_map\n\n\n\n\nNext, we will create a polygon that will show the audience exactly where Spencer Gulf is within Australia. We can use the ‚Äòbb‚Äô function from ‚Äòtmaptools‚Äô to do this:\n\n# Bounding box for Spencer Gulf\nbbox_sg &lt;- bb(c(xmin = 135, ymin = -32.25, xmax = 138.4, ymax = -35.8))\n\n# Convert to sf object\nbbox_sg &lt;- st_as_sfc(bbox_sg)\n\nNow we can add our polygon onto the map of Australia:\n\n# Create inset map\ninsetmap &lt;- tm_shape(Aus) +\n  tm_polygons() +\n  tm_shape(bbox_sg) +\n  tm_borders('blue', lwd = 1.5)\ninsetmap\n\n\n\n\nSince we have defined the study area bounding box (the blue box), we can use the ‚Äòbbox‚Äô argument in ‚Äòtm_shape‚Äô to increase the precision of our main map:\n\n# Map of study area\nStudyArea_map &lt;- tm_shape(Sa, bbox = bbox_sg) +\n  tm_polygons()\nStudyArea_map\n\n\n\n\nWe will now start the process of combining our maps to make our final figure. The function we will use to create the figure requires our maps to be in a slightly different format. We need to convert them from tmap objects, into objects compatible with ggplot functions. Thankfully, this is very straight forward, though depending on the detail of the map, it may take a few minutes to run:\n\n# Convert maps to tmap_grob objects\nStudyArea_gg &lt;- tmap_grob(StudyArea_map)\ninsetmap_gg &lt;- tmap_grob(insetmap)\n\nNow it‚Äôs finally time to produce the final figure.\nSome important arguments to pay attention to are the ‚Äòx‚Äô, ‚Äòy‚Äô, and ‚Äòscale‚Äô arguments. These control the position and size of the map you are plotting within ‚Äòdraw_plot‚Äô (you can also use ‚Äòheight‚Äô and ‚Äòwidth‚Äô in place of ‚Äòscale‚Äô for greater control of the map size).\nI won‚Äôt lie, these numbers make little sense to me. My best guess is that R is basing the positioning on a square grid, where the centre point is 0. So if ‚Äòx‚Äô is greater than 0, it‚Äôll move the plot to the right, and less than 0 will move the plot to the left. Similarly, if ‚Äòy‚Äô is greater than 0, it‚Äôll move the plot up, and less than 0 will move the plot down.\nIt always takes a bit of tinkering around with these numbers to get everything looking right. The important thing to remember is that these numbers range between -1 and 1 for ‚Äòx‚Äô and ‚Äòy‚Äô, and 0 and 1 for scale. Only make small changes with the numbers, as the movement of the plot can be quite dramatic.\nIncluded in the code below are a couple of functions called ‚Äòdev.size‚Äô and ‚Äòdev.new‚Äô. These are important, but I will explain their purpose shortly.\n\n# Plot window size\ndev.size()\n\n[1] 7 5\n\n# 4.944444 3.055556\ndev.new(width = 4.944444, height = 3.055556, noRStudioGD = T)\n\n\n# Study area with inset map\nfinal_figure &lt;- \n  ggdraw() +\n  draw_plot(StudyArea_gg, x = 0.02, y = 0.05, \n            width = 1, height = 0.82) +\n  draw_plot(insetmap_gg, x = -0.1, y = 0.3, \n            scale = 0.25)\nfinal_figure\n\n\n\n\nAnd that‚Äôs all it takes‚Ä¶or at least I wish it was. There is an important issue that should be raised, and it is this issue that is overcome using the ‚Äòdev.size‚Äô and ‚Äòdev.new‚Äô functions. It seems that the ‚Äòx‚Äô and ‚Äòy‚Äô numbers are linked to the actual size of your plot window in RStudio. Consequently, if you change the window size, or share your code with a collaborator, the results will not be reproduced as intended.\nTo overcome this, we used ‚Äòdev.size‚Äô to to find the dimensions of our plot window at the time we are viewing it, and then ‚Äòdev.new‚Äô to open up a new plotting window that matches these dimensions. These numbers should be changed once you have positioned your plot to your liking.\nGiven all the trouble you may have to go through to get the size and position right, you may be tempted to combine maps in Powerpoint or Publisher. However, you soon see the value in making reproducible code in R when you need to make changes to maps and do not need to continuously export the maps to these programs and rearrange them manually‚Ä¶this gets tedious after the first five times!\nWe will finish by quickly adding points that represent sample sites onto our map of South Australia, and then using an inset map to zoom in on this area to better visualise these points. Our points will also be coloured depending on the habitat present at that site.\nHere is our map of South Australia with the sample sites overlaid.\n\n# Map of South Australia with sites\npts_map &lt;- tm_shape(Sa) +\n  tm_polygons() +\n  tm_shape(sites) +\n  tm_dots(col = 'Habitat')\npts_map\n\n\n\n\nThis map doesn‚Äôt really tell us much about our sampling, and we didn‚Äôt do all that fieldwork to not show it off!\nAgain, we need to set the bounding box for the area we want to zoom in on (this will be the inset map). Don‚Äôt forget to convert this to a polygon (see above) if you want to highlight the area in the main map.\n\n# Bounding box for inset map\nbbox_sites &lt;- bb(c(xmin = 136.5, ymin = -32.25, xmax = 138.4, ymax = -33.8))\n\n\n# Inset map with sites\ninsetmap &lt;- tm_shape(Sa, bbox = bbox_sites) +\n  tm_polygons() +\n  tm_shape(sites) +\n  tm_dots(col = 'Habitat',\n          size = 0.1,\n          palette = 'Dark2') +\n  tm_layout(legend.outside = T)\ninsetmap\n\n\n\n\nThen the process is exactly the same as above when combining the maps. If you‚Äôd like to practice, see if you can adjust the Spencer Gulf polygon on the Australia map (or adding a new one onto the South Australia map) so it more accurately aligns with area showing the site points. Then combine that as an inset map with the site point map above to create your final figure.\nHappy mapping!"
  },
  {
    "objectID": "posts/2023-12-22-december-newsletter/index.html",
    "href": "posts/2023-12-22-december-newsletter/index.html",
    "title": "December Newsletter | End of Year re-cap",
    "section": "",
    "text": "Hi everyone,\nIt‚Äôs been a huge year for our geospatial community (now Geospatial Share ‚Äì see below). We hosted so many interesting and informative workshops. In February and March, Christina Buelow from Griffith Uni showed us how to calculate areas accurately in R using both vector and raster data, and ran through some R Spatial Basics. In April, Jacinta Holloway-Brown taught us how to implement spatial random forests. In May, Tony Howes from the UQ showed us how to create Interactive Maps with Python and Folium. In June, C√©sar Herrara Acosta from Griffith University walked us through some of his work with computer vision in Python. In July, Martin Peikert from Queensland University of Technology will provide an introduction to Google Earth Engine. In August, Nena Bulovic demonstrated the application of High Performance Computing (HPC) for spatial analysis. In September, Xiang Zhao from the Queensland University of Technology Led an interesting discussion about spatial autocorrelation. In October, Caitie Kuempel walked us through her use of gdalUtils for large raster manipulation.\nOn top of our monthly workshops, we also hosted a number of other events including, several drop-in help sessions; some ad-hoc short form sessions + blogs including Creating inset maps with tmap with Jackson Stockbridge and a presentation about Community groups for Continuing Professional Development with Chris Mancini; an in-person catchup; and a workshop at the 2023 Research Bazaar!\nA huge thanks to everyone who presented and attended workshops, maintained and updated the website, organised workshops, wrote blog posts and newsletters, and coordinated and attended in-person events. This is all voluntary, and shows a real commitment to sharing knowledge and helping others üòä\nLooking ahead to 2024, we have some very exciting opportunities. At long last, we have become a registered charity. After some conversations within the organising committee, we settled on ‚ÄúGeospatial Share‚Äù, which we thought gets to the heart of what the group is all about. The registration opens a number of opportunities, including eligibility for small grants, a new .org website domain, and a proper youtube channel. As an Incorporated association, we will also hold an AGM and elect members to the Management committee. For now, the official management committee consists of Myself as President, Christina Buelow as Secretary and Nicholas Wiggins as Treasurer (not to be confused with the organising committee). Next year, we will also look to grow our collaborations, starting with some planned joint workshops and activities with the Royal Geographic Society of QLD (RGSQ).\nIf you have speakers you would like to hear from, subjects you are interested in, ideas for collaborations or events, or want to get more involved in running the community, please get in touch.\nVery much looking forward to next year, but for now, I hope everyone takes a well-earned break over Christmas. Oh, and it wouldn‚Äôt be 2023 without some GenAI ‚Äì so here we go:\n\nMake an image representing a community of likeminded people who are all committed to sharing geospatial knowledge ‚Äì called Geospatial Share. And make the background Brisbane, Australia."
  },
  {
    "objectID": "posts/2023-12-22-december-newsletter/index.html#end-of-year-re-cap-from-geospatial-communitys-president-mitch-rudge",
    "href": "posts/2023-12-22-december-newsletter/index.html#end-of-year-re-cap-from-geospatial-communitys-president-mitch-rudge",
    "title": "December Newsletter | End of Year re-cap",
    "section": "",
    "text": "Hi everyone,\nIt‚Äôs been a huge year for our geospatial community (now Geospatial Share ‚Äì see below). We hosted so many interesting and informative workshops. In February and March, Christina Buelow from Griffith Uni showed us how to calculate areas accurately in R using both vector and raster data, and ran through some R Spatial Basics. In April, Jacinta Holloway-Brown taught us how to implement spatial random forests. In May, Tony Howes from the UQ showed us how to create Interactive Maps with Python and Folium. In June, C√©sar Herrara Acosta from Griffith University walked us through some of his work with computer vision in Python. In July, Martin Peikert from Queensland University of Technology will provide an introduction to Google Earth Engine. In August, Nena Bulovic demonstrated the application of High Performance Computing (HPC) for spatial analysis. In September, Xiang Zhao from the Queensland University of Technology Led an interesting discussion about spatial autocorrelation. In October, Caitie Kuempel walked us through her use of gdalUtils for large raster manipulation.\nOn top of our monthly workshops, we also hosted a number of other events including, several drop-in help sessions; some ad-hoc short form sessions + blogs including Creating inset maps with tmap with Jackson Stockbridge and a presentation about Community groups for Continuing Professional Development with Chris Mancini; an in-person catchup; and a workshop at the 2023 Research Bazaar!\nA huge thanks to everyone who presented and attended workshops, maintained and updated the website, organised workshops, wrote blog posts and newsletters, and coordinated and attended in-person events. This is all voluntary, and shows a real commitment to sharing knowledge and helping others üòä\nLooking ahead to 2024, we have some very exciting opportunities. At long last, we have become a registered charity. After some conversations within the organising committee, we settled on ‚ÄúGeospatial Share‚Äù, which we thought gets to the heart of what the group is all about. The registration opens a number of opportunities, including eligibility for small grants, a new .org website domain, and a proper youtube channel. As an Incorporated association, we will also hold an AGM and elect members to the Management committee. For now, the official management committee consists of Myself as President, Christina Buelow as Secretary and Nicholas Wiggins as Treasurer (not to be confused with the organising committee). Next year, we will also look to grow our collaborations, starting with some planned joint workshops and activities with the Royal Geographic Society of QLD (RGSQ).\nIf you have speakers you would like to hear from, subjects you are interested in, ideas for collaborations or events, or want to get more involved in running the community, please get in touch.\nVery much looking forward to next year, but for now, I hope everyone takes a well-earned break over Christmas. Oh, and it wouldn‚Äôt be 2023 without some GenAI ‚Äì so here we go:\n\nMake an image representing a community of likeminded people who are all committed to sharing geospatial knowledge ‚Äì called Geospatial Share. And make the background Brisbane, Australia."
  }
]